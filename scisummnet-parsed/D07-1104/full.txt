Introduction. Current statistical machine translation systems rely on very large rule sets. In phrase-based systems, rules are extracted from parallel corpora containingtens or hundreds of millions of words. This can result in millions of rules using even the most conser vative extraction heuristics. Efficient algorithms for rule storage and access are necessary for practical decoding algorithms. They are crucial to keeping up with the ever-increasing size of parallel corpora, as well as the introduction of new data sources such as web-mined and comparable corpora. Until recently, most approaches to this probleminvolved substantial tradeoffs. The common practice of test set filtering renders systems impracti cal for all but batch processing. Tight restrictions on phrase length curtail the power of phrase-basedmodels. However, some promising engineering so lutions are emerging. Zens and Ney (2007) use a disk-based prefix tree, enabling efficient access to phrase tables much too large to fit in main memory. An alternative approach introduced independently by both Callison-Burch et al (2005) and Zhang and Vogel (2005) is to store the training data itself inmemory, and use a suffix array as an efficient in dex to look up, extract, and score phrase pairs on the fly. We believe that the latter approach has several important applications (?7).So far, these techniques have focused on phrase based models using contiguous phrases (Koehn et al., 2003; Och and Ney, 2004). Some recent models permit discontiguous phrases (Chiang, 2007; Quirket al, 2005; Simard et al, 2005). Of particular in terest to us is the hierarchical phrase-based model ofChiang (2007), which has been shown to be supe rior to phrase-based models. The ruleset extractedby this model is a superset of the ruleset in an equivalent phrase-based model, and it is an order of magnitude larger. This makes efficient rule representa tion even more critical. We tackle the problem using the online rule extraction method of Callison-Burch et al (2005) and Zhang and Vogel (2005). The problem statement for our work is: Given an input sentence, efficiently find all hierarchical phrase-based translation rules for that sentence in the training corpus. 976 We first review suffix arrays (?2) and hierarchicalphrase-based translation (?3). We show that the obvious approach using state-of-the-art pattern match ing algorithms is hopelessly inefficient (?4). We then describe a series of algorithms to address thisinefficiency (?5). Our algorithms reduce computa tion time by two orders of magnitude, making the approach feasible (?6). We close with a discussion that describes several applications of our work (?7).
Suffix Arrays. . A suffix array is a data structure representing all suf fixes of a corpus in lexicographical order (Manber and Myers, 1993). Formally, for a text T , the ith suffix of T is the substring of the text beginning atposition i and continuing to the end of T . This suf fix can be uniquely identified by the index i of itsfirst word. The suffix array SAT of T is a permuta tion of [1, |T |] arranged by the lexicographical order of the corresponding suffixes. This representationenables fast lookup of any contiguous substring us ing binary search. Specifically, all occurrences of a length-m substring can be found in O(m + log |T |) time (Manber and Myers, 1993). 1 Callison-Burch et al (2005) and Zhang and Vogel (2005) use suffix arrays as follows. 1. Load the source training text F , the suffix array. SAF , the target training text E, and the align ment A into memory. 2. For each input sentence, look up each substring. (phrase) f? of the sentence in the suffix array. aligned phrase e? using the phrase extraction method of Koehn et al (2003). 4. Compute the relative frequency score p(e?|f?) of. each pair using the count of the extracted pair and the marginal count of f? 5. Compute the lexical weighting score of the. phrase pair using the alignment that gives the best score. 1Abouelhoda et al (2004) show that lookup can be done in optimal O(m) time using some auxiliaray data structures. Forour purposes O(m + log |T |) is practical, since for the 27M word corpus used to carry out our experiments, log |T | ? 25. 6. Use the scored rules to translate the input sen-. tence with a standard decoding algorithm. A difficulty with this approach is step 3, which canbe quite slow. Its complexity is linear in the num ber of occurrences of the source phrase f? Both Callison-Burch et al (2005) and Zhang and Vogel (2005) solve this with sampling. If a source phraseappears more than k times, they sample only k oc currences for rule extraction. Both papers reportthat translation performance is nearly identical to ex tracting all possible phrases when k = 100. 2
Hierarchical Phrase-Based Translation. . We consider the hierarchical translation model ofChiang (2007). Formally, this model is a syn chronous context-free grammar. The lexicalizedtranslation rules of the grammar may contain a sin gle nonterminal symbol, denoted X . We will use a, b, c and d to denote terminal symbols, and u, v, andw to denote (possibly empty) sequences of these ter minals. We will additionally use ? and ? to denote(possibly empty) sequences containing both termi nals and nonterminals. A translation rule is written X ? ?/?. This rule states that a span of the input matching ? is replacedby ? in translation. We require that ? and ? con tain an equal number (possibly zero) of coindexed nonterminals. An example rule with coindexes is X ? uX 1 vX 2w/u ?X 2 v ?X 1w ?. When discussing only the source side of such rules, we will leave out the coindexes. For instance, the source side of the above rule will be written uXvXw. 3 For the purposes of this paper, we adhere to therestrictions described by Chiang (2007) for rules ex tracted from the training data. Rules can contain at most two nonterminals. Rules can contain at most five terminals. Rules can span at most ten words. 2A sample size of 100 is actually quite small for many phrases, some of which occur tens or hundreds of thousands of times. It is perhaps surprising that such a small sample size works as well as the full data. However, recent work by Och (2005) and Federico and Bertoldi (2006) has shown that the statistics used by phrase-based systems are not very precise. 3In the canonical representation of the grammar, source-sidecoindexes are always in sorted order, making them unambigu ous. 977 ? Nonterminals must span at least two words. Adjacent nonterminals are disallowed in the source side of a rule. Expressed more economically, we say that our goal is to search for source phrases in the form u, uXv, or uXvXw, where 1 ? |uvw| ? 5, and |v| > 0 in the final case. Note that the model also allows rules in the form Xu, uX , XuX , XuXv, and uXvX . However, these rules are lexically identical to other rules, and thus will match the same locations in the source text.
The Collocation Problem. . On-the-fly lookup using suffix arrays involves an added complication when the rules are in form uXv or uXvXw. Binary search enables fast lookup of contiguous substrings. However, it cannot be used for discontiguous substrings. Consider the rule aXbXc. If we search for this rule in the followinglogical suffix array fragment, we will find the bold faced matches. ... a c a c b a d c a d ... a c a d b a a d b d ... a d d b a a d a b c ... a d d b d a a b b a ... a d d b d d c a a a ... Even though these suffixes are in lexicographicalorder, matching suffixes are interspersed with non matching suffixes. We will need another algorithmto find the source rules containing at least oneX surrounded by nonempty sequences of terminal sym bols. 4.1 Baseline Approach. In the pattern-matching literature, words spanned by the nonterminal symbols of Chiang?s grammar are called don?t cares and a nonterminal symbol in a query pattern that matches a sequence of don?t caresis called a variable length gap. The search prob lem for patterns containing these gaps is a variant of approximate pattern matching, which has receivedsubstantial attention (Navarro, 2001). The best algo rithm for pattern matching with variable-length gaps in a suffix array is a recent algorithm by Rahman et al (2006). It works on a pattern w1Xw2X...wI consisting of I contiguous substrings w1, w2, ...wI ,each separated by a gap. The algorithm is straight forward. After identifying all ni occurrences of each wi in O(|wi| + log |T |) time, collocations thatmeet the gap constraints are computed using an ef ficient data structure called a stratified tree (van Emde Boas et al, 1977). 4 Although we refer the reader to the source text for a full description of this data structure, its salient characteristic is that it implements priority queue operations insert and next-element in O(log log |T |) time. Therefore, thetotal running time for an algorithm to find all con tiguous subpatterns and compute their collocations is O( ?I i=1 [|wi|+ log|T |+ ni log log |T |]). We can improve on the algorithm of Rahman et al. (2006) using a variation on the idea of hashing. We exploit the fact that our large text is actually acollection of relatively short sentences, and that col located patterns must occur in the same sentence in order to be considered a rule. Therefore, we can use the sentence id of each subpattern occurrence as a kind of hash key. We create a hash table whosesize is exactly the number of sentences in our train ing corpus. Each location of the partially matched pattern w1X...Xwi is inserted into the hash bucket with the matching sentence id. To find collocated patterns wi+1, we probe the hash table with each of the ni+1 locations for that subpattern. When amatch is found, we compare the element with all el ements in the bucket to see if it is within the windowimposed by the phrase length constraints. Theoreti cally, the worst case for this algorithm occurs when all elements of both sets resolve to the same hash bucket, and we must compare all elements of one set with all elements of the other set. This leads to a worst case complexity of O( ?I i=1 [|wi|+ log|T |] +?Ii=1 ni). However, for real language data the per formance for sets of any significant size will be O( ?I i=1 [|wi|+ log|T |+ ni]), since most patterns will occur once in any given sentence. 4.2 Analysis. It is instructive to compare this with the complex ity for contiguous phrases. In that case, total lookup time is O(|w| + log|T |) for a contiguous pattern w. 4Often known in the literature as a van Emde Boas tree or van Emde Boas priority queue. 978 The crucial difference between the contiguous and discontiguous case is the added term ?I i=1 ni. Foreven moderately frequent subpatterns this term dom inates complexity. To make matters concrete, consider the training corpus used in our experiments (?6), which contains27M source words. The three most frequent uni grams occur 1.48M, 1.16M and 688K times ? thefirst two occur on average more than once per sen tence. In the worst case, looking up a contiguous phrase containing any number and combination ofthese unigrams requires no more than 25 compari son operations. In contrast, the worst case scenario for a pattern with a single gap, bookended on either side by the most frequent word, requires over two million operations using our baseline algorithm and over thirteen million using the algorithm of Rahman et al (2006). A single frequent word in an input sentence is enough to cause noticeable slowdowns, since it can appear in up to 530 hierarchical rules.To analyze the cost empirically, we ran our base line algorithm on the first 50 sentences of the NIST Chinese-English 2003 test set and measured the CPU time taken to compute collocations. We foundthat, on average, it took 2241.25 seconds (?37 min utes) per sentence just to compute all of the needed collocations. By comparison, decoding time persentence is roughly 10 seconds with moderately ag gressive pruning, using the Python implementation of Chiang (2007).
Solving the Collocation Problem. . Clearly, looking up patterns in this way is not prac tical. To analyze the problem, we measured the amount of CPU time per computation. Cumulative lookup time was dominated by a very small fraction of the computations (Fig. 1). As expected, further analysis showed that these expensive computations all involved one or more very frequent subpatterns. In the worst cases a single collocation took severalseconds to compute. However, there is a silver lining. Patterns follow a Zipf distribution, so the number of pattern types that cause the problem is actu ally quite small. The vast majority of patterns arerare. Therefore, our solution focuses on computa tions where one or more of the component patternsis frequent. Assume that we are computing a collo Computations (ranked by time) C u m u l a t i v e T i m e ( s ) 300K 150K Figure 1: Ranked computations vs. cumulative time. A small fraction of all computations account for most of the computational time. cation of pattern w1X...Xwi and pattern wi+1, and we know all locations of each. There are three cases. If both patterns are frequent, we resort to a precomputed intersection (?5.1). We were notaware of any algorithms to substantially im prove the efficiency of this computation when it is requested on the fly, but precomputation can be done in a single pass over the text at decoder startup. If one pattern is frequent and the other is rare,we use an algorithm whose complexity is de pendent mainly on the frequency of the rare pattern (?5.2). It can also be used for pairs of rare patterns when one pattern is much rarer than the other. If both patterns are rare, no special algorithms are needed. Any linear algorithm will suffice. However, for reasons described in ?5.3, our other collocation algorithms depend on sorted sets, so we use a merge algorithm.Finally, in order to cut down on the number of un necessary computations, we use an efficient method to enumerate the phrases to lookup (?5.4). This method also forms the basis of various caching strategies for additional speedups. We analyze the memory use of our algorithms in ?5.5. 5.1 Precomputation. Precomputation of the most expensive collocationscan be done in a single pass over the text. As in put, our algorithm requires the identities of the k 979 most frequent contiguous patterns. 5 It then iterates over the corpus. Whenever a pattern from the list is seen, we push a tuple consisting of its identity and current location onto a queue. Whenever the oldest item on the queue falls outside the maximum phrase length window with respect to the current position,we compute that item?s collocation with all succeed ing patterns (subject to pattern length constraints) and pop it from the queue. We repeat this step for every item that falls outside the window. At the end of each sentence, we compute collocations for any remaining items in the queue and then empty it. Our precomputation includes the most frequent n-gram subpatterns. Most of these are unigrams, but in our experiments we found 5-grams among the 1000 most frequent patterns. We precompute the locations of source phrase uXv for any pair u and v that both appear on this list. There is alsoa small number of patterns uXv that are very frequent. We cannot easily obtain a list of these in ad vance, but we observe that they always consist of apair u and v of patterns from near the top of the frequency list. Therefore we also precompute the loca tions uXvXw of patterns in which both u and v are among these super-frequent patterns (all unigrams), treating this as the collocation of the frequent pattern uXv and frequent pattern w. We also compute the analagous case for u and vXw. 5.2 Fast Intersection. For collocations of frequent and rare patterns, we use a fast set intersection method for sorted sets called double binary search (Baeza-Yates, 2004). 6 It is based on the intuition that if one set in a pair of sorted sets is much smaller than the other, thenwe can compute their intersection efficiently by per forming a binary search in the larger data set D for each element of the smaller query set Q. Double binary search takes this idea a step further.It performs a binary search in D for the median ele ment of Q. Whether or not the element is found, the 5These can be identified using a single traversal over alongest common prefix (LCP) array, an auxiliary data struc ture of the suffix array, described by Manber and Myers (1993). Since we don?t need the LCP array at runtime, we chose to do this computation once offline. 6Minor modifications are required since we are computing collocation rather than intersection. Due to space constraints, details and proof of correctness are available in Lopez (2007a). search divides both sets into two pairs of smaller sets that can be processed recursively. Detailed analysis and empirical results on an information retrieval task are reported in Baeza-Yates (2004) and Baeza-Yates and Salinger (2005). If |Q| log |D| < |D| then theperformance is guaranteed to be sublinear. In practice it is often sublinear even if |Q| log |D| is somewhat larger than |D|. In our implementation we sim ply check for the condition ?|Q| log |D| < |D| to decide whether we should use double binary search or the merge algorithm. This check is applied in the recursive cases as well as for the initial inputs. Thevariable ? can be adjusted for performance. We de termined experimentally that a good value for this parameter is 0.3. 5.3 Obtaining Sorted Sets. Double binary search requires that its input sets be in sorted order. However, the suffix array returnsmatchings in lexicographical order, not numeric or der. The algorithm of Rahman et al (2006) deals with this problem by inserting the unordered items into a stratified tree. This requires O(n log log |T |) time for n items. If we used the same strategy, our algorithm would no longer be sublinear.An alternative is to precompute all n-gram occur rences in order and store them in an inverted index. This can be done in one pass over the data. 7 This approach requires a separate inverted index for each n, up to the maximum n used by the model. The memory cost is one length-|T | array per index. In order to avoid the full n|T | cost in memory, our implementation uses a mixed strategy. We keep a precomputed inverted index only for unigrams.For bigrams and larger n-grams, we generate the in dex on the fly using stratified trees. This results in a superlinear algorithm for intersection. However,we can exploit the fact that we must compute col locations multiple times for each input n-gram by caching the sorted set after we create it (The cachingstrategy is described in ?5.4). Subsequent computations involving this n-gram can then be done in lin ear or sublinear time. Therefore, the cost of building the inverted index on the fly is amortized over a large number of computations. 7We combine this step with the other precomputations that require a pass over the data, thereby removing a redundant O(|T |) term from the startup cost. 980 5.4 Efficient Enumeration. A major difference between contiguous phrase based models and hierarchical phrase-based models is the number of rules that potentially apply to an input sentence. To make this concrete, on our data, with an average of 29 words per sentence, there were on average 133 contiguous phrases of length 5 orless that applied. By comparison, there were on av erage 7557 hierarchical phrases containing up to 5words. These patterns are obviously highly overlap ping and we employ an algorithm to exploit this fact.We first describe a baseline algorithm used for con tiguous phrases (?5.4.1). We then introduce some improvements (?5.4.2) and describe a data structureused by the algorithm (?5.4.3). Finally, we dis cuss some special cases for discontiguous phrases (?5.4.4). 5.4.1 The Zhang-Vogel AlgorithmZhang and Vogel (2005) present a clever algorithm for contiguous phrase searches in a suffix ar ray. It exploits the fact that for eachm-length source phrase that we want to look up, we will also want to look up its (m? 1)-length prefix. They observe that the region of the suffix array containing all suffixes prefixed by ua is a subset of the region containingthe suffixes prefixed by u. Therefore, if we enumer ate the phrases of our sentence in such a way that we always search for u before searching for ua, wecan restrict the binary search for ua to the range con taining the suffixes prefixed by u. If the search for u fails, we do not need to search for ua at all. They show that this approach leads to some time savings for phrase search, although the gains are relatively modest since the search for contiguous phrases is not very expensive to begin with. However, the potential savings in the discontiguous case are much greater. 5.4.2 Improvements and Extensions We can improve on the Zhang-Vogel algorithm. An m-length contiguous phrase aub depends not only on the existence of its prefix au, but also on the existence of its suffix ub. In the contiguous case, we cannot use this information to restrict the starting range of the binary search, but we can check for the existence of ub to decide whether we even need to search for aub at all. This can help us avoid searches that are guaranteed to be fruitless. Now consider the discontiguous case. As in the analogous contiguous case, a phrase a?b will onlyexist in the text if its maximal prefix a? and maxi mal suffix ?b both exist in the corpus and overlap at specific positions. 8 Searching for a?b is potentially very expensive, so we put all available information to work. Before searching, we require that both a?and ?b exist. Additionally, we compute the location of a?b using the locations of both maximal sub phrases. To see why the latter optimization is useful, consider a phrase abXcd. In our baseline algorithm, we would search for ab and cd, and then perform a computation to see whether these subphrases were collocated within an elastic window. However, if weinstead use abXc and bXcd as the basis of the com putation, we gain two advantages. First, the number elements of each set is likely to be smaller then in the former case. Second, the computation becomes simpler, because we now only need to check to see whether the patterns exactly overlap with a starting offset of one, rather than checking within a window of locations.We can improve efficiency even further if we con sider cases where the same substring occurs morethan once within the same sentence, or even in mul tiple sentences. If the computation required to look up a phrase is expensive, we would like to performthe lookup only once. This requires some mecha nism for caching. Depending on the situation, we might want to cache only certain subsets of phrases, based on their frequency or difficulty to compute.We would also like the flexibility to combine on the-fly lookups with a partially precomputed phrase table, as in the online/offline mixture of Zhang and Vogel (2005).We need a data structure that provides this flex ibility, in addition to providing fast access to both the maximal prefix and maximal suffix of any phrase that we might consider. 5.4.3 Prefix Trees and Suffix Links Our search optimizations are easily captured in a prefix tree data structure augmented with suffix links.Formally, a prefix tree is an unminimized determin istic finite-state automaton that recognizes all of thepatterns in some set. Each node in the tree repre8Except when ? = X , in which case a and b must be collo cated within a window defined by the phrase length constraints. 981 ab b c cX X (1)(2) (3) d (4) d a b b c cX X (1)(2) (3) d (4) d a b b c cX X (1)(2) (3) d (4) d a b b c cX X (1)(2) (3) d (4) d X e a c d Case 1 Case 2 Figure 2: Illustration of prefix tree construction showing a partial prefix tree, including suffix links. Suppose we are interested in pattern abXcd, represented by node (1). Its prefix is represented by node (2), and node (2)?s suffix is represented by node (3). Therefore, node (1)?s suffix is represented by the node pointed to by the d-edge from node (3), which is node (4). There are two cases. In case 1, node (4) is inactive, so we can mark node (1) inactive and stop. In case 2, node (4) is active, so we compute the collocation of abXc and bXcd with information stored at nodes (2) and (4), using either a precomputed intersection, double binary search, or merge, depending on the size of the sets. If the result is empty, we mark the node inactive. Otherwise, we store the results at node (1) and add its successor patterns to the frontier for the next iteration. This includes all patterns containing exactly one more terminal symbol than the current pattern. sents the prefix of a unique pattern from the set that is specified by the concatenation of the edge labels along the path from the root to that node. A suffix link is a pointer from a node representing path a? to the node representing path ?. We will use this data structure to record the set of patterns that we have searched for and to cache information for those that were found successfully. Our algorithm generates the tree breadth-search along a frontier. In the mth iteration we only searchfor patterns containingm terminal symbols. Regardless of whether we find a particular pattern, we cre ate a node for it in the tree. If the pattern was found in the corpus, its node is marked active. Otherwise, it is marked inactive. For found patterns, we storeeither the endpoints of the suffix array range con taining the phrase (if it is contiguous), or the list oflocations at which the phrase is found (if it is dis contiguous). We can also store the extracted rules. 9 Whenever a pattern is successfully found, we add all patterns with m + 1 terminals that are prefixed by it 9Conveniently, the implementation of Chiang (2007) uses aprefix tree grammar encoding, as described in Klein and Manning (2001). Our implementation decorates this tree with addi tional information required by our algorithms. to the frontier for processing in the next iteration.To search for a pattern, we use location infor mation from its parent node, which represents its maximal prefix. Assuming that the node representsphrase ?b, we find the node representing its max imal suffix by following the b-edge from the node pointed to by its parent node?s suffix link. If the node pointed to by this suffix link is inactive, we can mark the node inactive without running a search. When a node is marked inactive, we discontinue search for phrases that are prefixed by the path it represents. The algorithm is illustrated in Figure 2. 5.4.4 Special Cases for Phrases with GapsA few subtleties arise in the extraction of hierarchical patterns. Gaps are allowed to occur at the be ginning or end of a phrase. For instance, we mayhave a source phrase Xu or uX or even XuX . Al though each of these phrases requires its own path in the prefix tree, they are lexically identical to phrase u. An analogous situation occurs with the patterns XuXv, uXvX , and uXv. There are two cases that we are concerned with. The first case consists of all patterns prefixed with X . The paths to nodes representing these patterns 982 will all contain the X-edge originating at the rootnode. All of these paths form the shadow subtree. Path construction in this subtree proceeds dif ferently. Because they are lexically identical to theirsuffixes, they are automatically extended if their suffix paths are active, and they inherit location infor mation of their suffixes. The second case consists of all patterns suffixedwith X . Whenever we successfully find a new pat tern ?, we automatically extend it with an X edge,provided that ?X is allowed by the model con straints. The node pointed to by this edge inheritsits location information from its parent node (repre senting the maximal prefix ?). Note that both special cases occur for patterns in the form XuX . 5.5 Memory Requirements. As shown in Callison-Burch et al (2005), we must keep an array for the source text F , its suffix array,the target text E, and alignment A in memory. As suming that A and E are roughly the size of F , thecost is 4|T |. If we assume that all data use vocabu laries that can be represented using 32-bit integers, then our 27M word corpus can easily be represented in around 500MB of memory. Adding the inverted index for unigrams increases this by 20%. The main additional cost in memory comes from the storage of the precomputed collocations. This is dependentboth on the corpus size and the number of colloca tions that we choose to precompute. Using detailed timing data from our experiments we were able to simulate the memory-speed tradeoff (Fig. 3). If we include a trigram model trained on our bitext and the Chinese Gigaword corpus, the overall storage costs for our system are approximately 2GB.
Experiments. . All of our experiments were performed on ChineseEnglish in the news domain. We used a large train ing set consisting of over 1 million sentences from various newswire corpora. This corpus is roughly the same as the one used for large-scale experiments by Chiang et al (2005). To generate alignments,we used GIZA++ (Och and Ney, 2003). We symmetrized bidirectional alignments using the grow diag-final heuristic (Koehn et al, 2003). 0 0 0 1000 0 Number of frequent subpatterns Insert text here 41 sec/sent 41 seconds 405 sec/sent 0 MB. 725MB Figure 3: Effect of precomputation on memory useand processing time. Here we show only the mem ory requirements of the precomputed collocations. We used the first 50 sentences of the NIST 2003test set to compute timing results. All of our algo rithms were implemented in Python 2.4. 10 Timingresults are reported for machines with 8GB of mem ory and 4 3GHz Xeon processors running Red Hat linux 2.6.9. In order to understand the contributions of various improvements, we also ran the system with with various ablations. In the default setting, the prefix tree is constructed for each sentence to guide phrase lookup, and then discarded. To showthe effect of caching we also ran the algorithm without discarding the prefix tree between sentences, re sulting in full inter-sentence caching. The results are shown in Table 1. 11It is clear from the results that each of the op timizations is needed to sufficiently reduce lookuptime to practical levels. Although this is still rela tively slow, it is much closer to the decoding time of 10 seconds per sentence than the baseline. 10Python is an interpreted language and our implementations do not use any optimization features. It is therefore reasonable to think that a more efficient reimplementation would result in across-the-board speedups.11The results shown here do not include the startup time re quired to load the data structures into memory. In our Python implementation this takes several minutes, which in principle should be amortized over the cost for each sentence. However,just as Zens and Ney (2007) do for phrase tables, we could com pile our data structures into binary memory-mapped files, whichcan be read into memory in a matter of seconds. We are cur rently investigating this option in a C reimplementation. 983 Algorithms Secs/Sent Collocations Baseline 2241.25 325548 Prefix Tree 1578.77 69994 Prefix Tree + precomputation 696.35 69994 Prefix Tree + double binary 405.02 69994 Prefix Tree + precomputation + double binary 40.77 69994 Prefix Tree with full caching + precomputation + double binary 30.70 67712 Table 1: Timing results and number of collocations computed for various combinations of algorithms. The runs using precomputation use the 1000 most frequent patterns.
Conclusions and Future Work. . Our work solves a seemingly intractable problemand opens up a number of intriguing potential ap plications. Both Callison-Burch et al (2005) and Zhang and Vogel (2005) use suffix arrays to relax the length constraints on phrase-based models. Ourwork enables this in hierarchical phrase-based models. However, we are interested in additional appli cations. Recent work in discriminative learning for manynatural language tasks, such as part-of-speech tagging and information extraction, has shown that feature engineering plays a critical role in these approaches. However, in machine translation most fea tures can still be traced back to the IBM Models of 15 years ago (Lopez, 2007b). Recently, Lopez and Resnik (2006) showed that most of the features used in standard phrase-based models do not help very much. Our algorithms enable us to look up phrasepairs in context, which will allow us to compute interesting contextual features that can be used in discriminative learning algorithms to improve transla tion accuracy. Essentially, we can use the training data itself as an indirect representation of whateverfeatures we might want to compute. This is not pos sible with table-based architectures.Most of the data structures and algorithms discussed in this paper are widely used in bioinformatics, including suffix arrays, prefix trees, and suf fix links (Gusfield, 1997). As discussed in ?4.1, our problem is a variant of the approximate patternmatching problem. A major application of approx imate pattern matching in bioinformatics is queryprocessing in protein databases for purposes of se quencing, phylogeny, and motif identification.Current MT models, including hierarchical mod els, translate by breaking the input sentence intosmall pieces and translating them largely independently. Using approximate pattern matching algo rithms, we imagine that machine translation could be treated very much like search in a protein database. In this scenario, the goal is to select training sentences that match the input sentence as closely as possible, under some evaluation function that accounts for both matching and mismatched sequences, as well as possibly other data features. Once we have found the closest sentences we cantranslate the matched portions in their entirety, re placing mismatches with appropriate word, phrase, or hierarchical phrase translations as needed. This model would bring statistical machine translation closer to convergence with so-called example-based translation, following current trends (Marcu, 2001;Och, 2002). We intend to explore these ideas in fu ture work. AcknowledgementsI would like to thank Philip Resnik for encour agement, thoughtful discussions and wise counsel; David Chiang for providing the source code for his translation system; and Nitin Madnani, Smaranda Muresan and the anonymous reviewers for very helpful comments on earlier drafts of this paper. Any errors are my own. This research was supported in part by ONR MURI Contract FCPO.810548265 and the GALE program of the Defense AdvancedResearch Projects Agency, Contract No. HR0011 06-2-001. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the author and do not necessarily reflect the view of DARPA. 984