2: Test set performance of our systems: and output/reference length ratio. 4.3 Training and decoding parameters We tried to improve performance by increasing some of the limits imposed on the training and decoding setup. During training, long sentences are removed from the training data to speed up the GIZA++ word alignment process. Traditionally, we worked with a sentence length limit of 40. We found that increasing this limit to about 80 gave better results without causing undue problems with running the word alignment (GIZA++ increasingly fails and runs much slower with long sentences). We also tried to increase beam sizes and the limit on the number of translation options per coverage span (ttable-limit). This has shown to be successful in our experiments with Arabic–English and Chinese–English systems. Surprisingly, increasing the maximum stack size to 1000 (from 200) and ttable-limit to 100 (from 20) has barely any efon translation performance. The changed only by less than 0.05, and often worsened. 4.4 German–English system The German–English language pair is especially challenging due to the large differences in word order. Collins et al. (2005) suggest a method to reorder the German input before translating using a set of manually crafted rules. In our German–English submissions, this is done both to the training data and the input to the machine translation system. 5 Conclusions Our submission to the WMT 2007 shared task is a fairly straight-forward use of the Moses MT system using default parameters. In a sense, we submitted baseline performance of this system. for all our systems on the test sets are displayed in Table 2. Compared to other submitted systems, these are very good scores, often the best or second highest scores for these tasks. We made a special effort in two areas: We explored domain adaptation methods for the News- Commentary test sets and we used reordering rules for the German–English language pair.