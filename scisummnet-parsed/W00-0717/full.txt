1 Introduction. In this paper I present a novel program that induces syntactic categories from comparatively small corpora of unlabelled text, using only distributional information. There are various motivations for this task, which affect the algorithms employed. Many NLP systems use a set of tags, largely syntactic in motivation, that have been selected according to various criteria. In many circumstances it would be desirable for engineering reasons to generate a larger set of tags, or a set of domain-specific tags for a particular corpus. Furthermore, the construction of cognitive models of language acquisition — that will almost certainly involve some notion of syntactic category — requires an explanation of the acquisition of that set of syntactic categories. The amount of data used in this study is 12 million words, which is consistent with a pessimistic lower bound on the linguistic experience of the infant language learner in the period from 2 to 5 years of age, and has had capitalisation removed as being information not available in that circumstance.
2 Previous Work. Previous work falls into two categories. A number of researchers have obtained good results using pattern recognition techniques. Finch and Chater (1992), (1995) and Schiitze (1993), (1997) use a set of features derived from the co-occurrence statistics of common words together with standard clustering and information extraction techniques. For sufficiently frequent words this method produces satisfactory results. Brown et al. (1992) use a very large amount of data, and a well-founded information theoretic model to induce large numbers of plausible semantic and syntactic clusters. Both approaches have two flaws: they cannot deal well with ambiguity, though Schiitze addresses this issue partially, and they do not cope well with rare words. Since rare and ambiguous words are very common in natural language, these limitations are serious.
3 Context Distributions. Whereas earlier methods all share the same basic intuition, i.e. that similar words occur in similar contexts, I formalise this in a slightly different way: each word defines a probability distribution over all contexts, namely the probability of the context given the word. If the context is restricted to the word on either side, I can define the context distribution to be a distribution over all ordered pairs of words: the word before and the word after. The context distribution of a word can be estimated from the observed contexts in a corpus. We can then measure the similarity of words by the similarity of their context distributions, using the Kullback-Leibler (KL) divergence as a distance function. Unfortunately it is not possible to cluster based directly on the context distributions for two reasons: first the data is too sparse to estimate the context distributions adequately for any but the most frequent words, and secondly some words which intuitively are very similar (Schi_itze's example is 'a' and 'an') have radically different context distributions. Both of these problems can be overcome in the normal way by using clusters: approximate the context distribution as being a probability distribution over ordered pairs of clusters multiplied by the conditional distributions of the words given the clusters : I use an iterative algorithm, starting with a trivial clustering, with each of the K clusters filled with the kth most frequent word in the corpus. At each iteration, I calculate the context distribution of each cluster, which is the weighted average of the context distributions of each word in the cluster. The distribution is calculated with respect to the K current clusters and a further ground cluster of all unclassified words: each distribution therefore has (K + 1)2 parameters. For every word that occurs more than 50 times in the corpus, I calculate the context distribution, and then find the cluster with the lowest KL divergence from that distribution. I then sort the words by the divergence from the cluster that is closest to them, and select the best as being the members of the cluster for the next iteration. This is repeated, gradually increasing the number of words included at each iteration, until a high enough proportion has been clustered, for example 80%. After each iteration, if the distance between two clusters falls below a threshhold value, the clusters are merged, and a new cluster is formed from the most frequent unclustered word. Since there will be zeroes in the context distributions, they are smoothed using Good-Turing smoothing(Good, 1953) to avoid singularities in the KL divergence. At this point we have a preliminary clustering — no very rare words will be included, and some common words will also not be assigned, because they are ambiguous or have idiosyncratic distributional properties.
4 Ambiguity and Sparseness. Ambiguity can be handled naturally within this framework. The context distribution p(w) of a particular ambiguous word w can be modelled as a linear combination of the context distributions of the various clusters. We can find the mixing coefficients by minimising efficients that sum to unity and the qi are the context distributions of the clusters. A minimum of this function can be found using the EM algorithm(Dempster et al., 1977). There are often several local minima — in practice this does not seem to be a major problem. Note that with rare words, the KL divergence reduces to the log likelihood of the word's context distribution plus a constant factor. However, the observed context distributions of rare words may be insufficient to make a definite determination of its cluster membership. In this case, under the assumption that the word is unambiguous, which is only valid for comparatively rare words, we can use Bayes's rule to calculate the posterior probability that it is in each class, using as a prior probability the distribution of rare words in each class. This incorporates the fact that rare words are much more likely to be adjectives or nouns than, for example, pronouns.
5 Results. I used 12 million words of the British National Corpus as training data, and ran this algorithm with various numbers of clusters (77, 100 and 150). All of the results in this paper are produced with 77 clusters corresponding to the number of tags in the CLAWS tagset used to tag the BNC, plus a distinguished sentence boundary token. In each case, the clusters induced contained accurate classes corresponding to the major syntactic categories, and various subgroups of them such as prepositional verbs, first names, last names and so on. Appendix A shows the five most frequent words in a clustering with 77 clusters. In general, as can be seen, the clusters correspond to traditional syntactic classes. There are a few errors — notably, the right bracket is classified with adverbial particles like &quot;UP&quot;. For each word w, I then calculated the optimal coefficents crtv). Table 1 shows some sample ambiguous words, together with the clusters with largest values of ai. Each cluster is represented by the most frequent member of the cluster. Note that &quot;US&quot; is a proper noun cluster. As there is more than one common noun cluster, for many unambiguous nouns the optimum is a mixture of the various classes. with tags NN1 (common noun) and AJO (adjective). Table 2 shows the accuracy of cluster assignment for rare words. For two CLAWS tags, AJO (adjective) and NN1(singular common noun) that occur frequently among rare words in the corpus, I selected all of the words that occurred n times in the corpus, and at least half the time had that CLAWS tag. I then tested the accuracy of my assignment algorithm by marking it as correct if it assigned the word to a 'plausible' cluster — for AJO, either of the clusters &quot;NEW&quot; or &quot;IMPORTANT&quot;, and for NN1, one of the clusters &quot;TIME&quot;, &quot;PEOPLE&quot;, &quot;WORLD&quot;, &quot;GROUP&quot; or &quot;FACT&quot;. I did this for n in {1, 2, 3, 5, 10, 20}. I proceeded similarly for the Brown clustering algorithm, selecting two clusters for NN1 and four for AJO. This can only be approximate, since the choice of acceptable clusters is rather arbitrary, and the BNC tags are not perfectly accurate, but the results are quite clear; for words that occur 5 times or less the CDC algorithm is clearly more accurate. Evaluation is in general difficult with unsupervised learning algorithms. Previous authors have relied on both informal evaluations of the plausibility of the classes produced, and more formal statistical methods. Comparison against existing tag-sets is not meaningful — one set of tags chosen by linguists would score very badly against another without this implying any fault as there is no 'gold standard'. I therefore chose to use an objective statistical measure, the perplexity of a very simple finite state model, to compare the tags generated with this clustering technique against the BNC tags, which uses the CLAWS-4 tag set (Leech et al., 1994) which had 76 tags. I tagged 12 million words of BNC text with the 77 tags, assigning each word to the cluster with the highest a posteriori probability given its prior cluster distribution and its context. I then trained 2nd-order Markov models (equivalently class trigram models) on the original BNC tags, on the outputs from my algorithm (CDC), and for comparision on the output from the Brown algorithm. The perplexities on held-out data are shown in table 3. As can be seen, the perplexity is lower with the model trained on data tagged with the new algorithm. This does not imply that the new tagset is better; it merely shows that it is capturing statistical significant generalisations. In absolute terms the perplexities are rather high; I deliberately chose a rather crude model without backing off and only the minimum amount of smoothing, which I felt might sharpen the contrast.
6 Conclusion. The work of Chater and Finch can be seen as similar to the work presented here given an independence assumption. We can model the context distribution as being the product of independent distributions for each relative position; in this case the KL divergence is the sum of the divergences for each independent distribution. This independence assumption is most clearly false when the word is ambiguous; this perhaps explains the poor performance of these algorithms with ambiguous words. The new algorithm currently does not use information about the orthography of the word, an important source of information. In future work, I will integrate this with a morphology-learning program. I am currently applying this approach to the induction of phrase structure rules, and preliminary experiments have shown encouraging results. In summary, the new method avoids the limitations of other approaches, and is better suited to integration into a complete unsupervised language acquisition system.