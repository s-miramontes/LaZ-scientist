1 Introduction. The huge amount of information available on the web has led to a flurry of research on methods for automatic creation of structured information from large unstructured text corpora. The challenge is to create as much information as possible while providing as little input as possible. A lot of this research is based on the initial insight (Hearst, 1992) that certain lexical patterns (‘X is a country’) can be exploited to automatically generate hyponyms of a specified word. Subsequent work (to be discussed in detail below) extended this initial idea along two dimensions. One objective was to require as small a userprovided initial seed as possible. Thus, it was observed that given one or more such lexical patterns, a corpus could be used to generate examples of hyponyms that could then, in turn, be exploited to generate more lexical patterns. The larger and more reliable sets of patterns thus generated resulted in larger and more precise sets of hyponyms and vice versa. The initial step of the resulting alternating bootstrap process – the user-provided input – could just as well consist of examples of hyponyms as of lexical patterns. A second objective was to extend the information that could be learned from the process beyond hyponyms of a given word. Thus, the approach was extended to finding lexical patterns that could produce synonyms and other standard lexical relations. These relations comprise all those words that stand in some known binary relation with a specified word. In this paper, we introduce a novel extension of this problem: given a particular concept (initially represented by two seed words), discover relations in which it participates, without specifying their types in advance. We will generate a concept class and a variety of natural binary relations involving that class. An advantage of our method is that it is particularly suitable for web mining, even given the restrictions on query amounts that exist in some of today’s leading search engines. The outline of the paper is as follows. In the next section we will define more precisely the problem we intend to solve. In section 3, we will consider related work. In section 4 we will provide an overview of our solution and in section 5 we will consider the details of the method. In section 6 we will illustrate and evaluate the results obtained by our method. Finally, in section 7 we will offer some conclusions and considerations for further work. 2 Problem Definition 1992; Pantel et al, 2004), synonymy (Roark and In several studies (e.g., Widdows and Dorow, 2002; Charniak, 1998; Widdows and Dorow, 2002; DaviPantel et al, 2004; Davidov and Rappoport, 2006) dov and Rappoport, 2006) and meronymy (Berland it has been shown that relatively unsupervised and and Charniak, 1999). language-independent methods could be used to In addition to these basic types, several studgenerate many thousands of sets of words whose ies deal with the discovery and labeling of more semantics is similar in some sense. Although ex- specific relation sub-types, including inter-verb reamination of any such set invariably makes it clear lations (Chklovski and Pantel, 2004) and nounwhy these words have been grouped together into compound relationships (Moldovan et al, 2004). a single concept, it is important to emphasize that Studying relationships between tagged named enthe method itself provides no explicit concept defi- tities, (Hasegawa et al, 2004; Hassan et al, 2006) nition; in some sense, the implied class is in the eye proposed unsupervised clustering methods that asof the beholder. Nevertheless, both human judgment sign given (or semi-automatically extracted) sets of and comparison with standard lists indicate that the pairs into several clusters, where each cluster corregenerated sets correspond to concepts with high pre- sponds to one of a known relationship type. These cision. studies, however, focused on the classification of We wish now to build on that result in the fol- pairs that were either given or extracted using some lowing way. Given a large corpus (such as the web) supervision, rather than on discovery and definition and two or more examples of some concept X, au- of which relationships are actually in the corpus. tomatically generate examples of one or more rela- Several papers report on methods for using the tions R C X x Y , where Y is some concept and R web to discover instances of binary relations. Howis some binary relationship between elements of X ever, each of these assumes that the relations themand elements of Y . selves are known in advance (implicitly or explicWe can think of the relations we wish to gener- itly) so that the method can be provided with seed ate as bipartite graphs. Unlike most earlier work, patterns (Agichtein and Gravano, 2000; Pantel et al, the bipartite graphs we wish to generate might be 2004), pattern-based rules (Etzioni et al, 2004), relaone-to-one (for example, countries and their capi- tion keywords (Sekine, 2006), or word pairs exemtals), many-to-one (for example, countries and the plifying relation instances (Pasca et al, 2006; Alfonregions they are in) or many-to-many (for example, seca et al, 2006; Rosenfeld and Feldman, 2006). countries and the products they manufacture). For a In some recent work (Strube and Ponzetto, 2006), given class X, we would like to generate not one but it has been shown that related pairs can be generpossibly many different such relations. ated without pre-specifying the nature of the relaThe only input we require, aside from a corpus, tion sought. However, this work does not focus on is a small set of examples of some class. However, differentiating among different relations, so that the since such sets can be generated in entirely unsuper- generated relations might conflate a number of disvised fashion, our challenge is effectively to gener- tinct ones. ate relations directly from a corpus given no addi- It should be noted that some of these papers utilize tional information of any kind. The key point is that language and domain-dependent preprocessing inwe do not in any manner specify in advance what cluding syntactic parsing (Suchanek et al, 2006) and types of relations we wish to find. named entity tagging (Hasegawa et al, 2004), while 3 Related Work others take advantage of handcrafted databases such As far as we know, no previous work has directly as WordNet (Moldovan et al, 2004; Costello et al, addressed the discovery of generic binary relations 2006) and Wikipedia (Strube and Ponzetto, 2006). in an unrestricted domain without (at least implic- Finally, (Turney, 2006) provided a pattern disitly) pre-specifying relationship types. Most related tance measure which allows a fully unsupervised work deals with discovery of hypernymy (Hearst, measurement of relational similarity between two 233 pairs of words; however, relationship types were not discovered explicitly. 4 Outline of the Method 5.1 Generalizing the seed We will use two concept words contained in a con- The first step is to take the seed, which might concept class C to generate a collection of distinct re- sist of as few as two concept words, and generate lations in which C participates. In this section we many (ideally, all, when the concept is a closed set offer a brief overview of our method. of words) members of the class to which they beStep 1: Use a seed consisting of two (or more) ex- long. We do this as follows, essentially implementample words to automatically obtain other examples ing a simplified version of the method of Davidov that belong to the same class. Call these concept and Rappoport (2006). For any pair of seed words words. (For instance, if our example words were Si and Sj, search the corpus for word patterns of the France and Angola, we would generate more coun- form SiHSj, where H is a high-frequency word in try names.) the corpus (we used the 100 most frequent words Step 2: For each concept word, collect instances in the corpus). Of these, we keep all those patof contexts in which the word appears together with terns, which we call symmetric patterns, for which one other content word. Call this other word a tar- SjHSi is also found in the corpus. Repeat this proget word for that concept word. (For example, for cess to find symmetric patterns with any of the strucFrance we might find ‘Paris is the capital of France’. tures HSHS, SHSH or SHHS. It was shown in Paris would be a target word for France.) (Davidov and Rappoport, 2006) that pairs of words Step 3: For each concept word, group the contexts that often appear together in such symmetric patin which it appears according to the target word that terns tend to belong to the same class (that is, they appears in the context. (Thus ‘X is the capital of Y ’ share some notable aspect of their semantics). Other would likely be grouped with ‘Y ’s capital is X’.) words in the class can thus be generated by searchStep 4: Identify similar context groups that ap- ing a sub-corpus of documents including at least two pear across many different concept words. Merge concept words for those words X that appear in a these into a single concept-word-independent clus- sufficient number of instances of both the patterns ter. (The group including the two contexts above SiHX and XHSi, where Si is a word in the class. would appear, with some variation, for other coun- The same can be done for the other three pattern tries as well, and all these would be merged into structures. The process can be bootstrapped as more a single cluster representing the relation capital- words are added to the class. of(X,Y).) Note that our method differs from that of Davidov Step 5: For each cluster, output the relation con- and Rappoport (2006) in that here we provide an inisisting of all <concept word, target word> pairs that tial seed pair, representing our target concept, while appear together in a context included in the cluster. there the goal is grouping of as many words as pos(The cluster considered above would result in a set sible into concept classes. The focus of our paper is of pairs consisting of a country and its capital. Other on relations involving a specific concept. clusters generated by the same seed might include 5.2 Collecting contexts countries and their languages, countries and the re- For each concept word S, we search the corpus for gions in which they are located, and so forth.) distinct contexts in which S appears. (For our pur5 Details of the Method poses, a context is a window with exactly five words In this section we consider the details of each of or punctuation marks before or after the concept the above-enumerated steps. It should be noted word; we choose 10,000 of these, if available.) We that each step can be performed using standard web call the aggregate text found in all these context winsearches; no special pre-processed corpus is re- dows the S-corpus. quired. From among these contexts, we choose all pat234 terns of the form H1SH2XH3 or H1XH2SH3, where: with frequency above f2 in the S-corpus. We want H2 to consist mainly of words common in the context of S in order to restrict patterns to those that are somewhat generic. Thus, in the context of countries we would like to retain words like capital while eliminating more specific words that are unlikely to express generic patterns. We used f2 = 100 occurrences per million words (there is room here for automatic optimization, of course). If S is in fact related to X in some way, there might be a number of S-patterns that capture this relationship. For each X, we group all the S-patterns that have X as a target. (Note that two S-patterns with two different targets might be otherwise identical, so that essentially the same pattern might appear in two different groups.) We now merge groups with large (more than 2/3) overlap. We call the resulting groups, S-groups. If the S-patterns in a given S-group actually capture some relationship between S and the target, then one would expect that similar groups would appear for a multiplicity of concept words S. Suppose that we have S-groups for three different concept words S such that the pairwise overlap among the three groups is more than 2/3 (where for this purpose two patterns are deemed identical if they differ only at S and X). Then the set of patterns that appear in two or three of these S-groups is called a cluster core. We now group all patterns in other S-groups that have an overlap of more than 2/3 with the cluster core into a candidate pattern pool P. The set of all patterns in P that appear in at least two S-groups (among those that formed P) pattern cluster. A pattern cluster that has patterns instantiated by at least half of the concept words is said to represent a relation. A relation consists of pairs (S, X) where S is a concept word and X is the target of some S-pattern in a given pattern cluster. Note that for a given S, there might be one or many values of X satisfying the relation. As a final refinement, for each given S, we rank all such X according to pointwise mutual information with S and retain only the highest 2/3. If most values of S have only a single corresponding X satisfying the relation and the rest have none, we try to automatically fill in the missing values by searching the corpus for relevant S-patterns for the missing values of S. (In our case the corpus is the web, so we perform additional clarifying queries.) Finally, we delete all relations in which all concept words are related to most target words and all relations in which the concept words and the target words are identical. Such relations can certainly be of interest (see Section 7), but are not our focus in this paper. In our implementation we use the Google search engine. Google restricts individual users to 1,000 queries per day and 1,000 pages per query. In each stage we conducted queries iteratively, each time downloading all 1,000 documents for the query. In the first stage our goal was to discover symmetric relationships from the web and consequently discover additional concept words. For queries in this stage of our algorithm we invoked two requirements. First, the query should contain at least two concept words. This proved very effective in reducing ambiguity. Thus of 1,000 documents for the erated relation, authoritative resources must be marquery bass, 760 deal with music, while if we add to shaled as a gold standard. For purposes of evaluthe query a second word from the intended concept ation, we ran our algorithm on three representative (e.g., barracuda), then none of the 1,000 documents domains – countries, fish species and star consteldeal with music and the vast majority deal with fish, lations – and tracked down gold standard resources as intended. (encyclopedias, academic texts, informative webSecond, we avoid doing overlapping queries. To sites, etc) for the bulk of the relations generated in do this we used Google’s ability to exclude from each domain. search results those pages containing a given term This choice of domains allowed us to explore (in our case, one of the concept words). different aspects of algorithmic behavior. Country We performed up to 300 different queries for in- and constellation domains are both well defined and dividual concepts in the first stage of our algorithm. closed domains. However they are substantially difIn the second stage, we used web queries to as- ferent. semble S-corpora. On average, about 1/3 of the con- Country names is a relatively large domain which cept words initially lacked sufficient data and we has very low lexical ambiguity, and a large number performed up to twenty additional queries for each of potentially useful relations. The main challenge rare concept word to fill its corpus. in this domain was to capture it well. In the last stage, when clusters are constructed, Constellation names, in contrast, are a relatively we used web queries for filling missing pairs of one- small but highly ambiguous domain. They are used to-one or several-to-several relationships. The to- in proper names, mythology, names of entertainment tal number of filling queries for a specific concept facilities etc. Our evaluation examined how well the was below 1,000, and we needed only the first re- algorithm can deal with such ambiguity. sults of these queries. Empirically, it took between The fish domain contains a very high number of 0.5 to 6 day limits (i.e., 500–6,000 queries) to ex- members. Unlike countries, it is a semi-open nontract relationships for a concept, depending on its homogenous domain with a very large number of size (the number of documents used for each query subclasses and groups. Also, unlike countries, it was at most 100). Obviously this strategy can be does not contain many proper nouns, which are emimproved by focused crawling from primary Google pirically generally easier to identify in patterns. So hits, which can drastically reduce the required num- the main challenge in this domain is to extract unber of queries. blurred relationships and not to diverge from the do6 Evaluation main during the concept acquisition phase. In this section we wish to consider the variety of re- We do not show here all-to-all relationships such lations that can be generated by our method from a as fish parts (common to all or almost all fish), begiven seed and to measure the quality of these rela- cause we focus on relationships that separate betions in terms of their precision and recall. tween members of the concept class, which are With regard to precision, two claims are being harder to acquire and evaluate. made. One is that the generated relations correspond 6.1 Countries to identifiable relations. The other claim is that to Our seed consisted of two country names. The inthe extent that a generated relation can be reason- tended result for the first stage of the algorithm ably identified, the generated pairs do indeed belong was a list of countries. There are 193 countries in to the identified relation. (There is a small degree of the world (www.countrywatch.com) some of which circularity in this characterization but this is proba- have multiple names so that the total number of bly the best we can hope for.) commonly used country names is 243. Of these, As a practical matter, it is extremely difficult to 223 names (comprising 180 countries) are characmeasure precision and recall for relations that have ter strings with no white space. Since we consider not been pre-determined in any way. For each gen- only single word names, these 223 are the names we 236 hope to capture in this stage. Using the seed words France and Angola, we obtained 202 country names (comprising 167 distinct countries) as well as 32 other names (consisting mostly of names of other geopolitical entities). Using the list of 223 single word countries as our gold standard, this gives precision of 0.90 and recall of 0.86. (Ten other seed pairs gave results ranging in precision: 0.86-0.93 and recall: 0.79-0.90.) The second part of the algorithm generated a set of 31 binary relations. Of these, 25 were clearly identifiable relations many of which are shown in Table 1. Note that for three of these there are standard exhaustive lists against which we could measure both precision and recall; for the others shown, sources were available for measuring precision but no exhaustive list was available from which to measure recall, so we measured coverage (the number of countries for which at least one target concept is found as related). Another eleven meaningful relations were generated for which we did not compute precision numbers. These include celebrity-from, animal-of, lakein, borders-on and enemy-of. (The set of relations generated by other seed pairs differed only slightly from those shown here for France and Angola.) In our second experiment, our seed consisted of two fish species, barracuda and bluefish. There are 770 species listed in WordNet of which 447 names are character strings with no white space. The first stage of the algorithm returned 305 of the species listed in Wordnet, another 37 species not listed in Wordnet, as well as 48 other names (consisting mostly of other sea creatures). The second part of the algorithm generated a set of 15 binary relations all of which are meaningful. Those for which we could find some gold standard are listed in Table 2. Other relations generated include served-with, bait-for, food-type, spot-type, and gill-type. Our seed consisted of two constellation names, Orion and Cassiopeia. There are 88 standard constellations (www.astro.wisc.edu) some of which have multiple names so that the total number of commonly used constellations is 98. Of these, 87 names (77 constellations) are strings with no white space. The first stage of the algorithm returned 81 constellation names (77 distinct constellations) as well as 38 other names (consisting mostly of names of individual stars). Using the list of 87 single word constellation names as our gold standard, this gives precision of 0.68 and recall of 0.93. The second part of the algorithm generated a set of ten binary relations. Of these, one concerned travel and entertainment (constellations are quite popular as names of hotels and lounges) and another three were not interesting. Apparently, the requirement that half the constellations appear in a relation limited the number of viable relations since many constellations are quite obscure. The six interesting relations are shown in Table 3 along with precision and coverage.
7 Discussion. In this paper we have addressed a novel type of problem: given a specific concept, discover in fully unsupervised fashion, a range of relations in which it participates. This can be extremely useful for studying and researching a particular concept or field of study. As others have shown as well, two concept words can be sufficient to generate almost the entire class to which the words belong when the class is welldefined. With the method presented in this paper, using no further user-provided information, we can, for a given concept, automatically generate a diverse collection of binary relations on this concept. These relations need not be pre-specified in any way. Results on the three domains we considered indicate that, taken as an aggregate, the relations that are generated for a given domain paint a rather clear picture of the range of information pertinent to that domain. Moreover, all this was done using standard search engine methods on the web. No language-dependent tools were used (not even stemming); in fact, we reproduced many of our results using Google in Russian. The method depends on a number of numerical parameters that control the subtle tradeoff between quantity and quality of generated relations. There is certainly much room for tuning of these parameters. The concept and target words used in this paper are single words. Extending this to multiple-word expressions would substantially contribute to the applicability of our results. In this research we effectively disregard many relationships of an all-to-all nature. However, such relationships can often be very useful for ontology construction, since in many cases they introduce strong connections between two different concepts. Thus, for fish we discovered that one of the all-toall relationships captures a precise set of fish body parts, and another captures swimming verbs. Such relations introduce strong and distinct connections between the concept of fish and the concepts of fishbody-parts and swimming. Such connections may be extremely useful for ontology construction.