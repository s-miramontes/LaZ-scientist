1 Introduction. We present a statistical method for determining pronoun anaphora. This program differs from earlier work in its almost complete lack of hand-crafting, relying instead on a very small corpus of Penn Wall Street Journal Tree-bank text (Marcus et al., 1993) that has been marked with co-reference information. The first sections of this paper describe this program: the probabilistic model behind it, its implementation, and its performance. The second half of the paper describes a method for using (portions of) the aforementioned program to learn automatically the typical gender of English words, information that is itself used in the pronoun resolution program. In particular, the scheme infers the gender of a referent from the gender of the pronouns that refer to it and selects referents using the pronoun anaphora program. We present some typical results as well as the more rigorous results of a blind evaluation of its output.
2 A Probabilistic Model. There are many factors, both syntactic and semantic, upon which a pronoun resolution system relies. (Mitkov (1997) does a detailed study on factors in anaphora resolution.) We first discuss the training features we use and then derive the probability equations from them. The first piece of useful information we consider is the distance between the pronoun and the candidate antecedent. Obviously the greater the distance the lower the probability. Secondly, we look at the syntactic situation in which the pronoun finds itself. The most well studied constraints are those involving reflexive pronouns. One classical approach to resolving pronouns in text that takes some syntactic factors into consideration is that of Hobbs (1976). This algorithm searches the parse tree in a leftto-right, breadth-first fashion that obeys the major reflexive pronoun constraints while giving a preference to antecedents that are closer to the pronoun. In resolving inter-sentential pronouns, the algorithm searches the previous sentence, again in left-to-right, breadth-first order. This implements the observed preference for subject position antecedents. Next, the actual words in a proposed nounphrase antecedent give us information regarding the gender, number, and animaticity of the proposed referent. For example: Marie Giraud carries historical significance as one of the last women to be executed in France. She became an abortionist because it enabled her to Here it is helpful to recognize that &quot;Marie&quot; is probably female and thus is unlikely to be referred to by &quot;he&quot; or &quot;it&quot;. Given the words in the proposed antecedent we want to find the probability that it is the referent of the pronoun in question. We collect these probabilities on the training data, which are marked with reference links. The words in the antecedent sometimes also let us test for number agreement. Generally, a singular pronoun cannot refer to a plural noun phrase. so that in resolving such a pronoun any plural candidates should be ruled out. However a singular noun phrase can be the referent of a plural pronoun, as illustrated by the following example: &quot;I think if I tell Via corn I need more time, they will take 'Cosby' across the street,&quot; says the general manager of a network affiliate. It is also useful to note the interaction between the head constituent of the pronoun p and the antecedent. For example: A Japanese company might make television picture tubes in Japan, assemble the TV sets in Malaysia and export them to Indonesia. Here we would compare the degree to which each possible candidate antecedent (A Japanese company, television picture tubes, Japan, TV sets, and Malaysia in this example) could serve as the direct object of &quot;export&quot;. These probabilities give us a way to implement selectional restriction. A canonical example of selectional restriction is that of the verb &quot;eat&quot;, which selects food as its direct object. In the case of &quot;export&quot; the restriction is not as clearcut. Nevertheless it can still give us guidance on which candidates are more probable than others. The last factor we consider is referents' mention count. Noun phrases that are _mentioned repeatedly are preferred. The training corpus is marked with the number of times a referent has been mentioned up to that point in the story. Here we are concerned with the probability that a proposed antecedent is correct given that it has been repeated a certain number of times. In effect, we use this probability information to identify the topic of the segment with the belief that the topic is more likely to be referred to by a pronoun. The idea is similar to that used in the centering approach (Brennan et al., 1987) where a continued topic is the highest-ranked candidate for pronominalization. Given the above possible sources of information, we arrive at the following equation. where F(p) denotes a function from pronouns to their antecedents: where A(p) is a random variable denoting the referent of the pronoun p and a is a proposed antecedent. In the conditioning events, h is the head constituent above p,i/ir is the list of candidate antecedents to be considered, t is the type of phrase of the proposed antecedent (always a noun-phrase in this study), 1 is the type of the head constituent, sp describes the syntactic structure in which p appears, cr specifies the distance of each antecedent from p and M is the number of times the referent is mentioned. Note that W, d, and M are vector quantities in which each entry corresponds to a possible antecedent. When viewed in this way, a can be regarded as an index into these vectors that specifies which value is relevant to the particular choice of antecedent. This equation is decomposed into pieces that correspond to all the above factors but are more statistically manageable. The decomposition makes use of Bayes' theorem and is based on certain independence assumptions discussed below. Equation (1) is simply an application of Bayes' rule. The denominator is eliminated in the usual fashion, resulting in equation (2). Selectively applying the chain rule results in equations (3) and (4). In equation (4), the term P(h,t,lia, sp, cr) is the same for every antecedent and is thus removed. Equation (6) follows when we break the last component of (5) into two probability distributions. In equation (7) we make the following independence assumptions: P(sp, data, = P (s p, data) Then we combine so and dc, into one variable dH , Hobbs distance, since the Hobbs algorithm takes both the syntax and distance into account. Since_ 1-4.7 is a vector, we need to normalize P(W1h,t,l, a) to obtain the probability of each element in the vector. It is reasonable to assume that the antecedents in W are independent of each other; in other words, P(Wa-4-11W0i h, t, I, a) = P (wp+ilh, t, I , a). Thus, where Now we arrive at the final equation for computing the probability of each proposed antecedent: We obtain P(dHla) by running the Hobbs algorithm on the training data. Since the training corpus is tagged with reference information, the probability P(pluic,) is easily obtained. In building a statistical parser for the Penn Tree-bank various statistics have been collected P (with, t, 1, a) = P(wolh. t, I) if i = a Then we have, P(W. ih, t, I, a) = P(wilt) P(walh, t, I) . .P(tonit) To get the probability for each candidate, we divide the above product by: (Charniak, 1997), two of which are P(walh, t, I) and P(wolt, 1). To avoid the sparse-data problem, the heads h are clustered according to how they behave in P(walh, t, I). The probability of wa is then computed on the basis of h's cluster c(h). Our corpus also contains referents' repetition information, from which we can directly compute P(aIrna). The four components in equation (8) can be estimated in a reasonable fashion. The system computes this product and returns the antecedent wo for a pronoun p that maximizes this probability. More formally, we want the program to return our antecedent function F(p), where
3 The Implementation. We use a small portion of the Penn Wall Street Journal Tree-bank as our training corpus. From this data, we collect the three statistics detailed in the following subsections. The Hobbs algorithm makes a few assumptions about the syntactic trees upon which it operates that are not satisfied by the tree-bank trees that form the substrate for our algorithm. Most notably, the Hobbs algorithm depends on the existence of an N parse-tree node that is absent from the Penn Tree-bank trees. We have implemented a slightly modified version of Hobbs algorithm for the Tree-bank parse trees. We also transform our trees under certain conditions to meet Hobbs' assumptions as much as possible. We have not, however, been able to duplicate exactly the syntactic structures assumed by Hobbs. Once we have the trees in the proper form (to the degree this is possible) we run Hobbs' algorithm repeatedly for each pronoun until it has proposed n (= 15 in our experiment) candidates. The ith candidate is regarded as occurring at &quot;Hobbs distance&quot; dH = i. Then the probability P(dH = ila) is simply: We use I x I to denote the number of times x is observed in our training set. After we have identified the correct antecedents it is a simple counting procedure to compute P(plwa) where wo is in the correct antecedent for the pronoun p (Note the pronouns are grouped by their gender): When there are multiple relevant words in the antecedent we apply the likelihood test designed by Dunning (1993) on all the words in the candidate NP. Given our limited data, the Dunning test tells which word is the most informative, call it w, and we then use P(Piw.7)• The referents range from being mentioned only once to begin mentioned 120 times in the training examples. Instead of computing the probability for each one of them we group them into &quot;buckets&quot;, so that ma is the bucket for the number of times that a is mentioned. We also observe that the position of a pronoun in a story influences the mention count of its referent. In other words, the nearer the end of the story a pronoun occurs, the more probable it is that its referent has been mentioned several times. We measure position by the sentence number, j. The method to compute this probability is: (We omitted j from equations (1-7) to reduce the notational load.) After collecting the statistics on the training examples, we run the program on the test data. For any pronoun we collect n(= 15 in the experiment) candidate antecedents proposed by Hobbs' algorithm. It is quite possible that a word appears in the test data that the program never saw in the training data and fow which it hence has no P(plwo) probability. In this case I wain the antecedent for p I we simply use the prior probability of the pronoun P(p). From the parser project mentioned earlier, we obtain the probability P(walhl Finally, we extract the mention count number associated with each candidate NP, which is used to obtain P(ajma). The four probabilities are multiplied together. The procedure is repeated for each proposed NP in W and the one with the highest combined probability is selected as the antecedent.
4 The Experiment. The algorithm has two modules. One collects the statistics on the training corpus required by equation (8) and the other uses these probabilities to resolve pronouns in the test corpus. Our data consists of 93,931 words (3975 sentences) and contains 2477 pronouns, 1371 of which are singular (he, she and it). The corpus is manually tagged with reference indices and referents' repetition numbers. The result presented here is the accuracy of the program in finding antecedents for he, she, and it and their various forms (e.g. him, his, himself, etc.) The cases where &quot;it&quot; is merely a dummy subject in a cleft sentence (example 1) or has conventional unspecified referents (example 2) are excluded from computing the precision: We performed a ten-way cross-validation where we reserved 10% of the corpus for testing and used the remaining 90% for training. Our preliminary results are shown in the last line of Table 1. We are also interested in finding the relative importance of each probability (i.e. each of the four factors in equation (8) in pronoun resolution. To this end, we ran the program &quot;incrementally&quot;, each time incorporating one more probability. The results are shown in Table 1 (all obtained from cross-validation). The last column of Table 1 contains the p-values for testing the statistical significance of each improvement. Due to relatively large differences between Tree-bank parse trees and Hobbs' trees, our Hobbs' implementation does not yield as high an accuracy as it would have if we had had perfect Hobbs' tree representations. Since the Hobbs' algorithm serves as the base of our scheme, we expect the accuracy to be much higher with more accurately transformed trees. We also note that the very simple model that ignores syntax and takes the last mentioned noun-phrase as the referent performs quite a bit worse, about 43% correct. This indicates that syntax does play a very important role in anaphora resolution. We see a significant improvement after the word knowledge is added to the program. The P(plwa) probability gives the system information about gender and animaticity. The contribution of this factor is quite significant, as can be seen from Table 1. The impact of this probability can be seen more clearly from another experiment in which we tested the program (using just Hobbs distance and gender information) on the training data. Here the program can be thought of having &quot;perfect&quot; gender/animaticity knowledge. We obtained a success rate of 89.3%. Although this success rate overstates the effect, it is a clear indication that knowledge of a referent's gender and animaticity is essential to anaphora resolution. We hoped that the knowledge about the governing constituent would, like gender and animaticity, make a large contribution. To our surprise, the improvement is only about 2.2%. This is partly because selection restrictions are not clearcut in many cases. Also, some head verbs are too general to restrict the selection of any NP. Examples are &quot;is&quot; and &quot;has&quot;, which appear frequently in Wall Street Journal: these verbs are not &quot;selective&quot; enough and the a.ssociated probability is not strong enough to rule out erroneous candidates. Sparse data also causes a problem in this statistic. Consequently, we observe a relatively small enhancement to the system. The mention information gives the system some idea of the story's focus. The more frequently an entity is repeated, the more likely it is to be the topic of the story and thus to be a candidate for pronominalization. Our results show that this is indeed the case. References by pronouns are closely related to the topic or the center of the discourse. NP repetition is one simple way of approximately identifying the topic. The more accurately the topic of a segment can be identified, the higher the success rate we expect an anaphora resolution system can achieve.
5 Unsupervised Learning of Gender Information. The importance of gender information as revealed in the previous experiments caused us to consider automatic methods for estimating the probability that nouns occurring in a large corpus of English text deonote inanimate, masculine or feminine things. The method described here is based on simply counting co-occurrences of pronouns and noun phrases, and thus can employ any method of analysis of the text stream that results in referent/pronoun pairs (cf. (Hatziv-assiloglou and McKeown, 1997) for another application in which no explicit indicators are available in the stream). We present two very simple methods for finding referent/pronoun pairs, and also give an application of a salience statistic that can indicate how confident we should be about the predictions the method makes. Following this, we show the results of applying this method to the 21-million-word 1987 Wall Street Journal corpus using two different pronoun reference strategies of varying sophistication, and evaluate their performance using honorifics as reliable gender indicators. The method is a very simple mechanism for harvesting the kind of gender information present in discourse fragments like &quot;Kim slept. She slept for a long time.&quot; Even if Kim's gender was unknown before seeing the first sentence, after the second sentence, it is known. The probability that a referent is in a particular gender class is just the relative frequency with which that referent is referred to by a pronoun p that is part of that gender class. That is, the probability of a referent ref being in gender class gc, is P(ref E gc,) (9) I refs to refwith p E gci I El refs to ref with p E gci In this work we have considered only three gender classes, masculine, feminine and inanimate, which are indicated by their typical pronouns, HE, SHE, and IT. However, a variety of pronouns indicate the same class: Plural pronouns like &quot;they&quot; and &quot;us&quot; reveal no gender information about their referent and consequently aren't useful, although this might be a way to learn pluralization in an unsupervised manner. In order to gather statistics on the gender of referents in a corpus, there must be some way of identifying the referents. In attempting to bootstrap lexical information about referents' gender, we consider two strategies, both completely blind to any kind of semantics. One of the most naive pronoun reference strategies is the &quot;previous noun&quot; heuristic. On the intuition pronouns closely follow their referents, this heuristic simply keeps track of the last noun seen and submits that noun as the referent of any pronouns following. This strategy is certainly simple-minded but, as noted earlier, it achieves an accuracy of 43%. In the present system, a statistical parser is used (see (Charniak, 1997)) simply as a tagger. This apparent parser overkill is a control to ensure that the part-of-speech tags assigned to words are the same when we use the previous noun heuristic and the Hobbs algorithm, to which we wish to compare the previous noun method. In fact, the only part-of-speech tags necessary are those indicating nouns and pronouns. Obviously a much superior strategy would be to apply the anaphora-resolution strategy from previous sections to finding putative referents. However, we chose to use only the Hobbs distance portion thereof. We do not use the &quot;mention&quot; probabilities P(alma), as they are not given in the unmarked text. Nor do we use the gender/animiticity information gathered from the much smaller hand-marked text, both because we were interested in seeing what unsupervised learning could accomplish, and because we were concerned with inheriting strong biases from the limited hand-marked data. Thus our second method of finding the pronoun/noun co-occurrences is simply to parse the text and then assume that the noun-phrase at Hobbs distance one is the antecedent. Given a pronoun resolution method and a corpus, the result is a set of pronoun/referent pairs. By collating by referent and abstracting away to the gender classes of pronouns, rather than individual pronouns, we have the relative frequencies with which a given referent is referred to by pronouns of each gender class. We will say that the gender class for which this relative frequency is the highest is the gender class to which the referent most probably belongs. However, any syntax-only pronoun resolution strategy will be wrong some of the time - these methods know nothing about discourse boundaries, intentions, or real-world knowledge. We would like to know, therefore, whether the pattern of pronoun references that we observe for a given referent is the result of our supposed &quot;hypothesis about pronoun reference&quot; - that is, the pronoun reference strategy we have provisionally adopted in order to gather statistics or whether the result of some other unidentified process. This decision is made by ranking the referents by log-likelihood ratio, termed salience, for each referent. The likelihood ratio is adapted from Dunning (1993, page 66) and uses the raw frequencies of each pronoun class in the corpus as the null hypothesis, Pr(gc01) as well as Pr(ref E gci) from equation 9. HP(ref E gC;)1 ref gc■ Making the unrealistic simplifying assumption that references of one gender class are completely independent of references for another classes', the likelihood function in this case is just the product over all classes of the probabilities of each class of reference to the power of the number of observations of this class.
6 Evaluation. We ran the program on 21 million words of Wall Street Journal text. One can judge the program informally by simply examining the results and determining if the program's gender decisions are correct (occasionally looking at the text for difficult cases). Figure 1 shows the 43 noun phrases with the highest salience figures (run using the Hobbs algorithm). An examination of these show that all but three are correct. (The three mistakes are &quot;husband,&quot; &quot;wife,&quot; and &quot;years.&quot; We return to the significance of these mistakes later.) As a measure of the utility of these results, we also ran our pronoun-anaphora program with these statistics added. This achieved an accuracy rate of 84.2%. This is only a small improvement over what was achieved without the data. We believe, however, that there are ways to improve the accuracy of the learning method and thus increase its influence on pronoun anaphora resolution. Finally we attempted a fully automatic direct test of the accuracy of both pronoun methods for gender determination. To that end, we devised a more objective test, useful only for scoring the subset of referents that are names of people. In particular, we assume that any noun-phrase with the honorifics &quot;Mrs.&quot; or &quot;Ms.&quot; may be confidently assigned to gender classes HE, SHE, and SHE, respectively. Thus we compute precision as follows: precision = r attrib. as HE A Mr. E r I + r attrib. as SHE A Mrs. or Ms. E r I Mr., Mrs., or Ms. E r Here r varies over referent types, not tokens. The precision score computed over all phrases containing any of the target honorifics are 66.0% for the last-noun method and 70.3% for the Hobbs method. There are several things to note about these results. First, as one might expect given the already noted superior performance of the Hobbs scheme over last-noun, Hobbs also performs better at determining gender. Secondly, at first glance,the 70.3% accuracy of the Hobbs method is disappointing, only slightly superior to the 65.3% accuracy of Hobbs at finding correct referents. It might have been hoped that the statistics would make things considerably more accurate. In fact, the statistics do make things considerably more accurate. Figure 2 shows average accuracy as a function of number of references for a given referent. It can be seen that there is a significant improvement with increased referent count. The reason that the average over all referents is so low is that the counts on referents obey Zipf's law, so that the mode -Of the distribution on counts is one. Thus the 70.3% overall accuracy is a mix of relatively high accuracy for referents with counts greater than one, and relatively low accuracy for referents with counts of exactly one.
7 Previous Work. The literature on pronoun anaphora is too extensive to summarize, so we concentrate here on corpus-based anaphora research. Aone and Bennett (1996) present an approach to an automatically trainable anaphora resolution system. They use Japanese newspaper articles tagged with discourse information as training examples for a machine-learning algorithm which is the C4.5 derision-tree algorithm by Quinlan (1993). They train their decision tree using (anaphora, antecedent) pairs together with a set of feature vectors. Among the 66 features are lexical, syntactic, semantic, and positional features. Their Machine Learning-based Resolver (MLR) is trained using decision trees with 1971 anaphoras (excluding those referring to multiple discontinuous antecedents) and they report an average success rate of 74.8%. Mitkov (1997) describes an approach that uses a set of factors as constraints and preferences. The constraints rule out implausible candidates and the preferences emphasize the selection of the most likely antecedent. The system is not entirely &quot;statistical&quot; in that it consists of various types of rule-based knowledge — syntactic, semantic, domain, discourse, and heuristic. A statistical approach is present in the discourse module only where it is used to determine the probability that a noun (verb) phrase is the center of a sentence. The system also contains domain knowledge including the domain concepts, specific list of subjects and verbs, and topic headings. The evaluation was conducted on 133 paragraphs of annotated Computer Science text. The results show an accuracy of 83% for the 512 occurrences of it. Lappin and Leass (1994) report on a (essentially non-statistical) approach that relies on salience measures derived from syntactic structure and a dynamic model of attentional state. The system employs various constraints for NPpronoun non-coreference within a sentence. It also uses person, number, and gender features for ruling out anaphoric dependence of a pronoun on an NP. The algorithm has a sophisticated mechanism for assigning values to several salience parameters and for computing global salience values. A blind test was conducted on manual text containing 360 pronoun occurrences; the algorithm successfully identified the antecedent of the pronoun in 86% of these pronoun occurrences. The addition of a module that contributes statistically measured lexical preferences to the range of factors the algorithm considers improved the performance by 2%.
8 Conclustion and Future Research. We have presented a statistical method for pronominal anaphora that achieves an accuracy of 84.2%. The main advantage of the method is its essential simplicity. Except for implementing the Hobbs referent-ordering algorithm, all other system knowledge is imbedded in tables giving the various component probabilities used in the probability model. We believe that this simplicity of method will translate into comparative simplicity as we improve the method. Since the research described herein we have thought of other influences on anaphora resolution and their statistical correlates. We hope to include some of them in future work. Also, as indicated by the work on unsupervised learning of gender information, there is a growing arsenal of learning techniques to be applied to statistical problems. Consider again the three high-salience words to which our unsupervised learning program assigned incorrect gender: &quot;husband&quot;, &quot;wife&quot;, and &quot;years.&quot; We suspect that had our pronoun-assignment method been able to use the topic information used in the complete method, these might well have been decided correctly. That is, we suspect that &quot;husband&quot;, for example, was decided incorrectly because the topic of the article was the woman, there was a mention of her &quot;husband,&quot; but the article kept on talking about the woman and used the pronoun &quot;she.&quot; While our simple program got confused, a program using better statistics might not have. This too is a topic for future research.
9 Acknowledgements. The authors would like to thank Mark Johnson and other members of the Brown NLP group for many useful ideas and NSF and ONR for support (NSF grants IRI-9319516 and SBR9720368, ONR grant N0014-96-1-0549).