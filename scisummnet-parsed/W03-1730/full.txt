1 Introduction. ICT (Institute of Computing Technology, Chinese Academy of Sciences) participated the First International Chinese Word Segmentation Bakeoff. We have taken six tracks: Academia Sinica closed (ASc), U. Penn Chinese Tree Bank open and closed(CTBo,c), Hong Kong CityU closed (HKc), Peking University open and closed(PKo,c). The structure of this document is as follows. The next section presents the HHMM-based framework of ICTCLAS. Next we detail the operation of six tracks. The following section provides evaluation result and gives further analysis. 2 HHMM-based Chinese lexical analysis
2.1 ICTCLAS Framework. As illustrated in Figure 1, HHMM-based Chinese lexical analysis comprises five levels: atom segmentation, simple and recursive unknown words recognition, class-based segmentation and POS tagging. In the whole frame, class-based segmentation graph, which is a directed graph designed for word segmentation, is an essential intermediate data structure that links disambiguation, unknown words recognition with word segmentation and POS tagging. Atom segmentation, the bottom level of HHMM, is an initial step. Here, atom is defined to be the minimal segmentation unit that cannot be split in any stage. The atom consists of Chinese character, punctuation, symbol string, numeric expression and other non-Chinese char string. Any word is made up of an atom or more. Atom segmentation is to segment original text into atom sequence and it provides pure and simple source for its parent HMM. For instance, a sentence like &quot;2002.9,ICTCLAS (6,nEh:gfFApRtti&quot; (The free source codes of ICTCLAS was distributed in September, 2002) would be segmented as atom sequence &quot;2002.9/,/ICTCLAS/�J/n/Eh/�/q/fF /44M/M/�/&quot;. In this HMM, the original symbol is observation while the atom is state. We skip the detail of operation in that it's a simple application on the basis of HMM. POS tagging and role tagging using Viterbi are also skipped because they are classic application of HMM. Because of paper length limit, unknown words recognition is omitted. Our previous papers (Zhang et al. 2003) gave more Given a word wi, classc i is defined in Figure 2. Suppose ILEXI to be the lexicon size, then the total number of word classes is ILEXI+9. Given the atom sequence A=(al,...an), let W=(wl,...wm) be the words sequence, C= (cl,...cm) be a corresponding class sequence of W, and W# be the choice of word segmentation with the maximized probability, respectively. Then, we could get: For a specific atom sequence A, P(A) is a constant and P(W,A)= P(W). So, On the basis of Baye's Theorem, it can be induced that: where co is begin of sentence. For convenience, we often use the negative log probability instead of the proper form. That is:
2.2 Class-based HMM for word segmentation. We apply to word segmentation class-based HMM, which is a generalized approach covering both common words and unknown words. wi iff wi is listed in the segmentation lexicon; PER iff wi is unlisted� personal name; LOC iff wi is unlisted location name; ORG iff wi is unlisted organization name; TIME iff wi is unlisted time expression; NUM iff wi is unlisted numeric expression; STR iffwi is unlisted symbol string; BEG iff beginning of a sentence END iff ending of a sentence OTHER otherwise. � &quot;unlisted&quot; is referred as being outside the lexicon According to the word class definition, if wi is listed in lexicon, then ci is wi, and p(wiIci) is equal to 1.0.Otherwise, p(wiIci) is probability that class ci initially activates wi , and it could be estimated in its child HMM for unknown words recognition. As demonstrated in Figure 3, we provide the process of class-based word segmentation on &quot;Et MT,, 1893' 01&quot; (Mao Ze-Dong was born in the year of 1893). The significance of our method is: it covers the possible ambiguity. Moreover, unknown words, which are recognized in the following steps, can be added into the segmentation graph and proceeded as any other common words. After transformation through class-based HMM, word segmentation becomes single-source shortest paths problem. Hence the best choice W# of word segmentation is easy to find using Djikstra's algorithm.
3 Tracks. Here, we would introduce the operation of some different track. We participate all the closed tracks. As for each closed track, we first extracted all the common words and tokens that appear in the training corpus. Then build the segmentation core lexicons with the words. Those named entity words are classified into different named entities: numeric and time expression, personal names, location names, and transliterated names. According to named entities in the given corpus, we could train both class-based segmentation HMM and rolebased HMM model for unknown word recognition. Therefore, the whole lexical system including unknown word detection is accomplished as shown in Figure 1. We only participate GB code open tracks. Actually, open track is similar to closed one. The only difference is the size of training data set. In Peking University open track, ICTCLAS is trained on sixmonth news corpus that is 5 months more than closed track. The entire corpus is also from Peking University. Except for the additional corpus, we have not employed any other special libraries or other resources. As for CTB open track, we find that it cannot benefit from that 5 month PKU corpus. Actually, PKU standard is very different from CTB one though they seemed similar. Core lexicon extracted from Peking corpus degraded the performance on CTB testing data. Except for some named entity corpus, we could not get any more sources related to CTB standard. Therefore, CTB open track is operated in the similar way as closed track. Before the bakeoff, BIG5-coded word segmentation has never been researched in our institute. Besides the character code, common words and sentence styles are greatly different in China mainland and Taiwan or Hong Kong. Because of time limitation, we have only spent two days on transforming our GB-coded ICTCLAS to BIG5coded lexical analyzer. For each BIG5 closed, we extracted a BIG5-coded core lexicon. Then, the Compared with other systems, ICTCLAS especially GB-coded version is competitive. In both GB-coded closed tracks, ICTCLAS ranked top. ICTCLAS also rank second position in Peking open track. Because of the lack of resources, CTB open track is almost as same as CTB closed track. The final performance in BIG5 track is not very good. As a preliminary BIG-coded system, however, we are satisfied with the result. As is shown in Table 1, It could also be concluded that class-based segmentation HMM is effective. Excepted for CTB, IV Recall is over 97%.
5 Conclusion. Through the first bakeoff, we have learn more about the development in Chinese word segmentation and become more confident on our HHMMbased approach. At the same time, we really find our problems during the evaluation. The bakeoff is interesting and helpful. We look forward to participate forthcoming bakeoff.
6 Acknowledgements. The authors would like to thank Prof. Shiwen Yu of Peking University for the Peking corpus. And we acknowledge our debt to Gang Zou, Dr. Bin Wang, Dr. Jian Sun, Ji-Feng Li, Hao Zhang and other colleagues. Huaping Zhang would especially express gratitude to his graceful girl friend Feifei and her family for their encouragement. We also thank Richard Sproat, Qing Ma, Fei Xia and other SIGHAN colleagues for their elaborate organization and enthusiastic help in the First International Chinese Word Segmentation Bakeoff.