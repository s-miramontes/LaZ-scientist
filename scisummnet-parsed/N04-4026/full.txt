1 Introduction. In recent years, phrase-based systems for statistical machine translation (Och et al., 1999; Koehn et al., 2003; Venugopal et al., 2003) have delivered state-of-the-art performance on standard translation tasks. In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model. The units of translation are blocks, pairs of phrases without internal structure. Fig. 1 shows an example block translation using five Arabic-English blocks . The unigram orientation model is trained from word-aligned training data. During decoding, we view translation as a block segmentation process, where the input sentence is segmented from left to right and the target sentence is generated from bottom to top, one block at a time. A monotone block sequence is generated except for the possibility to swap a pair of neighbor blocks. The novel orientation model is used to assist the block swapping: as shown in section 3, block swapping where only a trigram language model is used to compute probabilities between neighbor blocks fails to improve translation performance. (Wu, 1996; Zens and Ney, 2003) present re-ordering models that make use of a straight/inverted orientation model that is related to our work. Here, we investigate in detail the effect of restricting the word re-ordering to neighbor block swapping only. In this paper, we assume a block generation process that generates block sequences from bottom to top, one block at a time. The score of a successor block depends on its predecessor block and on its orientation relative to the block . In Fig. 1 for example, block is the predecessor of block , and block is the predecessor of block . The target clump of a predecessor block is adjacent to the target clump of a successor block . A right adjacent predecessor block is a block where additionally the source clumps are adjacent and the source clump of occurs to the right of the source clump of . A left adjacent predecessor block is defined accordingly. During decoding, we compute the score of a block sequence with orientation as a product of block bigram scores: where is a block and is a three-valued orientation component linked to the block (the orientation of the predecessor block is ignored.). A block has right orientation ( ) if it has a left adjacent predecessor. Accordingly, a block has left orientation ( ) if it has a right adjacent predecessor. If a block has neither a left or right adjacent predecessor, its orientation is neutral ( ). The neutral orientation is not modeled explicitly in this paper, rather it is handled as a default case as explained below. In Fig. 1, the orientation sequence is , i.e. block and block are generated using left orientation. During decoding most blocks have right orientation , since the block translations are mostly monotone. We try to find a block sequence with orientation that maximizes . The following three types of parameters are used to model the block bigram score in Eq. 1: Two unigram count-based models: and . We compute the unigram probability of a block based on its occurrence count . The blocks are counted from word-aligned training data. We also collect unigram counts with orientation: a left count and a right count . These counts are defined via an enumeration process and are used to define the orientation model : Trigram language model: The block language model score is computed as the probability of the first target word in the target clump of given the final two words of the target clump of . The three models are combined in a log-linear way, as shown in the following section.
2 Orientation Unigram Model. The basic idea of the orientation model can be illustrated as follows: In the example translation in Fig. 1, block occurs to the left of block . Although the joint block consisting of the two smaller blocks and has not been seen in the training data, we can still profit from the fact that block occurs more frequently with left than with right orientation. In our Arabic-English training data, block has been seen times with left orientation, and with right orientation, i.e. it is always involved in swapping. This intuition is formalized using unigram counts with orientation. The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training. We rather enumerate all relevant blocks in some order. Enumeration does not allow us to capture position dependent distortion probabilities, but we can compute statistics about adjacent block predecessors. Our baseline model is the unigram monotone model described in (Tillmann and Xia, 2003). Here, we select blocks from word-aligned training data and unigram block occurrence counts are computed: all blocks for a training sentence pair are enumerated in some order and we count how often a given block occurs in the parallel training data 1. The training algorithm yields a list of about blocks per training sentence pair. In this paper, we make extended use of the baseline enumeration procedure: for each block , we additionally enumerate all its left and right predecessors . No optimal block segmentation is needed to compute the predecessors: for each block , we check for adjacent predecessor blocks that also occur in the enumeration list. We compute left orientation counts as follows: Here, we enumerate all adjacent predecessors of block over all training sentence pairs. The identity of is ignored. is the number of times the block succeeds some right adjacent predecessor block . The ’right’ orientation count is defined accordingly. Note, that in general the unigram count : during enumeration, a block might have both left and right adjacent predecessors, either a left or a right adjacent predecessor, or no adjacent predecessors at all. The orientation count collection is illustrated in Fig. 2: each time a block has a left or right adjacent predecessor in the parallel training data, the orientation counts are incremented accordingly. The decoding orientation restrictions are illustrated in Fig 3: a monotone block sequence with right ( 'We keep all blocks for which and the phrase length is less or equal . No other selection criteria are applied. For the model, we keep all blocks for which . order: for each block , we look for left and right adjacent predecessors . orientation is generated. If a block is skipped e.g. block in Fig 3 by first generating block then block , the block is generated using left orientation . Since the block translation is generated from bottom-to-top, the blocks and do not have adjacent predecessors below them: they are generated by a default model without orientation component. The orientation model is given in Eq. 2, the default model is given in Eq. 3. The block bigram model in Eq. 1 is defined as: where and the orientation of the predecessor is ignored. The are chosen to be optimal on the devtest set (the optimal parameter setting is shown in Table. 1). Only two parameters have to be optimized due to the constraint that the have to sum to . The default model is defined as: . Straightforward normalization over all successor blocks in Eq. 2 and in Eq. 3 is not feasible: there are tens of millions of possible successor blocks . In future work, normalization over a restricted successor set, e.g. for a given source input sentence, all blocks that match this sentence might be useful for both training and decoding. The segmentation model in Eq. 1 naturally prefers translations that make use of a smaller number of blocks which leads to a smaller number of factors in Eq. 1. Using fewer ’bigger’ blocks to carry out the translation generally seems to improve translation performance. Since normalization does not influence the number of blocks used to carry out the translation, it might be less important for our segmentation model. We use a DP-based beam search procedure similar to the one presented in (Tillmann and Xia, 2003). We maximize over all block segmentations with orientation for which the source phrases yield a segmentation of the input sentence. Swapping involves only blocks for which for the successor block , e.g. the blocks and in Fig 1. We tried several thresholds for , and performance is reduced significantly only if . No other parameters are used to control the block swapping. In particular the orientation of the predecessor block is ignored: in future work, we might take into account that a certain predecessor block typically precedes other blocks.
3 Experimental Results. The translation system is tested on an Arabic-to-English translation task. The training data comes from the UN news sources: million Arabic and million English words. The training data is sentence-aligned yielding million training sentence pairs. The Arabic data is romanized, some punctuation tokenization and some number classing are carried out on the English and the Arabic training data. As devtest set, we use testing data provided by LDC, which consists of sentences with Arabic words with reference translations. As a blind test set, we use MT 03 Arabic-English DARPA evaluation test set consisting of sentences with Arabic words. Three systems are evaluated in our experiments: is the baseline block unigram model without re-ordering. Here, monotone block alignments are generated: the blocks have only left predecessors (no blocks are swapped). This is the model presented in (Tillmann and Xia, 2003). For the model, the sentence is translated mostly monotonously, and only neighbor blocks are allowed to be swapped (at most block is skipped). The model allows for the same block swapping as the model, but additionally uses the orientation component described in Section 2: the block swapping is controlled where . The are not optimized separately, rather we define: by the unigram orientation counts. The and models use the block bigram model in Eq. 3: all blocks are generated with neutral orientation , and only two components, the block unigram model and the block bigram score are used. Experimental results are reported in Table 1: three BLEU results are presented for both devtest set and blind test set. Two scaling parameters are set on the devtest set and copied for use on the blind test set. The second column shows the model name, the third column presents the optimal weighting as obtained from the devtest set by carrying out an exhaustive grid search. The fourth column shows BLEU results together with confidence intervals (Here, the word casing is ignored). The block swapping model obtains a statistical significant improvement over the baseline model. Interestingly, the swapping model without orientation performs worse than the baseline model: the word-based trigram language model alone is too weak to control the block swapping: the model is too unrestrictive to handle the block swapping reliably. Additionally, Table 2 presents devtest set example blocks that have actually been swapped. The training data is unsegmented, as can be seen from the first two blocks. The block in the first line has been seen times more often with left than with right orientation. Blocks for which the ratio is bigger than are likely candidates for swapping in our Arabic-English experiments. The ratio itself is not currently used in the orientation model. The orientation model mostly effects blocks where the Arabic and English words are verbs or nouns. As shown in Fig. 1, the orientation model uses the orientation probability for the noun block , and only the default model for the adjective block . Although the noun block might occur by itself without adjective, the swapping is not controlled by the occurrence of the adjective block (which does not have adjacent predecessors). We rather model the fact that a noun block is typically preceded by some block . This situation seems typical for the block swapping that occurs on the evaluation test set.
Acknowledgment. This work was partially supported by DARPA and monitored by SPAWAR under contract No. N66001-99-28916. The paper has greatly profited from discussion with Kishore Papineni and Fei Xia.