1 Introduction. Much of natural language work over the past decade has employed probabilistic finite-state transducers (FSTs) operating on strings. This has occurred somewhat under the influence of speech recognition, where transducing acoustic sequences to word sequences is neatly captured by left-to-right stateful substitution. Many conceptual tools exist, such as Viterbi decoding (Viterbi, 1967) and forward-backward training (Baum and Eagon, 1967), as well as generic software toolkits. Moreover, a surprising variety of problems are attackable with FSTs, from partof-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive reordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful to understand generic algorithms that may support all these tasks and more. (Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language. The Rounds tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees in parallel, with each subtree transformed depending only on its own passed-down state. This class of transducer is often nowadays called R, for “Root-to-frontier” (Gécseg and Steinby, 1984). Rounds uses a mathematics-oriented example of an R transducer, which we summarize in Figure 1. At each point in the top-down traversal, the transducer chooses a production to apply, based only on the current state and the current root symbol. The traversal continues until there are no more state-annotated nodes. Nondeterministic transducers may have several productions with the same left-hand side, and therefore some free choices to make during transduction. An R transducer compactly represents a potentiallyinfinite set of input/output tree pairs: exactly those pairs (T1, T2) for which some sequence of productions applied to T1 (starting in the initial state) results in T2. This is similar to an FST, which compactly represents a set of input/output string pairs, and in fact, R is a generalization of FST. If we think of strings written down vertically, as degenerate trees, we can convert any FST into an R transducer by automatically replacing FST transitions with R productions. and state-based record keeping. It can copy whole subtrees, and transform those subtrees differently. It can also delete subtrees without inspecting them (imagine by analogy an FST that quits and accepts right in the middle of an input string). Variants of R that disallow copying and deleting are called RL (for linear) and RN (for nondeleting), respectively. One advantage of working with tree transducers is the large and useful body of literature about these automata; two excellent surveys are (Gécseg and Steinby, 1984) and (Comon et al., 1997). For example, R is not closed under composition (Rounds, 1970), and neither are RL or F (the “frontier-to-root” cousin of R), but the non-copying FL is closed under composition. Many of these composition results are first found in (Engelfriet, 1975). R has surprising ability to change the structure of an input tree. For example, it may not be initially obvious how an R transducer can transform the English structure S(PRO, VP(V, NP)) into the Arabic equivalent S(V, PRO, NP), as it is difficult to move the subject PRO into position between the verb V and the direct object NP. First, R productions have no lookahead capability—the left-handside of the S production consists only of q S(x0, x1), although we want our English-to-Arabic transformation to apply only when it faces the entire structure q S(PRO, VP(V, NP)). However, we can simulate lookahead using states, as in these productions: By omitting rules like qpro NP → ..., we ensure that the entire production sequence will dead-end unless the first child of the input tree is in fact PRO. So finite lookahead is not a problem. The next problem is how to get the PRO to appear in between the V and NP, as in Arabic. This can be carried out using copying. We make two copies of the English VP, and assign them different states: While general properties of R are understood, there are many algorithmic questions. In this paper, we take on the problem of training probabilistic R transducers. For many language problems (machine translation, paraphrasing, text compression, etc. ), it is possible to collect training data in the form of tree pairs and to distill linguistic knowledge automatically. Our problem statement is: Given (1) a particular transducer with productions P, and (2) a finite training set of sample input/output tree pairs, we want to produce (3) a probability estimate for each production in P such that we maximize the probability of the output trees given the input trees. As organized in the rest of this paper, we accomplish this by intersecting the given transducer with each input/output pair in turn. Each such intersection produces a set of weighted derivations that are packed into a regular tree grammar (Sections 3-5), which is equivalent to a tree substitution grammar. The inside and outside probabilities of this packed derivation structure are used to compute expected counts of the productions from the original, given transducer (Sections 6-7). Section 9 gives a sample transducer implementing a published machine translation model; some readers may wish to skip to this section directly.
2 Trees. TΣ is the set of (rooted, ordered, labeled, finite) trees over alphabet E. An alphabet is just a finite set. TΣ(X) are the trees over alphabet E, indexed by X— the subset of TΣuX where only leaves may be labeled by X. (TΣ(∅) = TΣ.) Leaves are nodes with no children. The nodes of a tree t are identified one-to-one with its paths: pathst ⊂ paths ≡ N* ≡ U i=0 Ni (A0 ≡ {()}). The path to the root is the empty sequence (), and p1 extended by p2 is p1 · p2, where · is concatenation. For p ∈ pathst, rankt(p) is the number of children, or rank, of the node at p in t, and labelt(p) ∈ The paths to X in t are pathst(X) ≡ {p ∈ pathst  |labelt(p) ∈ X}. A frontier is a set of paths f that are pairwise prefix-independent: A frontier of t is a frontier f ⊆ pathst. Fort, s ∈ TΣ(X), p ∈ pathst, t[p ← s] is the substitution of s for p in t, where the subtree at path p is replaced by s. For a frontier f of t, the mass substitution of X for the frontier f in t is written t[p ← X, ∀p ∈ f] and is equivalent to substituting the X(p) for the p serially in any order. in the usual way. For example, the tree t = S(NP, VP(V, NP)) has labelandrankt((2)) = (VP, 2) and labelandrankt((2, 1)) = (V, 0). Fort ∈ TΣ, u ∈ E, u(t) is the tree whose root has label u and whose single child is t. The yield of X in t is yieldt(X), the string formed by reading out the leaves labeled with X in left-to-right order. The usual case (the yield oft) is yieldt ≡ yieldt(E).
3 Regular Tree Grammars. In this section, we describe the regular tree grammar, a common way of compactly representing a potentially infinite set of trees (similar to the role played by the finitestate acceptor FSA for strings). We describe the version (equivalent to TSG (Schabes,1990)) where the generated trees are given weights, as are strings in a WFSA. A weighted regular tree grammar (wRTG) G is a quadruple (E, N, S, P), where E is the alphabet, N is the finite set of nonterminals, S ∈ N is the start (or initial) nonterminal, and P ⊆ N ×TΣ(N)×R+ is the finite set of weighted productions (R+ ≡ {r ∈ R  |r > 0}). A production (lhs, rhs, w) is written lhs →w rhs. Productions whose rhs contains no nonterminals (rhs ∈ TΣ) are called terminal productions, and rules of the form A →w B, for A, B ∈ N are called ǫ-productions, or epsilon productions, and can be used in lieu of multiple initial nonterminals. Figure 2 shows a sample wRTG. This grammar accepts an infinite number of trees. The tree S(NP(DT(the), N(sons)), VP(V(run))) comes out with probability 0.3. We define the binary relation ⇒G (single-step derives in G) on TΣ(N)×(paths×P)*, pairs of trees and derivation histories, which are logs of (location, production used): where (a, h) ⇒G (b, h · (p, (l, r, w))) iff tree b may be derived from tree a by using the rule l →w r to replace the nonterminal leaf l at path p with r. For a derivation history h = ((p1, (l1, r1, w1)), ... , (pn, (l1, r1, w1))), the weight of h is w(h) ≡ 11ni=1 wi, and call h leftmost if The reflexive, transitive closure of ⇒G is written ⇒∗G (derives in G), and the restriction of ⇒∗G to leftmost derivation histories is ⇒L∗ G (leftmost derives in G). The weight of a becoming b in G is wG (a, b) Eh:(a,())⇒c∗(b,h) w(h), the sum of weights of all unique (leftmost) derivations transforming a to b, and the weight of t in G is WG(t) = wG(S,t). The weighted regular tree language produced by G is LG ≡ {(t,w) ∈ TE × IIB+  |WG(t) = w}. For every weighted context-free grammar, there is an equivalent wRTG that produces its weighted derivation trees with yields being the string produced, and the yields of regular tree grammars are context free string languages (Gécseg and Steinby, 1984). What is sometimes called a forest in natural language generation (Langkilde, 2000; Nederhof and Satta, 2002) is a finite wRTG without loops, i.e., ∀n ∈ N(n, ()) ⇒∗G (t, h) =⇒ pathst({n}) = ∅. Regular tree languages are strictly contained in tree sets of tree adjoining grammars (Joshi and Schabes, 1997).
4 Extended-LHS Tree Transducers (xR). Section 1 informally described the root-to-frontier transducer class R. We saw that R allows, by use of states, finite lookahead and arbitrary rearrangement of nonsibling input subtrees removed by a finite distance. However, it is often easier to write rules that explicitly represent such lookahead and movement, relieving the burden on the user to produce the requisite intermediary rules and states. We define xR, a convenience-oriented generalization of weighted R. Because of its good fit to natural language problems, xR is already briefly touched on, though not defined, in (Rounds, 1970). A weighted extended-lhs root-to-frontier tree transducer X is a quintuple (E, A, Q, Qi, R) where E is the input alphabet, and A is the output alphabet, Q is a finite set of states, Qi ∈ Q is the initial (or start, or root) state, and R ⊆ Q × XRPATE × To(Q × paths) × IIB+ is a finite set of weighted transformation rules, written (q, pattern) →w rhs, meaning that an input subtree matching pattern while in state q is transformed into rhs, with Q × paths leaves replaced by their (recursive) transformations. The Q×paths leaves of a rhs are called nonterminals (there may also be terminal leaves labeled by the output tree alphabet A). XRPATE is the set of finite tree patterns: predicate functions f : TE → {0, 1} that depend only on the label and rank of a finite number of fixed paths their input. xR is the set of all such transducers. R, the set of conventional top-down transducers, is a subset of xR where the rules are restricted to use finite tree patterns that depend only on the root: RPATE ≡ {pσ,r(t)} where pσ,r(t) ≡ (labelt(()) = σ ∧ rankt(()) = r). Rules whose rhs are a pure To with no states/paths for further expansion are called terminal rules. Rules of the form (q, pat) →w (q′, ()) are ǫ-rules, or epsilon rules, which substitute state q′ for state q without producing output, and stay at the current input subtree. Multiple initial states are not needed: we can use a single start state Qi, and instead of each initial state q with starting weight w add the rule (Qi, TRUE) →w (q, ()) (where TRUE(t) ≡ 1, ∀t). We define the binary relation ⇒X for xR tranducer X on TE∪o∪Q×(paths×R)∗, pairs of partially transformed (working) trees and derivation histories: That is, b is derived from a by application of a rule (q, pat) →w r to an unprocessed input subtree a ↓ i which is in state q, replacing it by output given by r, with its nonterminals replaced by the instruction to transform descendant input subtrees at relative path i′ in state q′. The sources of a rule r = (q, l, rhs, w) ∈ R are the inputpath parts of the rhs nonterminals: If the sources of a rule refer to input paths that do not exist in the input, then the rule cannot apply (because a ↓ (i · (1) · i′) would not exist). In the traditional statement of R, sources(rhs) is always {(1), ... , (n)}, writing xi instead of (i), but in xR, we identify mapped input subtrees by arbitrary (finite) paths. An input tree is transformed by starting at the root in the initial state, and recursively applying outputgenerating rules to a frontier of (copies of) input subtrees (each marked with their own state), until (in a complete derivation, finishing at the leaves with terminal rules) no states remain. Let ⇒∗ X, ⇒L∗X , and wX(a, b) follow from ⇒X exactly as in Section 3. Then the weight of (i, o) in X is WX(i,o) ≡ wX(Qi(i),o). The weighted tree transduction given by X is XX ≡ {(i, o, w) ∈ TE × To × R+|WX(i, o) = w}.
5 Parsing a Tree Transduction. Derivation trees for a transducer X = (E, A, Q, Qi, R) are trees labeled by rules (R) that dictate the choice of rules in a complete X-derivation. Figure 3 shows derivation trees for a particular transducer. In order to generate derivation trees for X automatically, we build a modified transducer X′. This new transducer produces derivation trees on its output instead of normal output trees. X′ is That is, the original rhs of rules are flattened into a tree of depth 1, with the root labeled by the original rule, and all the non-expanding A-labeled nodes of the rhs removed, so that the remaining children are the nonterminal yield in left to right order. Derivation trees deterministically produce a single weighted output tree. The derived transducer X′ nicely produces derivation trees for a given input, but in explaining an observed (input/output) pair, we must restrict the possibilities further. Because the transformations of an input subtree depend only on that subtree and its state, we can (Algorithm 1) build a compact wRTG that produces exactly the weighted derivation trees corresponding to Xtransductions (I, ()) �� X(O, h) (with weight equal to wX(h)).
6 Inside-Outside for wRTG. Given a wRTG G = (E, N, S, P), we can compute the sums of weights of trees derived using each production by adapting the well-known inside-outside algorithm for weighted context-free (string) grammars (Lari and Young,1990). The inside weights using G are given by βG : TE → (R−R−), giving the sum of weights of all tree-producing derivatons from trees with nonterminal leaves: By definition, βG(S) gives the sum of the weights of all trees generated by G. For the wRTG generated by DERIV(X, I, O), this is exactly WX(I, O). Outside weights αG for a nonterminal are the sums of weights of trees generated by the wRTG that have derivations containing it, but excluding its inside weights (that is, the weights summed do not include the weights of rules used to expand an instance of it). Input: xR transducer X = (E, A, Q, Qi, R) and observed tree pair I ∈ TΣ, O ∈ TΔ. Output: derivation wRTG G = (R, N ⊆ Q × pathsI × pathsO, S, P) generating all weighted derivation trees for X that produce O from I. Returns false instead if there are no such trees. if anyrule? then N ← N ∪ {(q, i, o)} return anyrule? The possible derivations for a given PRODUCEI,O(q, i, o) are constant and need not be computed more than once, so the function is memoized. We have in the worst case to visit all |Q |· |I |· |O| (q, i, o) pairs and have all |R |transducer rules match at each of them. If enumerating rules matching transducer input-patterns and output-subtrees has cost L (constant given a transducer), then DERIV has time complexity O(L · |Q |· |I |· |O |· |R|). Finally, given inside and outside weights, the sum of weights of trees using a particular production is γG((n, r, w) ∈ P) ≡ αG(n) · w · βG(r). Computing αG and βG for nonrecursive wRTG is a straightforward translation of the above recursive definitions (using memoization to compute each result only once) and is O(|G|) in time and space.
7 EM Training. Estimation-Maximization training (Dempster, Laird, and Rubin, 1977) works on the principle that the corpus likelihood can be maximized subject to some normalization constraint on the parameters by repeatedly (1) estimating the expectation of decisions taken for all possible ways of generating the training corpus given the current parameters, accumulating parameter counts, and (2) maximizing by assigning the counts to the parameters and renormalizing. Each iteration is guaranteed to increase the likelihood until a local maximum is reached. Algorithm 2 implements EM xR training, repeatedly computing inside-outside weights (using fixed transducer derivation wRTGs for each input/output tree pair) to efficiently sum each parameter contribution to likelihood over all derivations. Each EM iteration takes time linear in the size of the transducer and linear in the size of the derivation tree grammars for the training examples. The size of the derivation trees is at worst O(|Q|·|I|·|O|·|R|). For a corpus of K examples with average input/output size M, an iteration takes (at worst) O(|Q |· |R |· K · M2) time—quadratic, like the forward-backward algorithm.
8 Tree-to-String Transducers (xRS). We now turn to tree-to-string transducers (xRS). In the automata literature, these were first called generalized syntax-directed translations (Aho and Ullman, 1971) and used to specify compilers. Tree-to-string transducers have also been applied to machine translation (Yamada and Knight, 2001; Eisner, 2003). We give an explicit tree-to-string transducer example in the next section. Formally, a weighted extended-lhs root-to-frontier tree-to-string transducer X is a quintuple (E, A, Q, Qi, R) where E is the input alphabet, and A is the output alphabet, Q is a finite set of states, Qi ∈ Q is the initial (or start, or root) state, and R ⊆ Q × XRPATΣ × (A ∪(Q × paths))⋆ × R+ are a finite set of weighted transformation rules, written (q, pattern) →w rhs. A rule says that to transform (with weight w) an input subtree matching pattern while in state q, replace it by the string of rhs with its nonterminal (Q × paths) letters replaced by their (recursive) transformation. xRS is the same as xR, except that the rhs are strings containing some nonterminals instead of trees containing nonterminal leaves (so the intermediate derivation objects
Algorithm 2: TRAIN. Input: xR transducer X = (E, A, Q, Qd, R), observed weighted tree pairs T ∈ TE × TA × R+, normalization function Z({countr  |r ∈ R}, r′ ∈ R), minimum relative log-likelihood change for convergence ǫ ∈ R+, maximum number of iterations maxit ∈ N, and prior counts (for a so-called Dirichlet prior) {priorr  |r ∈ R} for smoothing each rule. Output: New rule weights W ≡ {wr  |r ∈ R}. begin for (i, o, w) ∈ T do di,o ← DERIV(X, i, o)//Alg. 1 if di,o = false then warn(more rules are needed to explain (i,o)) compute inside/outside weights for di,o and remove all useless nonterminals n whose βdi,o(n) = 0 or αdi,o(n) = 0 itno ← 0, lastL ← −∞, δ ← ǫ for r = (q, pat, rhs, w) ∈ R do wr ← w
end. are strings containing state-marked input subtrees). We have developed an xRS training procedure similar to the xR procedure, with extra computational expense to consider how different productions might map to different spans of the output string. Space limitations prohibit a detailed description; we refer the reader to a longer version of this paper (submitted). We note that this algorithm subsumes normal inside-outside training of PCFG on strings (Lari and Young, 1990), since we can always fix the input tree to some constant for all training examples.
9 Example. It is possible to cast many current probabilistic natural language models as R-type tree transducers. In this section, we implement the translation model of (Yamada and Knight, 2001). Their generative model provides a formula for P(Japanese string  |English tree), in terms of individual parameters, and their appendix gives special EM re-estimation formulae for maximizing the product of these conditional probabilities across the whole tree/string corpus. We now build a trainable xRS tree-to-string transducer that embodies the same P(Japanese string  |English tree). First, we need start productions like these, where q is the start state: - q x:S → q.TOP.S x - q x:VP → q.TOP.VP x These set up states like q.TOP.S, which means “translate this tree, whose root is S.” Then every q.parent.child pair gets its own set ofthree insert-function-wordproductions, e.g. : - q.TOP.S x → i x, r x - q.TOP.S x → r x, i x - q.TOP.S x → r x - q.NP.NN x → i x, r x - q.NP.NN x → r x, i x - q.NP.NN x → r x State i means “produce a Japanese function word out of thin air.” We include an i production for every Japanese word in the vocabulary, e.g. : State r means “re-order my children and then recurse.” For internal nodes, we include a production for every parent/child-sequence and every permutation thereof, e.g. : - r NP(x0:CD, x1:NN) → q.NP.CD x0, q.NP.NN x1 - r NP(x0:CD, x1:NN) → q.NP.NN x1, q.NP.CD x0 The rhs sends the child subtrees back to state q for recursive processing. However, for English leaf nodes, we instead transition to a different state t, so as to prohibit any subsequent Japanese function word insertion: - r NN(x0:car) → t x0 State t means “translate this word,” and we have a production for every pair of co-occurring English and Japanese words: - t car → kuruma - t car → wa - t car → *e* This follows (Yamada and Knight, 2001) in also allowing English words to disappear, or translate to epsilon. Every production in the xRS transducer has an associated weight and corresponds to exactly one of the model parameters. There are several benefits to this xRS formulation. First, it clarifies the model, in the same way that (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003) elucidate other machine translation models in easily-grasped FST terms. Second, the model can be trained with generic, off-the-shelf tools—versus the alternative of working out model-specific re-estimation formulae and implementing custom training software. Third, we can easily extend the model in interesting ways. For example, we can add productions for multi-level and lexical re-ordering: - r NP(x0:NP, PP(IN(of), x1:NP)) → q x1, no, q x0 We can add productions for phrasal translations: - r NP(JJ(big), NN(cars)) → ooki, kuruma This can now include crucial non-constituent phrasal translations: - r S(NP(PRO(there),VP(VB(are), x0:NP) → q x0, ga, arimasu We can also eliminate many epsilon word-translation rules in favor of more syntactically-controlled ones, e.g. : - r NP(DT(the),x0:NN) → q x0 We can make many such changes without modifying the training procedure, as long as we stick to tree automata.
10 Related Work. Tree substitution grammars or TSG (Schabes, 1990) are equivalent to regular tree grammars. xR transducers are similar to (weighted) Synchronous TSG, except that xR can copy input trees (and transform the copies differently), but does not model deleted input subtrees. (Eisner, 2003) discusses training for Synchronous TSG. Our training algorithm is a generalization of forwardbackward EM training for finite-state (string) transducers, which is in turn a generalization of the original forwardbackward algorithm for Hidden Markov Models.
11 Acknowledgments. Thanks to Daniel Gildea and Kenji Yamada for comments on a draft of this paper, and to David McAllester for helping us connect into previous work in automata theory.