Introduction. Newspaper texts, narratives and other texts describe events that occur in time and specify the temporallocation and order of these events. Text comprehen sion, amongst other capabilities, clearly requires the capability to identify the events described in a text and locate these in time. This capability is crucial to a wide range of NLP applications, from document summarization and question answering to machine translation.Recent work on the annotation of events and temporal relations has resulted in both a de-facto stan dard for expressing these relations and a hand-builtgold standard of annotated texts. TimeML (Puste jovsky et al, 2003a) is an emerging ISO standard for annotation of events, temporal expressions and the anchoring and ordering relations between them. TimeBank (Pustejovsky et al, 2003b; Boguraev et al., forthcoming) was originally conceived of as aproof of concept that illustrates the TimeML lan guage, but has since gone through several rounds of revisions and can now be considered a gold standard for temporal information. TimeML and TimeBank have already been used as the basis for automatic time, event and temporal relation annotation tasks in a number of research projects in recent years (Mani et al, 2006; Boguraev et al, forthcoming).An open evaluation challenge in the area of temporal annotation should serve to drive research forward, as it has in other areas of NLP. The automatic identification of all temporal referring expres sions, events and temporal relations within a text is the ultimate aim of research in this area. However, addressing this aim in a first evaluation challenge was judged to be too difficult, both for organizers and participants, and a staged approach was deemedmore effective. Thus we here present an initial eval uation exercise based on three limited tasks that webelieve are realistic both from the perspective of as sembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks. They are also tasks, whichshould they be performable automatically, have ap plication potential.
Task Description. . The tasks as originally proposed were modified slightly during the course of resource development for the evaluation exercise due to constraints on dataand annotator availability. In the following we de scribe the tasks as they were ultimately realized in the evaluation. There were three tasks ? A, B and C. For allthree tasks the data provided for testing and train ing includes annotations identifying: (1) sentence boundaries; (2) all temporal referring expression as 75 specified by TIMEX3; (3) all events as specifiedin TimeML; (4) selected instances of temporal re lations, as relevant to the given task. For tasks A and B a restricted set of event terms were identified ? those whose stems occurred twenty times or more in TimeBank. This set is referred to as the Event Target List or ETL.TASK A This task addresses only the temporal re lations holding between time and event expressions that occur within the same sentence. Furthermore only event expressions that occur within the ETL areconsidered. In the training and test data, TLINK an notations for these temporal relations are provided, the difference being that in the test data the relation type is withheld. The task is to supply this label. TASK B This task addresses only the temporal relations holding between the Document Creation Time (DCT) and event expressions. Again onlyevent expressions that occur within the ETL are con sidered. As in Task A, TLINK annotations for these temporal relations are provided in both training and test data, and again the relation type is withheld in the test data and the task is to supply this label. TASK C Task C relies upon the idea of their beinga main event within a sentence, typically the syn tactically dominant verb. The aim is to assign thetemporal relation between the main events of adja cent sentences. In both training and test data the main events are identified (via an attribute in the event annotation) and TLINKs between these main events are supplied. As for Tasks A and B, the task here is to supply the correct relation label for these TLINKs.
Data Description and Data Preparation. . The TempEval annotation language is a simplifiedversion of TimeML 1. For TempEval, we use the fol lowing five tags: TempEval, s, TIMEX3, EVENT, and TLINK. TempEval is the document root and s marks sentence boundaries. All sentence tags in the TempEval data are automatically created using the Alembic Natural Language processing tools. The other three tags are discussed here in more detail:1See http://www.timeml.org for language specifica tions and annotation guidelines ? TIMEX3. Tags the time expressions in the text. It is identical to the TIMEX3 tag in TimeML. See the TimeML specifications and guidelines for further details on this tag and its attributes. Each document has one special TIMEX3 tag,the Document Creation Time, which is inter preted as an interval that spans a whole day. EVENT. Tags the event expressions in the text. The interpretation of what an event is is taken from TimeML where an event is a cover term for predicates describing situations that happen or occur as well as some, but not all, stative predicates. Events can be denoted by verbs,nouns or adjectives. The TempEval event an notation scheme is somewhat simpler than thatused in TimeML, whose complexity was designed to handle event expressions that intro duced multiple event instances (consider, e.g. He taught on Wednesday and Friday). Thiscomplication was not necessary for the Tem pEval data. The most salient attributes encodetense, aspect, modality and polarity informa tion. For TempEval task C, one extra attribute is added: mainevent, with possible values YES and NO. ? TLINK. This is a simplified version of the TimeML TLINK tag. The relation types for the TimeML version form a fine-grained set based on James Allen?s interval logic (Allen, 1983). For TempEval, we use only six relation typesincluding the three core relations BEFORE, AFTER, and OVERLAP, the two less specific relations BEFORE-OR-OVERLAP and OVERLAP OR-AFTER for ambiguous cases, and finally therelation VAGUE for those cases where no partic ular relation can be established. As stated above the TLINKs of concern for each task are explicitly included in the training and in thetest data. However, in the latter the relType at tribute of each TLINK is set to UNKNOWN. For each task the system must replace the UNKNOWN values with one of the six allowed values listed above. The EVENT and TIMEX3 annotations were takenverbatim from TimeBank version 1.2.2 The annota 2TimeBank 1.2 is available for free through the Linguistic Data Consortium, see http://www.timeml.org for more 76tion procedure for TLINK tags involved dual annotation by seven annotators using a web-based anno tation interface. After this phase, three experiencedannotators looked at all occurrences where two an notators differed as to what relation type to select and decided on the best option. For task C, there was an extra annotation phase where the main events were marked up. Main events are those events that are syntactically dominant in the sentences.It should be noted that annotation of temporal relations is not an easy task for humans due to ram pant temporal vagueness in natural language. As aresult, inter-annotator agreement scores are well be low the often kicked-around threshold of 90%, both for the TimeML relation set as well as the TempEvalrelation set. For TimeML temporal links, an inter annotator agreement of 0.77 was reported, whereagreement was measured by the average of preci sion and recall. The numbers for TempEval are even lower, with an agreement of 0.72 for anchorings of events to times (tasks A and B) and an agreement of0.65 for event orderings (task C). Obviously, num bers like this temper the expectations for automatic temporal linking. The lower number for TempEval came a bit asa surprise because, after all, there were fewer relations to choose form. However, the TempEval an notation task is different in the sense that it did not give the annotator the option to ignore certain pairs of events and made it therefore impossible to skip hard-to-classify temporal relations.
Evaluating Temporal Relations. . In full temporal annotation, evaluation of temporal annotation runs into the same issues as evaluation of anaphora chains: simple pairwise comparisons maynot be the best way to evaluate. In temporal annota tion, for example, one may wonder how the response in (1) should be evaluated given the key in (2). (1) {A before B, A before C, B equals C} (2) {A after B, A after C, B equals C}Scoring (1) at 0.33 precision misses the interde pendence between the temporal relations. What we need to compare is not individual judgements but two partial orders. details. For TempEval however, the tasks are defined in a such a way that a simple pairwise comparison is possible since we do not aim to create a full temporal graph and judgements are made in isolation. Recall that there are three basic temporal relations (BEFORE, OVERLAP, and AFTER) as well as three disjunctions over this set (BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER and VAGUE). The addition of these disjunctions raises the question of how to score a response of, for example, BEFORE given akey of BEFORE-OR-OVERLAP. We use two scor ing schemes: strict and relaxed. The strict scoring scheme only counts exact matches as success. For example, if the key is OVERLAP and the responseBEFORE-OR-OVERLAP than this is counted as fail ure. We can use standard definitions of precision and recall Precision = Rc/R Recall = Rc/Kwhere Rc is number of correct answers in the response, R the total number of answers in the re sponse, and K the total number of answers in the key. For the relaxed scoring scheme, precision and recall are defined as Precision = Rcw/R Recall = Rcw/K where Rcw reflects the weighted number of correctanswers. A response is not simply counted as 1 (correct) or 0 (incorrect), but is assigned one of the val ues in table 1. B O A B-O O-A V B 1 0 0 0.5 0 0.33 O 0 1 0 0.5 0.5 0.33 A 0 0 1 0 0.5 0.33 B-O 0.5 0.5 0 1 0.5 0.67 O-A 0 0.5 0.5 0.5 1 0.67 V 0.33 0.33 0.33 0.67 0.67 1 Table 1: Evaluation weights This scheme gives partial credit for disjunctions,but not so much that non-commitment edges out pre cise assignments. For example, assigning VAGUE as the relation type for every temporal relation results in a precision of 0.33. 77
Participants. . Six teams participated in the TempEval tasks. Three of the teams used statistics exclusively, one used arule-based system and the other two employed a hy brid approach. This section gives a short description of the participating systems. CU-TMP trained three support vector machine (SVM) models, one for each task. All models used the gold-standard TimeBank features for events and times as well as syntactic features derived from the text. Additionally, the relation types obtained by running the task B system on the training data for Task A and Task C, were added as a feature to the two latter systems. A subset of features was selectedusing cross-validations on the training data, discarding features whose removal improved the cross validation F-score. When applied to the test data, the Task B system was run first in order to supplythe necessary features to the Task A and Task C sys tems.LCC-TE automatically identifies temporal refer ring expressions, events and temporal relations in text using a hybrid approach, leveraging variousNLP tools and linguistic resources at LCC. For tem poral expression labeling and normalization, they used a syntactic pattern matching tool that deploys a large set of hand-crafted finite state rules. For event detection, they used a small set of heuristics as well as a lexicon to determine whether or not a token is an event, based on the lemma, part of speech and WordNet senses. For temporal relation discovery, LCC-TE used a large set of syntactic and semantic features as input to a machine learning components.NAIST-japan defined the temporal relation iden tification task as a sequence labeling model, in which the target pairs ? a TIMEX3 and an EVENT? are linearly ordered in the document. For analyz ing the relative positions, they used features fromdependency trees which are obtained from a dependency parser. The relative position between the tar get EVENT and a word in the target TIMEX3 is used as a feature for a machine learning based relation identifier. The relative positions between a word inthe target entities and another word are also intro duced. The USFD system uses an off-the-shelf Machine Learning suite(WEKA), treating the assignment of temporal relations as a simple classification task. The features used were the ones provided in theTempEval data annotation together with a few features straightforwardly computed from the docu ment without any deeper NLP analysis.WVALI?s approach for discovering intra sentence temporal relations relies on sentence-levelsyntactic tree generation, bottom-up propaga tion of the temporal relations between syntactic constituents, a temporal reasoning mechanism that relates the two targeted temporal entities to their closest ancestor and then to each other, and on conflict resolution heuristics. In establishing the temporal relation between an event and theDocument Creation Time (DCT), the temporal ex pressions directly or indirectly linked to that event are first analyzed and, if no relation is detected, the temporal relation with the DCT is propagatedtop-down in the syntactic tree. Inter-sentence tem poral relations are discovered by applying several heuristics and by using statistical data extracted from the training corpus. XRCE-T used a rule-based system that relies on a deep syntactic analyzer that was extended to treattemporal expressions. Temporal processing is inte grated into a more generic tool, a general purpose linguistic analyzer, and is thus a complement for a better general purpose text understanding system.Temporal analysis is intertwined with syntacticosemantic text processing like deep syntactic analysis and determination of thematic roles. TempEval specific treatment is performed in a post-processing stage.
Results. . The results for the six teams are presented in tables 2, 3, and 4. team strict relaxed P R F P R F CU-TMP 0.61 0.61 0.61 0.63 0.63 0.63 LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60 NAIST 0.61 0.61 0.61 0.63 0.63 0.63 USFD* 0.59 0.59 0.59 0.60 0.60 0.60 WVALI 0.62 0.62 0.62 0.64 0.64 0.64 XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41 average 0.59 0.54 0.56 0.62 0.57 0.59 stddev 0.03 0.13 0.10 0.01 0.12 0.08 Table 2: Results for Task A 78 team strict relaxed P R F P R F CU-TMP 0.75 0.75 0.75 0.76 0.76 0.76 LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74 NAIST 0.75 0.75 0.75 0.76 0.76 0.76 USFD* 0.73 0.73 0.73 0.74 0.74 0.74 WVALI 0.80 0.80 0.80 0.81 0.81 0.81 XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71 average 0.76 0.72 0.74 0.78 0.74 0.75 stddev 0.03 0.08 0.05 0.03 0.06 0.03 Table 3: Results for Task B team strict relaxed P R F P R F CU-TMP 0.54 0.54 0.54 0.58 0.58 0.58 LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58 NAIST 0.49 0.49 0.49 0.53 0.53 0.53 USFD* 0.54 0.54 0.54 0.57 0.57 0.57 WVALI 0.54 0.54 0.54 0.64 0.64 0.64 XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58 average 0.51 0.51 0.51 0.58 0.58 0.58 stddev 0.05 0.05 0.05 0.04 0.04 0.04 Table 4: Results for Task C All tables give precision, recall and f-measure for both the strict and the relaxed scoring scheme, aswell as averages and standard deviation on the pre cision, recall and f-measure numbers. The entry for USFD is starred because the system developers are co-organizers of the TempEval task.3 For task A, the f-measure scores range from 0.34 to 0.62 for the strict scheme and from 0.41 to 0.63 for the relaxed scheme. For task B, the scores range from 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed). Finally, task C scores range from 0.42 to 0.55 (strict) and from 0.56 to 0.66 (relaxed).The differences between the systems is not spec tacular. WVALI?s hybrid approach outperforms the other systems in task B and, using relaxed scoring, in task C as well. But for task A, the winners barely edge out the rest of the field. Similarly, for task C using strict scoring, there is no system that clearly separates itself from the field.It should be noted that for task A, and in lesser ex tent for task B, the XRCE-T system has recall scores that are far below all other systems. This seemsmostly due to a choice by the developers to not as sign a temporal relation if the syntactic analyzer did not find a clear syntactic relation between the two 3There was a strict separation between people assisting in the annotation of the evaluation corpus and people involved in system development. elements that needed to be linked for the TempEval task.
Conclusion: the Future of Temporal. . EvaluationThe evaluation approach of TempEval avoids the in terdependencies that are inherent to a network of temporal relations, where relations in one part of the network may constrain relations in any other part ofthe network. To accomplish that, TempEval delib erately focused on subtasks of the larger problem of automatic temporal annotation. One thing we may want to change to the present TempEval is the definition of task A. Currently, it instructs to temporally link all events in a sentence to all time expressions in the same sentence. In the future we may consider splitting this into two tasks, where one subtask focuses on those anchorings thatare very local, like ?...White House spokesman Marlin Fitzwater [said] [late yesterday] that...?. We expect both inter-annotator agreement and system per formance to be higher on this subtask. There are two research avenues that loom beyondthe current TempEval: (1) definition of other subtasks with the ultimate goal of establishing a hierar chy of subtasks ranked on performance of automatictaggers, and (2) an approach to evaluate entire time lines. Some other temporal linking tasks that can be considered are ordering of consecutive events in a sentence, ordering of events that occur in syntacticsubordination relations, ordering events in coordi nations, and temporal linking of reporting events to the document creation time. Once enough temporallinks from all these subtasks are added to the entire temporal graph, it becomes possible to let confidence scores from the separate subtasks drive a con straint propagation algorithm as proposed in (Allen, 1983), in effect using high-precision relations to constrain lower-precision relations elsewhere in the graph. With this more complete temporal annotation it is no longer possible to simply evaluate the entire graph by scoring pairwise comparisons. Instead the entire timeline must be evaluated. Initial ideas regarding this focus on transforming the temporal graph of a document into a set of partial orders built 79 around precedence and inclusion relations and then evaluating each of these partial orders using some kind of edit distance measure.4 We hope to have taken the first baby steps with the three TempEval tasks.
Acknowledgements. . We would like to thank all the people who helped prepare the data for TempEval, listed here in no particular order: Amber Stubbs, Jessica Littman, Hongyuan Qiu, Emin Mimaroglu, Emma Barker, Catherine Havasi, Yonit Boussany, Roser Saur??, and Anna Rumshisky. Thanks also to all participants to this new task: Steven Bethard and James Martin (University ofColorado at Boulder), Congmin Min, Munirathnam Srikanth and Abraham Fowler (Language Computer Corporation), Yuchang Cheng, Masayuki Asa hara and Yuji Matsumoto (Nara Institute of Science and Technology), Mark Hepple, Andrea Setzer and Rob Gaizauskas (University of Sheffield), CarolineHageg`e and Xavier Tannier (XEROX Research Cen tre Europe), and Georgiana Pus?cas?u (University of Wolverhampton and University of Alicante). Part of the work in this paper was funded bythe DTO/AQUAINT program under grant num ber N61339-06-C-0140 and part funded by the EU VIKEF project (IST- 2002-507173).