Introduction. Common assumptions about the role and useful ness of word sense disambiguation (WSD) models in full-scale statistical machine translation (SMT) systems have recently been challenged. On the one hand, in previous work (Carpuat and Wu, 2005b) we obtained disappointing results when using the predictions of a Senseval WSD system inconjunction with a standard word-based SMT system: we reported slightly lower BLEU scores de spite trying to incorporate WSD using a number of apparently sensible methods. These results cast doubt on the assumption that sophisticated dedicated WSD systems that were developed independently from any particular NLP application can easily beintegrated into a SMT system so as to improve trans lation quality through stronger models of context and rich linguistic information. Rather, it has beenargued, SMT systems have managed to achieve sig nificant improvements in translation quality without directly addressing translation disambiguation as aWSD task. Instead, translation disambiguation deci sions are made indirectly, typically using only word surface forms and very local contextual information, forgoing the much richer linguistic information that WSD systems typically take advantage of. On the other hand, error analysis reveals that theperformance of SMT systems still suffers from inaccurate lexical choice. In subsequent empirical stud ies, we have shown that SMT systems perform much worse than dedicated WSD models, both supervised RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Anyopinions, findings and conclusions or recommendations ex pressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 61 and unsupervised, on a Senseval WSD task (Carpuat and Wu, 2005a), and therefore suggest that WSD should have a role to play in state-of-the-art SMT systems. In addition to the Senseval shared tasks, which have provided standard sense inventories and data sets, WSD research has also turned increasinglyto designing specific models for a particular applica tion. For instance, Vickrey et al (2005) and Specia (2006) proposed WSD systems designed for Frenchto English, and Portuguese to English translation re spectively, and present a more optimistic outlook forthe use of WSD in MT, although these WSD sys tems have not yet been integrated nor evaluated in full-scale machine translation systems.Taken together, these seemingly contradictory results suggest that improving SMT lexical choice ac curacy remains a key challenge to improve current SMT quality, and that it is still unclear what is the most appropriate integration framework for the WSD models in SMT. In this paper, we present first results with a new architecture that integrates a state-of-the-artWSD model into phrase-based SMT so as to per form multi-word phrasal lexical disambiguation, and show that this new WSD approach not only produces gains across all available Chinese-EnglishIWSLT06 test sets for all eight commonly used au tomated MT evaluation metrics, but also produces statistically significant gains on the much larger NIST Chinese-English task. The main differencebetween this approach and several of our earlier ap proaches as described in Carpuat and Wu (2005b) and subsequently Carpuat et al (2006) lies in the fact that we focus on repurposing the WSD systemfor multi-word phrase-based SMT. Rather than us ing a generic Senseval WSD model as we did inCarpuat and Wu (2005b), here both the WSD train ing and the WSD predictions are integrated into the phrase-based SMT framework. Furthermore, rather than using a single word based WSD approach to augment a phrase-based SMT model as we did in Carpuat et al (2006) to improve BLEU and NISTscores, here the WSD training and predictions operate on full multi-word phrasal units, resulting in significantly more reliable and consistent gains as eva luted by many other translation accuracy metrics as well. Specifically: ? Instead of using a Senseval system, we redefinethe WSD task to be exactly the same as lexi cal choice task faced by the multi-word phrasal translation disambiguation task faced by the phrase-based SMT system. Instead of using predefined senses drawn from manually constructed sense inventories such asHowNet (Dong, 1998), our WSD for SMT sys tem directly disambiguates between all phrasaltranslation candidates seen during SMT train ing. Instead of learning from manually annotated training data, our WSD system is trained on the same corpora as the SMT system. However, despite these adaptations to the SMT task, the core sense disambiguation task remains pure WSD: ? The rich context features are typical of WSD and almost never used in SMT. The dynamic integration of context-sensitive translation probabilities is not typical of SMT.? Although it is embedded in a real SMT sys tem, the WSD task is exactly the same as inrecent and coming Senseval Multilingual Lexi cal Sample tasks (e.g., Chklovski et al (2004)), where sense inventories represent the semantic distinctions made by another language. We begin by presenting the WSD module and the SMT integration technique. We then show that incorporating it into a standard phrase-based SMT baseline system consistently improves translation quality across all three different test sets from the Chinese-English IWSLT text translation evaluation,as well as on the larger NIST Chinese-English trans lation task. Depending on the metric, the individualgains are sometimes modest, but remarkably, incorporating WSD never hurts, and helps enough to al ways make it a worthwile additional component in an SMT system. Finally, we analyze the reasons for the improvement. 62
Problems in context-sensitive lexical. . choice for SMTTo the best of our knowledge, there has been no pre vious attempt at integrating a state-of-the-art WSD system for fully phrasal multi-word lexical choiceinto phrase-based SMT, with evaluation of the re sulting system on a translation task. While there are many evaluations of WSD quality, in particular the Senseval series of shared tasks (Kilgarriff and Rosenzweig (1999), Kilgarriff (2001), Mihalcea et al. (2004)), very little work has been done to addressthe actual integration of WSD in realistic SMT ap plications. To fully integrate WSD into phrase-based SMT, it is necessary to perform lexical disambiguation on multi-word phrasal lexical units; in contrast, the model reported in Cabezas and Resnik (2005)can only perform lexical disambiguation on sin gle words. Like the model proposed in this paper,Cabezas and Resnik attempted to integrate phrasebased WSD models into decoding. However, although they reported that incorporating these predic tions via the Pharaoh XML markup scheme yielded a small improvement in BLEU score over a Pharaoh baseline on a single Spanish-English translation data set, we have determined empirically that applyingtheir single-word based model to several ChineseEnglish datasets does not yield systematic improve ments on most MT evaluation metrics (Carpuat andWu, 2007). The single-word model has the disad vantage of forcing the decoder to choose between the baseline phrasal translation probabilities versusthe WSD model predictions for single words. In ad dition, the single-word model does not generalize to WSD for phrasal lexical choice, as overlapping spans cannot be specified with the XML markup scheme. Providing WSD predictions for phraseswould require committing to a phrase segmenta tion of the input sentence before decoding, which is likely to hurt translation quality. It is also necessary to focus directly on translationaccuracy rather than other measures such as alignment error rate, which may not actually lead to im proved translation quality; in contrast, for example, Garcia-Varea et al (2001) and Garcia-Varea et al (2002) show improved alignment error rate with a maximum entropy based context-dependent lexicalchoice model, but not improved translation accuracy. In contrast, our evaluation in this paper is conducted on the actual decoding task, rather than in termediate tasks such as word alignment. Moreover,in the present work, all commonly available auto mated MT evaluation metrics are used, rather than only BLEU score, so as to maintain a more balanced perspective. Another problem in the context-sensitive lexical choice in SMT models of Garcia Varea et al is that their feature set is insufficiently rich to make much better predictions than the SMT model itself. In contrast, our WSD-based lexical choice models are designed to directly model the lexical choice in the actual translation direction, and take full advantageof not residing strictly within the Bayesian source channel model in order to benefit from the much richer Senseval-style feature set this facilitates. Garcia Varea et al found that the best results are obtained when the training of the context-dependent translation model is fully incorporated with the EM training of the SMT system. As described below,the training of our new WSD model, though not in corporated within the EM training, is also far more closely tied to the SMT model than is the case with traditional standalone WSD models.In contrast with Brown et al (1991), our approach incorporates the predictions of state-of-the art WSD models that use rich contextual features for any phrase in the input vocabulary. In Brown et al?s early study of WSD impact on SMT performance, the authors reported improved translation quality on a French to English task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative. However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT architectures.
Problems in translation-oriented WSD. . The close relationship between WSD and SMT has been emphasized since the emergence of WSD asan independent task. However, most of previous re search has focused on using multilingual resourcestypically used in SMT systems to improve WSD ac curacy, e.g., Dagan and Itai (1994), Li and Li (2002), 63 Diab (2004). In contrast, this paper focuses on theconverse goal of using WSD models to improve ac tual translation quality.Recently, several researchers have focused on de signing WSD systems for the specific purpose oftranslation. Vickrey et al (2005) train a logistic regression WSD model on data extracted from auto matically word aligned parallel corpora, but evaluateon a blank filling task, which is essentially an eval uation of WSD accuracy. Specia (2006) describesan inductive logic programming-based WSD sys tem, which was specifically designed for the purpose of Portuguese to English translation, but this system was also only evaluated on WSD accuracy, and not integrated in a full-scale machine translation system. Ng et al (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. The purpose ofthe study was to lower the annotation cost for su pervised WSD, as suggested earlier by Resnik andYarowsky (1999). However this result is also en couraging for the integration of WSD in SMT, since it suggests that accurate WSD can be achieved using training data of the kind needed for SMT.
Building WSD models for phrase-based. . SMT 4.1 WSD models for every phrase in the input. vocabulary Just like for the baseline phrase translation model,WSD models are defined for every phrase in the in put vocabulary. Lexical choice in SMT is naturallyframed as a WSD problem, so the first step of inte gration consists of defining a WSD model for every phrase in the SMT input vocabulary. This differs from traditional WSD tasks, wherethe WSD target is a single content word. Sense val for instance has either lexical sample or all wordtasks. The target words for both categories of Sen seval WSD tasks are typically only content words? primarily nouns, verbs, and adjectives?while in thecontext of SMT, we need to translate entire sen tences, and therefore have a WSD model not only for every word in the input sentences, regardless of their POS tag, but for every phrase, including tokens such as articles, prepositions and even punctuation.Further empirical studies have suggested that includ ing WSD predictions for those longer phrases is akey factor to help the decoder produce better trans lations (Carpuat and Wu, 2007). 4.2 WSD uses the same sense definitions as the. SMT system Instead of using pre-defined sense inventories, theWSD models disambiguate between the SMT trans lation candidates. In order to closely integrate WSDpredictions into the SMT system, we need to formu late WSD models so that they produce features that can directly be used in translation decisions taken by the SMT system. It is therefore necessary for the WSD and SMT systems to consider exactly the same translation candidates for a given word in the input language. Assuming a standard phrase-based SMT system(e.g., Koehn et al (2003)), WSD senses are thus ei ther words or phrases, as learned in the SMT phrasal translation lexicon. Those ?sense? candidates arevery different from those typically used even in ded icated WSD tasks, even in the multilingual Sensevaltasks. Each candidate is a phrase that is not neces sarily a syntactic noun or verb phrase as in manuallycompiled dictionaries. It is quite possible that dis tinct ?senses? in our WSD for SMT system could beconsidered synonyms in a traditional WSD frame work, especially in monolingual WSD.In addition to the consistency requirements for integration, this requirement is also motivated by empirical studies, which show that predefined trans lations derived from sense distinctions defined in monolingual ontologies do not match translation distinction made by human translators (Specia et al, 2006). 4.3 WSD uses the same training data as the. SMT system WSD training does not require any other resourcesthan SMT training, nor any manual sense annota tion. We employ supervised WSD systems, sinceSenseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexi cal sample tasks results described by Mihalcea et al (2004)). Training examples are annotated using the phrasealignments learned during SMT training. Every in 64 put language phrase is sense-tagged with its aligned output language phrase in the parallel corpus. The phrase alignment method used to extract the WSD training data therefore depends on the one used by the SMT system. This presents the advantage of training WSD and SMT models on exactly the same data, thus eliminating domain mismatches betweenSenseval data and parallel corpora. But most importantly, this allows WSD training data to be gener ated entirely automatically, since the parallel corpus is automatically phrase-aligned in order to learn the SMT phrase bilexicon. 4.4 The WSD system. The word sense disambiguation subsystem is mod eled after the best performing WSD system in the Chinese lexical sample task at Senseval-3 (Carpuat et al, 2004). The features employed are typical of WSD and are therefore far richer than those used in mostSMT systems. The feature set consists of positionsensitive, syntactic, and local collocational fea tures, since these features yielded the best results when combined in a na??ve Bayes model on severalSenseval-2 lexical sample tasks (Yarowsky and Flo rian, 2002). These features scale easily to the bigger vocabulary and sense candidates to be considered in a SMT task. The Senseval system consists of an ensemble of four combined WSD models: The first model is a na??ve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (Klein and Manning, 2002) found that this model yielded higher accuracy than na??ve Bayes in a subsequent comparison of WSD performance. The third model is a boosting model (Freund and Schapire, 1997), since boosting has consistently turned in very competitive scores on related tasks such as named entity classification. We also use the Adaboost.MH algorithm. The fourth model is a Kernel PCA-based model (Wu et al, 2004). Kernel Principal Component Analysis or KPCA is a nonlinear kernel method forextracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their origi nal space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transformby which the input vectors can be mapped nonlin early to a new set of vectors (Scho?lkopf et al, 1998).WSD can be performed by a Nearest Neighbor Clas sifier in the high-dimensional KPCA feature space. All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant. Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent. 4.5 Integrating WSD predictions in. phrase-based SMT architectures It is non-trivial to incorporate WSD into an existing phrase-based architecture such as Pharaoh (Koehn,2004), since the decoder is not set up to easily accept multiple translation probabilities that are dy namically computed in context-sensitive fashion. For every phrase in a given SMT input sentence,the WSD probabilities can be used as additional feature in a loglinear translation model, in combination with typical context-independent SMT bilexi con probabilities. We overcome this obstacle by devising a callingarchitecture that reinitializes the decoder with dynamically generated lexicons on a per-sentence ba sis.Unlike a n-best reranking approach, which is limited by the lexical choices made by the decoder using only the baseline context-independent transla tion probabilities, our method allows the system to make full use of WSD information for all competing phrases at all decoding stages.
Experimental setup. . The evaluation is conducted on two standard Chinese to English translation tasks. We follow standard machine translation evaluation procedure us ing automatic evaluation metrics. Since our goal is to evaluate translation quality, we use standard MTevaluation methodology and do not evaluate the ac curacy of the WSD model independently. 65 Table 1: Evaluation results on the IWSLT06 dataset: integrating the WSD translation predictions improves BLEU, NIST, METEOR, WER, PER, CDER and TER across all 3 different available test sets. Test Set Exper. BLEU NIST METEOR METEOR (no syn) TER WER PER CDER Test 1 SMT 42.21 7.888 65.40 63.24 40.45 45.58 37.80 40.09 SMT+WSD 42.38 7.902 65.73 63.64 39.98 45.30 37.60 39.91 Test 2 SMT 41.49 8.167 66.25 63.85 40.95 46.42 37.52 40.35 SMT+WSD 41.97 8.244 66.35 63.86 40.63 46.14 37.25 40.10 Test 3 SMT 49.91 9.016 73.36 70.70 35.60 40.60 32.30 35.46 SMT+WSD 51.05 9.142 74.13 71.44 34.68 39.75 31.71 34.58 Table 2: Evaluation results on the NIST test set: integrating the WSD translation predictions improves BLEU, NIST, METEOR, WER, PER, CDER and TER Exper. BLEU NIST METEOR METEOR (no syn) TER WER PER CDER SMT 20.41 7.155 60.21 56.15 76.76 88.26 61.71 70.32 SMT+WSD 20.92 7.468 60.30 56.79 71.34 83.87 57.29 67.38 5.1 Data set. Preliminary experiments are conducted using training and evaluation data drawn from the multilin gual BTEC corpus, which contains sentences used inconversations in the travel domain, and their transla tions in several languages. A subset of this data wasmade available for the IWSLT06 evaluation cam paign (Paul, 2006); the training set consists of 40000 sentence pairs, and each test set contains around 500 sentences. We used only the pure text data, and notthe speech transcriptions, so that speech-specific issues would not interfere with our primary goal of understanding the effect of integrating WSD in a full scale phrase-based model.A larger scale evaluation is conducted on the stan dard NIST Chinese-English test set (MT-04), whichcontains 1788 sentences drawn from newswire cor pora, and therefore of a much wider domain than the IWSLT data set. The training set consists of about 1 million sentence pairs in the news domain. Basic preprocessing was applied to the corpus.The English side was simply tokenized and case normalized. The Chinese side was word segmented using the LDC segmenter. 5.2 Baseline SMT system. Since our focus is not on a specific SMT architec ture, we use the off-the-shelf phrase-based decoderPharaoh (Koehn, 2004) trained on the IWSLT train ing set. Pharaoh implements a beam search decoder for phrase-based statistical models, and presents the advantages of being freely available and widely used.The phrase bilexicon is derived from the inter section of bidirectional IBM Model 4 alignments,obtained with GIZA++ (Och and Ney, 2003), aug mented to improve recall using the grow-diag-finalheuristic. The language model is trained on the English side of the corpus using the SRI language mod eling toolkit (Stolcke, 2002). The loglinear model weights are learned using Chiang?s implementation of the maximum BLEUtraining algorithm (Och, 2003), both for the base line, and the WSD-augmented system. Due totime constraints, this optimization was only con ducted on the IWSLT task. The weights used in the WSD-augmented NIST model are based on the best IWSLT model. Given that the two tasks are quite different, we expect further improvements on the WSD-augmented system after running maximum BLEU optimization for the NIST task.
Results and discussion. . Using WSD predictions in SMT yields better trans lation quality on all test sets, as measured by all eight commonly used automatic evaluation metrics. 66 Table 3: Translation examples with and without WSD for SMT, drawn from IWSLT data sets. Input ?lX-. ? Ref. Please transfer to the Chuo train line. SMT Please turn to the Central Line. SMT+WSD Please transfer to Central Line. Input fh(f p Ref. Do I pay on the bus? SMT Please get on the bus? SMT+WSD I buy a ticket on the bus? Input