Figure 3: Size and devset performance of the cumulatively annotated models, starting with the markovized baseline. The two columns show the change in the baseline for each annotation introduced, both cumulatively and for each single annotation applied to the baseline in isolation. history models similar in intent to those described in Ron et al. (1994). For variable horizontal histories, we did not split intermediate states below 10 occurrences of a symbol. For example, if the symbol too rare, we would colit to For vertical histories, we used a cutoff which included both frequency and mutual information between the history and the expansions (this was not appropriate for the horizontal because unreliable at such low counts). Figure 2 shows parsing accuracies as well as the number of symbols in each markovization. These symbol counts include all the intermediate states which represent partially completed constituents. The general trend is that, in the absence of further annotation, more vertical annotation is better – even exhaustive grandparent annotation. This is not true for horizontal markovization, where the variableorder second-order model was superior. The best has an 79.74, already a substantial improvement over the baseline. In the remaining sections, we discuss other annotations which increasingly split the symbol space. Since we expressly do not smooth the grammar, not all splits are guaranteed to be beneficial, and not all sets of useful splits are guaranteed to co-exist well. particular, while markovization is good on its own, it has a large number of states and does not tolerate further splitting well. Therefore, base all further exploration on the ROOT S&quot;ROOT 4: An error which can be resolved with the (incorrect baseline parse shown). grammar. Although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories. 3 External vs. Internal Annotation The two major previous annotation strategies, parent annotation and head lexicalization, can be seen as instances of external and internal annotation, respectively. Parent annotation lets us indicate an important feature of the external environment of a node which influences the internal expansion of that node. On the other hand, lexicalization is a (radical) method of marking a distinctive aspect of the otherwise hidden internal contents of a node which influence the external distribution. Both kinds of annotation can be useful. To identify split states, we suffixes of the form mark internal content and mark external features. To illustrate the difference, consider unary productions. In the raw grammar, there are many unaries, and once any major category is constructed over a span, most others become constructible as well using unary chains (see Klein and Manning (2001) for discussion). Such chains are rare in real treebank trees: unary rewrites only appear in very specific for example of verbs where an empty, controlled subject. Figure 4 shows an erroneous output of the parser, using the baseline markovized grammar. Intuitively, there are several reasons this parse should be ruled out, but is that the lower which is intended prifor of communication verbs, is not a unary rewrite position (such complements usually have subjects). It would therefore be natural to annotate the trees so as to confine unary productions to the contexts in which they are actually ap- We tried two annotations. First, . NP&quot;S VP&quot;S NP&quot;VP VBD , NN SˆVP . VPˆS QP , NP&quot;VP $ CD CD VBG 444.9 million including , CONJP NP&quot;NP NP&quot;NP $ QP JJ NN , RB RB IN net interest down slightly from CD $ 450.7 million was Revenue $ CD (with a any nonterminal node which has only one child. In isolation, this resulted in an absolute gain of 0.55% (see figure 3). The same sentence, parsed using only the baseline and is parsed correctly, because the in the incorrect parse ends with an very low marked nodes had no siblings with It was similar to solo benefit (0.01% worse), but provided far less marginal benefit on top of later features (none at all on top of our top models), and was One restricted place where external unary annotation was very useful, however, was at the preterminal level, where internal annotation was meaningless. One distributionally salient tag conflation in the Penn treebank is the identification of demonstraand regular determiners based on whether they were only captured this distinction. The same external unary annotation was even more efwhen applied to adverbs disfor example, well Beyond these cases, unary tag marking was detrimen- The 78.86%. 4 Tag Splitting The idea that part-of-speech tags are not fine-grained enough to abstract away from specific-word behaviour is a cornerstone of lexicalization. The for example, showed that the determiners which occur alone are usefully distinguished from those which occur with other nomimaterial. This marks the with a single bit about their immediate external context: whether there are sisters. Given the success of parent annotation for nonterminals, it makes sense to parent antags, as well In fact, as figure 3 shows, exhaustively marking all preterminals with their parent category was the most effective single annotation we tried. Why should this be useful? Most tags have a canonical category. For example, occur under (only 234 of 70855 do not, mostly mistakes). However, when a tag that when we show such trees, we generally only show one annotation on top of the baseline at a time. Moreover, we do not explicitly show the binarization implicit by the horizontal markovization. two are not equivalent even given infinite data. 5: An error resolved with the (of the (a) the incorrect baseline parse and (b) the correct resolves this error. somewhat regularly occurs in a non-canonical position, its distribution is usually distinct. For example, most common adverbs directly under and Under they are and Under and and so on. substantially, to 80.62%. In addition to the adverb case, the Penn tag set conflates various grammatical distinctions that are commonly made in traditional and generative grammar, and from which a parser could hope to get useful information. For example, subordinating conas, complementizers prepositions in, all get the tag of these distinctions are captured by conjunctions occur under under but are not (both subordinating conjunctions and complementizers appear Also, there are exclusively nounprepositions predominantly verbones and so on. The annotation a linguistically motivated 6-way split the and brought the total to 81.19%. Figure 5 shows an example error in the baseline is equally well fixed by either In this case, the more common nominal of preferred unless the is annoto allow prefer We also got value from three other annotations which subcategorized tags for specific lexemes. we split off auxiliary verbs with the which appends all forms all forms of More miconjunction tags to indicate is an extended uniform version of the partial auxiliary annotation of Charniak (1997), wherein all auxiliaries are as a added to gerund auxiliaries and VP&quot;S VP&quot;S TO VP&quot;VP TO&quot;VP VP&quot;VP to VB PP&quot;VP to VB&quot;VP SBAR&quot;VP see NP&quot;PP see IN&quot;SBAR S&quot;SBAR IN (a) (b) NNS NN if VP&quot;S VBZ&quot;VP works works NN&quot;NP advertising advertising or not they were the strings &, each of which have distinctly different distributions from other conjunctions. Finally, we gave the percent sign (%) its own tag, in line with the dollar sign ($) already having its own. Together these three anbrought the 81.81%. 5 What is an Unlexicalized Grammar? Around this point, we must address exactly what we by an To the extent that go about subcategorizing many of them might come to represent a single word. One might thus feel that the approach of this paper is to walk down a slippery slope, and that we are merely arguing degrees. However, we believe that there is a fundamental qualitative distinction, grounded in linguistic practice, between what we see as permitted an unlexicalized against what one finds hopes to exploit in lexicalized The division rests on the traditional distinction between words closed-class words) and open class or lexical words). It is standard practice in linguistics, dating back decades, to annotate phrasal nodes with important functiondistinctions, for example to have a a whereas content words are not part of grammatical structure, and one would not have sperules or constraints for an for example. We follow this approach in our model: various closed classes are subcategorized to better represent important distinctions, and important features commonly expressed by function words are annotated phrasal nodes (such as whether a finite, or a participle, or an infinitive clause). However, no use is made of lexical class words, to provide either or bilexical At any rate, we have kept ourselves honest by estimating our models exclusively by maximum likelihood estimation over our subcategorized grammar, without any form of interpolation or shrinkage to unsubcategorized categories (although we do rules, as explained above). This effecshould be noted that we started with four tags in the Penn tagset that rewrite as a single word: and some of the punctuation tags, which rewrite as barely more. To the extent that we subcategorize tags, there will be more such cases, but many of them already exist in other tag sets. For instance, many tag sets, such as the Brown and tagsets give a separate sets of tags to each form of verbal auxiliaries and most of which rewrite as only a single word (and any corresponding contractions). 6: An error resolved with the (a) incorrect baseline parse and (b) the correct tively means that the subcategories that we break off must themselves be very frequent in the language. In such a framework, if we try to annotate categories with any detailed lexical information, many sentences either entirely fail to parse, or have only extremely weird parses. The resulting battle against sparsity means that we can only afford to make a few distinctions which have major distributional impact. Even with the individual-lexeme annotations in this section, the grammar still has only 9255 states compared to the 7619 of the baseline model. 6 Annotations Already in the Treebank At this point, one might wonder as to the wisdom of stripping off all treebank functional tags, only to heuristically add other such markings back in to the grammar. By and large, the treebank out-of-the tags, such as have negative utility. Recall that the raw treebank gramwith no annotation or markovization, had an of 72.62% on our development set. With the functional annotation left in, this drops to 71.49%. The v markovization baseline of 77.77% dropped even further, all the way to 72.87%, when these annotations were included. Nonetheless, some distinctions present in the raw trees were valuable. For example, an an could be either a temporal a For the annotation we retained the on and, furthermore, propathe tag down to the tag of the head of the This is illustrated in figure 6, which also shows an of its utility, clarifying that last night is not a plausible compound and facilitating the othunusual high attachment of the smaller the cumulative 82.25%. Note that this technique of pushing the functional tags down to preterminals might be useful more generfor example, locative expand roughly the VPˆVP VPˆVP VB to NPˆVP appear NNˆTMP NPˆNP PPˆNP JJ PPˆNP NPˆNP appear NPˆVP VB NP-TMPˆVP to IN NNS last night JJ times three NNP NN on CD on times three last CNN night NPˆPP NNP CNN NPˆPP IN NNS CD (a) (b) ROOT Distance SˆROOT SˆROOT 7: An error resolved with the (a) incorrect baseline parse and (b) the correct way as all other (usually as but do tend to have different prepositions below A second kind of information in the original trees is the presence of empty elements. Following (1999), the annotation nodes which have an empty subject (i.e., raising and constructions). This brought 82.28%. 7 Head Annotation The notion that the head word of a constituent can affect its behavior is a useful one. However, often the head tag is as good (or better) an indicator of how constituent will We found several head annotations to be particularly effective. First, poshave a very different distribution than – in particular, are only used in the treebank when the leftmost child is possessive (as opposed to other imaginable uses like for York which is left flat). To address this, all possessive This brought total 83.06%. Second, the is very overloaded in the Penn treebank, most severely in that there is no distinction between finite and in- An example of the damage this conflation can do is given in figure 7, where one needs to capture the fact that present-tense verbs do not take bare infinitive To allow the finite/non-finite distinction, and other verb distinctions, all with their head tag, merging all finite forms to a sintag In particular, this also accomplished This was extremely bringing the cumulative 85.72%, 2.66% absolute improvement (more than its solo improvement over the baseline). is part of the explanation of why (Charniak, 2000) finds that early generation of head tags as in (Collins, 1999) is so beneficial. The rest of the benefit is presumably in the availability of the tags for smoothing purposes. Error analysis at this point suggested that many remaining errors were attachment level and conjunction scope. While these kinds of errors are undoubtedly profitable targets for lexical preference, most attachment mistakes were overly high attachments, indicating that the overall right-branching tendency of English was not being captured. Indeed, this tenis a difficult trend to capture in a because often the high and low attachments involve the very same rules. Even if not, attachment height is modeled by a it is somehow explicitly encoded into category labels. More complex parsing models have indirectly overcome this by modeling distance (rather than height). distance is difficult to encode in a – marking nodes with the size of their yields masmultiplies the state Therefore, we wish to find indirect indicators that distinguish high from low ones. In the case of two a with the question of whether the a second modifier of the leftmost should attach lower, inside the first the important distinction is usually that the lower site is a base Collins (1999) captures this by introducing the notion of a base in any dominates only preterminals is with a Further, if an not have non-base it is given one with a unary production. This was helpful, but substantially less than marking base the unary, whose presence actually erased a useful indicator – base are more frequent in subject position than object position, for example. In isolation, the Collins method actually hurt the base- (absolute cost to 0.37%), while skipping the unary insertion added an absolute 0.73% to the and brought the cumulative 86.04%. the case of attachment of a an eiabove or inside a relative clause, the high is distinct from the low one in that the already modified one contains a verb (and the low one may be base well). This is a partial explanation of the utility of verbal distance in Collins (1999). To inability to encode distance naturally in a naive somewhat ironic. In the heart of any the fundamental table entry or chart item is a label over a span, for exan position 0 to position 5. The concrete use of a grammar rule is to take two adjacent span-marked labels and them (for example and into Yet, only the labels are used to score the combination. “ DT “ NPˆS VBZ VPˆVP VPˆS ! . ” ” “ “ NPˆS DT NPˆVP ” . ! ” VPˆS-VBF VBZ (a) (b) NPˆVP buying This is This NN NN panic buying LP LR Exact CB 0 CB Magerman (1995) 84.9 84.6 1.26 56.6 Collins (1996) 86.3 85.8 1.14 59.9 this paper 86.9 85.7 86.3 30.9 1.10 60.3 Charniak (1997) 87.4 87.5 1.00 62.1 Collins (1999) 88.7 88.6 0.90 67.1 Figure 8: Results of the final model on the test set (section 23). this, all nodes which any verbal node with a This the cumulative 86.91%. We also tried marking nodes which dominated prepositions and/or conjunctions, but these features did not help the cumulative hill-climb. The final distance/depth feature we used was an explicit attempt to model depth, rather than use distance and linear intervention as a proxy. With we marked all which contained their right periphery (i.e., as a rightmost descendant). This captured some further attachment trends, and brought us to a final develop- 87.04%. 9 Final Results We took the final model and used it to parse section 23 of the treebank. Figure 8 shows the re- The test set 86.32% for words, already higher than early lexicalized models, though of course lower than the state-of-the-art parsers. 10 Conclusion The advantages of unlexicalized grammars are clear enough – easy to estimate, easy to parse with, and timeand space-efficient. However, the dismal performance of basic unannotated unlexicalized grammars has generally rendered those advantages irrelevant. Here, we have shown that, surprisingly, the maximum-likelihood estimate of a compact unlexiparse on par with early lexicalized parsers. We do not want to argue that lexical selection is not a worthwhile component of a state-ofthe-art parser – certain attachments, at least, require it – though perhaps its necessity has been overstated. Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well.