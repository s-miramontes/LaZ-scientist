1 Introduction. Some constraint-based grammar formalisms incorporate both syntactic and semantic representations within the same structure. For instance, Figure 1 shows representations of typed feature structures (TFSs) for Kim, sleeps and the phrase Kim sleeps, in an HPSG-like representation, loosely based on Sag and Wasow (1999). The semantic representation expressed is intended to be equivalent to r name(x, Kim) ∧ sleep(e, x).1 Note: A similar approach has been used in a large number of implemented grammars (see Shieber (1986) for a fairly early example). It is in many ways easier to work with than A-calculus based approaches (which we discuss further below) and has the great advantage of allowing generalizations about the syntax-semantics interface to be easily expressed. But there are problems. The operations are only specified in terms of the TFS logic: the interpretation relies on an intuitive correspondence with a conventional logical representation, but this is not spelled out. Furthermore the operations on the semantics are not tightly specified or constrained. For instance, although HPSG has the Semantics Principle (Pollard and Sag, 1994) this does not stop the composition process accessing arbitrary pieces of structure, so it is often not easy to conceptually disentangle the syntax and semantics in an HPSG. Nothing guarantees that the grammar is monotonic, by which we mean that in each rule application the semantic content of each daughter subsumes some portion of the semantic content of the mother (i.e., no semantic information is dropped during composition): this makes it impossible to guarantee that certain generation algorithms will work effectively. Finally, from a theoretical perspective, it seems clear that substantive generalizations are being missed. Minimal Recursion Semantics (MRS: Copestake et al (1999), see also Egg (1998)) tightens up the specification of composition a little. It enforces monotonic accumulation of EPs by making all rules append the EPs of their daughters (an approach which was followed by Sag and Wasow (1999)) but it does not fully spectics in TFSs d to other work on unification based grammar. and abstracts away from the specific feature architecture used in individual grammars, but the essential features of the algebra can be encoded in the hierarchy of lexical and constructional type constraints. Our work actually started as an attempt at rational reconstruction of semantic composition in the large grammar implemented by the LinGO project at (available via Semantics and the syntax/semantics interface have accounted for approximately nine-tenths of the development time of the English Resource Grammar (ERG), largely because the account of semantics within is so underdetermined. In this paper, we begin by giving a formal account of a very simplified form of the algebra and in §3, we consider its interpretation. In §4 to §6, we generalize to the full algebra needed to capture the use of in the LinGO English Resource Grammar (ERG). Finally we conclude with some comparisons to the an
2 A simple semantic algebra. a hook and the second part and is the holes. The third element (the lzt) is a bag of elementary predications Intuitively, the hook is a record of the value in the semantic entity that can be used to fill a hole in another entity during composition. The holes record gaps in the semantic form which occur because it represents a syntactically unsaturated structure. Some structures have no holes, such as that for Kim. When structures are composed, a hole in one structure (the semantic head) is filled with the hook of the other (by equating the variables) and their lzts are appended. It should be intuitively obvious that there is a straightforward relationship between this algebra and the shown in Figure 1, although there are other architectures which would share the same encoding. We now give a formal description of the algebra. In this section, we simplify by assuming that each entity has only one hole, which is unlabelled, and only consider two sorts of variables: events and individuals. The set of semantic entities is built from the following vocabulary : The following shows the equivalents of the structures in Figure 1 in our algebra: ify compositional principles and does not formalize composition. We attempt to rectify these problems, by developing an algebra which gives a general way of expressing composition. The semantic algebra lets us specify the allowable operations in a less cumbersome notation than e.g., big big car. This is written relation(arg1, ... ,argn). For instance, like(e, x, y) is a well-formed SEP. Equality Conditions: Where i1 and i2 are indices, i1 = i2 is an equality condition. We write a SSEMENT as: [i1][i2][SEPs]{EQs}. Note for convenience we omit the set markers {} from the hook and hole when there is no possible confusion. The SEPs, and EQs are (partial) descriptions of the fully specified formulae of first order logic. Definition 3 The Semantic Algebra A Semantic Algebra defined on vocabulary V is the algebra (E, op) where: where Tr stands for transitive closure (i.e., if S = {x = y, y = z}, then This definition makes a2 the equivalent of a semantic functor and a1 its argument. Theorem 1 op is a function If a1 = a3 and a2 = a4, then a5 = op(a1, a2) = op(a3, a4) = a6. Thus op is a function. Furthermore, the range of op is within E. So (E, op) is an algebra. We can assume that semantic composition always involves two arguments, since we can define composition in ternary rules etc as a sequence of binary operations. Grammar rules (i.e., constructions) may contribute semantic information, but we assume that this information obeys all the same constraints as the semantics for a sign, so in effect such a rule is semantically equivalent to having null elements in the grammar. The correspondence between the order of the arguments to op and linear order is specified by syntax. We use variables and equality statements to achieve the same effect as coindexation in TFSs. This raises one problem, which is the need to avoid accidental variable equivalences (e.g., accidentally using x in both the signs for cat and dog when building the logical form of A dog chased a cat). We avoid this by adopting a convention that each instance of a lexical sign comes from a set of basic sements that have pairwise distinct variables. The equivalent of coindexation within a lexical sign is represented by repeating the same variable but the equivalent of coindexation that occurs during semantic composition is an equality condition which identifies two different variables. Stating this formally is straightforward but a little long-winded, so we omit it here.
3 Interpretation. The SEPs and EQs can be interpreted with respect to a first order model hE, A, Fi where: Definition 4 Denotations of SEMENTs If a =6 ⊥ is a SEMENT, [a]M = h[i], [i0], Gi where: The truth definition of the SEPs and EQs (which we group together under the term SMRS, for simple MRS) is as follows: Thus, with respect to a model M, an SMRS can be viewed as denoting an element of P(G), where G is the set of variable assignment functions (i.e., elements of G assign the variables e,... and x, .. . their denotations): [smrs]M = {g : g is a variable assignment function and M |=g smrs} We now consider the semantics of the algebra. This must define the semantics of the operation op in terms of a function f which is defined entirely in terms of the denotations of op’s arguments. In other words, [op(a1, a2)] = f([a1], [a2]) for some function f. Intuitively, where the SMRS of the SEMENT a1 denotes G1 and the SMRS of the SEMENT a2 denotes G2, we want the semantic value of the SMRS of op(a1, a2) to denote the following: G1 ∩ G2 ∩ [hook(a1) = hole(a2)] But this cannot be constructed purely as a function of G1 and G2. The solution is to add hooks and holes to the denotations of SEMENTS (cf. Zeevat, 1989). We define the denotation of a SEMENT to be an element of I × I × P(G), where I = E ∪ A, as follows: So, the meanings of SEMENTs are ordered threetuples, consisting of the hook and hole elements (from I) and a set of variable assignment functions that satisfy the SMRS. We can now define the following operation f over these denotations to create an algebra: Definition 5 Semantics of the Semantic Construction Algebra where G0 = {g : g(i1) = g(i02)} And this operation demonstrates that semantic construction is compositional: Theorem 2 Semantics of Semantic Construction is Compositional The mapping [] : hΣ, opi −→ hhI, I, Gi, fi is a homomorphism (so [op(a1, a2)] = f([a1], [a2])). This follows from the definitions of [], op and f.
4 Labelling holes. We now start considering the elaborations necessary for real grammars. As we suggested earlier, it is necessary to have multiple labelled holes. There will be a fixed inventory of labels for any grammar framework, although there may be some differences between variants.3 In HPSG, complements are represented using a list, but in general there will be a fixed upper limit for the number of complements so we can label holes COMP1, COMP2, etc. The full inventory of labels for the ERG is: SUBJ, SPR, SPEC, COMP1, COMP2, COMP3 and MOD (see Pollard and Sag, 1994). To illustrate the way the formalization goes with multiple slots, consider opsubj: Definition 6 The definition of opsubj where Tr stands for transitive closure. There will be similar operations opcomp1, opcomp2 etc for each labelled hole. These operations can be proved to form an algebra (E, opsubj, opcomp1, . . .) in a similar way to the unlabelled case shown in Theorem 1. A little more work is needed to prove that opl is closed on E. In particular, with respect to clause 2 of the above definition, it is necessary to prove that opl(a1, a2) = L or for all labels l', |holel,(opl(a1, a2)) |< 1, but it is straightforward to see this is the case. These operations can be extended in a straightforward way to handle simple constituent coordination of the kind that is currently dealt with in the ERG (e.g., Kim sleeps and talks and Kim and Sandy sleep); such cases involve daughters with non-empty holes of the same label, and the semantic operation equates these holes in the mother SEMENT.
5 Scopal relationships. The algebra with labelled holes is sufficient to deal with simple grammars, such as that in Sag and Wasow (1999), but to deal with scope, more is needed. It is now usual in constraint based grammars to allow for underspecification of quantifier scope by giving labels to pieces of semantic information and stating constraints between the labels. In MRS, labels called handles are associated with each EP. Scopal relationships are represented by EPs with handle-taking arguments. If all handle arguments are filled by handles labelling EPs, the structure is fully scoped, but in general the relationship is not directly specified in a logical form but is constrained by the grammar via additional conditions (handle constraints or hcons).4 A variety of different types of condition are possible, and the algebra developed here is neutral between them, so we will simply use relh to stand for such a constraint, intending it to be neutral between, for instance, =Q (qeq: equality modulo quantifiers) relationships used in MRS and the more usual < relationships from UDRT (Reyle, 1993). The conditions in hcons are accumulated by append. To accommodate scoping in the algebra, we will make hooks and holes pairs of indices and handles. The handle in the hook corresponds to the LTOP feature in MRS. The new vocabulary is: The revised definition of an EP is as in MRS: Definition 7 Elementary Predications (EPs) An EP contains exactly four components: This is written h:r(a1, ... ,an,sa1, ... ,sam). For instance, h:every(x, h1, h2) is an EP.5 We revise the definition of semantic entities to add the hcons conditions and to make hooks and holes pairs of handles and indices. We will not repeat the full composition definition, since it is unchanged from that in §2 apart from the addition of the append operation on hcons and a slight complication of eq to deal with the handle/index pairs: where Tr stands for transitive closure as before and hdle and ind access the handle and index of a pair. We can extend this to include (several) labelled holes and operations, as before. And these revised operations still form an algebra. The truth definition for SEMENTS is analogous to before. We add to the model a set of labels L (handles denote these via g) and a wellfounded partial order G on L (this helps interpret the hcons; cf. Fernando (1997)). A SEMENT then denotes an element of H x ...H x P(G), where the Hs (= L x I) are the new hook and holes. Note that the language E is first order, and we do not use A-abstraction over higher order elements.6 For example, in the standard Montagovian view, a quantifier such as every is represented by the higher-order expression APAQbx(P(x), Q(x)). In our framework, however, every is the following (using qeq conditions, as in the LinGO ERG): A slight complication is that the determiner is also syntactically selected by the N' via the SPR slot (following Pollard and Sag (1994)). However, from the standpoint of the compositional semantics, the determiner is the semantic head, and it is only its SPEC hole which is involved: the N' must be treated as having an empty SPR hole. In the ERG, the distinction between intersective and scopal modification arises because of distinctions in representation at the lexical level. The repetition of variables in the SEMENT of a lexical sign (corresponding to TFS coindexation) and the choice of type on those variables determines the type of modification.
6 Control and external arguments. We need to make one further extension to allow for control, which we do by adding an extra slot to the hooks and holes corresponding to the external argument (e.g., the external argument of a verb always corresponds to its subject position). We illustrate this by showing two uses of expect; note the third slot in the hooks and holes for the external argument of each entity. In both cases, x0e is both the external argument of expect and its subject’s index, but in the first structure x0e is also the external argument of the complement, thus giving the control effect. expect 1 (as in Kim expected to sleep) [he, ee, x0 e]{[hs, x0 e, x0 s]subj, [hc, ec, x0 e]comp1,.. .} [he : expect(ee, x0e, h0e)][h0e =q hc]{} expect 2 (Kim expected that Sandy would sleep) [he, ee, x0e]{[hs, x0e, x0s]subj, [hc, ec, x0c]comp1,.. .} [h : expect(ee, x0e, h0e)][h0e =q hc]{} Although these uses require different lexical entries, the semantic predicate expect used in the two examples is the same, in contrast to Montagovian approaches, which either relate two distinct predicates via meaning postulates, or require an additional semantic combinator. The HPSG account does not involve such additional machinery, but its formal underpinnings have been unclear: in this algebra, it can be seen that the desired result arises as a consequence of the restrictions on variable assignments imposed by the equalities. This completes our sketch of the algebra necessary to encode semantic composition in the ERG. We have constrained accessibility by enumerating the possible labels for holes and by stipulating the contents of the hooks. We believe that the handle, index, external argument triple constitutes all the semantic information that a sign should make accessible to a functor. The fact that only these pieces of information are visible means, for instance, that it is impossible to define a verb that controls the object of its complement.7 Although obviously changes to the syntactic valence features would necessitate modification of the hole labels, we think it unlikely that we will need to increase the inventory further. In combination with the principles defined in Copestake et al (1999) for qeq conditions, the algebra presented here results in a much more tightly specified approach to semantic composition than that in Pollard and Sag (1994).
7 Comparison. Compared with A-calculus, the approach to composition adopted in constraint-based grammars and formalized here has considerable advantages in terms of simplicity. The standard Montague grammar approach requires that arguments be presented in a fixed order, and that they be strictly typed, which leads to unnecessary multiplication of predicates which then have to be interrelated by meaning postulates (e.g., the two uses of expect mentioned earlier). Type raising also adds to the complexity. As standardly presented, Acalculus does not constrain grammars to be monotonic, and does not control accessibility, since the variable of the functor that is A-abstracted over may be arbitrarily deeply embedded inside a Aexpression. None of the previous work on unificationbased approaches to semantics has considered constraints on composition in the way we have presented. In fact, Nerbonne (1995) explicitly advocates nonmonotonicity. Moore (1989) is also concerned with formalizing existing practice in unification grammars (see also Alshawi, 1992), though he assumes Prolog-style unification, rather than TFSs. Moore attempts to formalize his approach in the logic of unification, but it is not clear this is entirely successful. He has to divorce the interpretation of the expressions from the notion of truth with respect to the model, which is much like treating the semantics as a description of a logic formula. Our strategy for formalization is closest to that adopted in Unification Categorial Grammar (Zeevat et al, 1987), but rather than composing actual logical forms we compose partial descriptions to handle semantic underspecification.
8 Conclusions and future work. We have developed a framework for formally specifying semantics within constraint-based representations which allows semantic operations in a grammar to be tightly specified and which allows a representation of semantic content which is largely independent of the feature structure architecture of the syntactic representation. HPSGs can be written which encode much of the algebra described here as constraints on types in the grammar, thus ensuring that the grammar is consistent with the rules on composition. There are some aspects which cannot be encoded within currently implemented TFS formalisms because they involve negative conditions: for instance, we could not write TFS constraints that absolutely prevent a grammar writer sneaking in a disallowed coindexation by specifying a path into the lzt. There is the option of moving to a more general TFS logic but this would require very considerable research to develop reasonable tractability. Since the constraints need not be checked at runtime, it seems better to regard them as metalevel conditions on the description of the grammar, which can anyway easily be checked by code which converts the TFS into the algebraic representation. Because the ERG is large and complex, we have not yet fully completed the exercise of retrospectively implementing the constraints throughout. However, much of the work has been done and the process revealed many bugs in the grammar, which demonstrates the potential for enhanced maintainability. We have modified the grammar to be monotonic, which is important for the chart generator described in Carroll et al (1999). A chart generator must determine lexical entries directly from an input logical form: hence it will only work if all instances of nonmonotonicity can be identified in a grammar-specific preparatory step. We have increased the generator’s reliability by making the ERG monotonic and we expect further improvements in practical performance once we take full advantage of the restrictions in the grammar to cut down the search space.
Acknowledgements. This research was partially supported by the National Science Foundation, grant number IRI9612682. Alex Lascarides was supported by an ESRC (UK) research fellowship. We are grateful to Ted Briscoe, Alistair Knott and the anonymous reviewers for their comments on this paper.