1 Introduction. Automatic capitalization is a practically relevant problem: speech recognition output needs to be capitalized; also, modern word processors perform capitalization among other text proofing algorithms such as spelling correction and grammar checking. Capitalization can be also used as a preprocessing step in named entity extraction or machine translation. We study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to data-driven approaches since vast amounts of “training” data are easily obtainable by simply wiping the case information in text. As in previous approaches, the problem is framed as an instance of the class of sequence labeling problems. A case frequently encountered in practice is that of using mismatched — out-of-domain, in this particular case we used Broadcast News — test data. For example, one may wish to use a capitalization engine developed on newswire text for email or office documents. This typically affects negatively the performance of a given model, and more sophisticated models tend to be more brittle. In the capitalization case we have studied, the relative performance improvement of the MEMM capitalizer over the 1-gram baseline drops from in-domain — WSJ — performance of 45% to 35-40% when used on the slightly mismatched BN data. In order to take advantage of the adaptation data in our scenario, a maximum a-posteriori (MAP) adaptation technique for maximum entropy (MaxEnt) models is developed. The adaptation procedure proves to be quite effective in further reducing the capitalization error of the WSJ MEMM capitalizer on BN test data. It is also quite general and could improve performance of MaxEnt models in any scenario where model adaptation is desirable. A further relative improvement of about 20% is obtained by adapting the WSJ model to Broadcast News (BN) text. Overall, the MEMM capitalizer adapted to BN data achieves 60% relative improvement in accuracy over the 1-gram baseline. The paper is organized as follows: the next section frames automatic capitalization as a sequence labeling problem, presents previous approaches as well as the widespread and highly sub-optimal 1gram capitalization technique that is used as a baseline in most experiments in this work and others. The MEMM sequence labeling technique is briefly reviewed in Section 3. Section 4 describes the MAP adaptation technique used for the capitalization of out-of-domain text. The detailed mathematical derivation is presented in Appendix A. The experimental results are presented in Section 5, followed by conclusions and suggestions for future work.
2 Capitalization as Sequence Tagging. Automatic capitalization can be seen as a sequence tagging problem: each lower-case word receives a tag that describes its capitalization form. Similar to the work in (Lita et al., 2003), we tag each word in a sentence with one of the tags: For training a given capitalizer one needs to convert running text into uniform case text accompanied by the above capitalization tags. For example, PrimeTime continues on ABC .PERIOD Now ,COMMA from Los Angeles ,COMMA Diane Sawyer .PERIOD The text is assumed to be already segmented into sentences. Any sequence labeling algorithm can then be trained for tagging lowercase word sequences with capitalization tags. At test time, the uniform case text to be capitalized is first segmented into sentences1 after which each sentence is tagged. A widespread algorithm used for capitalization is the 1-gram tagger: for every word in a given vocabulary (usually large, 100kwds or more) use the most frequent tag encountered in a large amount of training data. As a special case for automatic capitalization, the most frequent tag for the first word in a sentence is overridden by CAP, thus capitalizing on the fact that the first word in a sentence is most likely capitalized2. Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline. The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well. We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003). In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques. The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output. A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors. Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996). The MEMM approach models the tag sequence T conditionally on the word sequence W, which has a few substantial advantages over the 1-gram tagging approach: More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task. In a similar vein, the work of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs. The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000). We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004). Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario. A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible.
3 MEMM for Sequence Labeling. A simple approach to sequence labeling is the maximum entropy Markov model. The model assigns a probability P(T|W) to any possible tag sequence model is built. The approach we took is the one in (Ratnaparkhi, 1996), which uses xi(W, T i−1 1 ) = {wi, wi−1, wi+1, ti−1, ti−2}. We note that the probability model is causal in the sequencing of tags (the probability assignment for ti only depends on previous tags ti−1, ti−2) which allows for efficient algorithms that search for the most likely tag sequence T∗(W) = arg maxT P(T |W) as well as ensures a properly normalized conditional probability model P(T|W). The probability P(ti|xi(W,T i−1 1 )) is modeled using a maximum entropy model. The next section briefly describes the training procedure; for details the reader is referred to (Berger et al., 1996). The sufficient statistics that are extracted from the training data are tuples the tag assigned in context xi(W, T i−1 1 ) = {wi, wi−1, wi+1, ti−1, ti−2} and # denotes the count with which this event has been observed in the training data. By way of example, the event associated with the first word in the example in Section 2 is (*bdw* denotes a special boundary type): MXC 1 currentword=primetime previousword=*bdw* nextword=continues t1=*bdw* t1,2=*bdw*,*bdw* prefix1=p prefix2=pr prefix3=pri suffix1=e suffix2=me suffix3=ime The maximum entropy probability model P(y|x) uses features which are indicator functions of the type: Assuming a set of features F whose cardinality is F, the probability assignment is made according to: where A = {A1 ... AF} E RF is the set of realvalued model parameters. We used a simple count cut-off feature selection algorithm which counts the number of occurrences of all features in a predefined set after which it discards the features whose count is less than a pre-specified threshold. The parameter of the feature selection algorithm is the threshold value; a value of 0 will keep all features encountered in the training data. The model parameters A are estimated such that the model assigns maximum log-likelihood to the training data subject to a Gaussian prior centered at 0, A ∼ N(0, diag(u2i )), that ensures smoothing (Chen and Rosenfeld, 2000): As shown in (Chen and Rosenfeld, 2000) — and rederived in Appendix A for the non-zero mean case — the update equations are: In our experiments the variances are tied to σi = σ whose value is determined by line search on development data such that it yields the best tagging accuracy.
4 MAP Adaptation of Maximum Entropy Models. In the adaptation scenario we already have a MaxEnt model trained on the background data and we wish to make best use of the adaptation data by balancing the two. A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters. A Gaussian prior for the model parameters A has been previously used in (Chen and Rosenfeld, 2000) for smoothing MaxEnt models. The prior has 0 mean and diagonal covariance: A ∼ N(0, diag(σ2 i)). In the adaptation scenario, the prior distribution used is centered at the parameter values A0 estimated from the background data instead of 0: A ∼ N(A0, diag(σ2i )). The regularized log-likelihood of the adaptation training data becomes: The adaptation is performed in stages: Fadapt \ Fbackground4 introduced in the model receive 0 weight. The resulting model is thus equivalent with the background model. • train the model such that the regularized loglikelihood of the adaptation training data is maximized. The prior mean is set at A0 = Abackground · 0; · denotes concatenation between the parameter vector for the background model and a 0-valued vector of length |Fadapt\ Fbackground |corresponding to the weights for the new features. As shown in Appendix A, the update equations are very similar to the 0-mean case: The effect of the prior is to keep the model parameters λi close to the background ones. The cost of moving away from the mean for each feature fi is specified by the magnitude of the variance σi: a small variance σi will keep the weight λi close to its mean; a large variance σi will make the regularized log-likelihood (see Eq. 3) insensitive to the prior on λi, allowing the use of the best value λi for modeling the adaptation data. Another observation is that not only the features observed in the adaptation data get updated: even if E˜p(x,y)[fi] = 0, the weight λi for feature fi will still get updated if the feature fi triggers for a context x encountered in the adaptation data and some predicted value y — not necessarily present in the adaptation data in context x. In our experiments the variances were tied to σi = σ whose value was determined by line search on development data drawn from the adaptation data. The common variance σ will thus balance optimally the log-likelihood of the adaptation data with the A0 mean values obtained from the background data. Other tying schemes are possible: separate values could be used for the Fadapt \ Fbackground and Fbackground feature sets, respectively. We did not experiment with various tying schemes although this is a promising research direction. Another possibility to adapt the background model is to do minimum KL divergence (MinDiv) training (Pietra et al., 1995) between the background exponential model B — assumed fixed — and an exponential model A built using the Fbackground U Fadapt feature set. It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 — following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models — then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5. However, we wish to point out that the equivalence holds only if the feature set for the new model A is Fbackground U Fadapt. The straightforward application of MinDiv training — by using only the Fadapt feature set for A — will not result in an equivalent procedure to ours. In fact, the difference in performance between this latter approach and ours could be quite large since the cardinality of Fbackground is typically several orders of magnitude larger than that of Fadapt and our approach also updates the weights corresponding to features in Fbackground \ Fadapt. Further experiments are needed to compare the performance of the two approaches.
5 Experiments. The baseline 1-gram and the background MEMM capitalizer were trained on various amounts of WSJ (Paul and Baker, 1992) data from 1987 — files WS87_{001-126}. The in-domain test data used was file WS94_000 (8.7kwds). As for the adaptation experiments, two different sets of BN data were used, whose sizes are summarized in Table 1: We have proceeded building both 1-gram and MEMM capitalizers using various amounts of background training data. The model sizes for the 1gram and MEMM capitalizer are presented in Table 2. Count cut-off feature selection has been used for the MEMM capitalizer with the threshold set at 5, so the MEMM model size is a function of the training data. The 1-gram capitalizer used a vocabulary of the most likely 100k wds derived from the training data. We first evaluated the in-domain and out-ofdomain relative performance of the 1-gram and the MEMM capitalizers as a function of the amount of training data. The results are presented in Table 3. The MEMM capitalizer performs about 45% better domain (WSJ-test) and out-of-domain (BN-dev) data for various amounts of training data than the 1-gram one when trained and evaluated on Wall Street Journal text. The relative performance improvement of the MEMM capitalizer over the 1gram baseline drops to 35-40% when using out-ofdomain Broadcast News data. Both models benefit from using more training data. We have then adapted the best MEMM model built on 20Mwds on the two BN data sets (CNN/ABC) and compared performance against the 1-gram and the unadapted MEMM models. There are a number of parameters to be tuned on development data. Table 4 presents the variation in model size with different count cut-off values for the feature selection procedure on the adaptation data. As can be seen, very few features are added to the background model. Table 5 presents the variation in log-likelihood and capitalization accuracy on the CNN adaptation training and development data, respectively. The adaptation procedure was found cut-off threshold used for feature selection on CNNtrn adaptation data; the entry corresponding to the cut-off threshold of 106 represents the number of features in the background model to be insensitive to the number of reestimation iterations, and, more surprisingly, to the number of features added to the background model from the adaptation data, as shown in 5. The most sensitive parameter is the prior variance σ2, as shown in Figure 1; its value is chosen to maximize classification accuracy on development data. As expected, low values of σ2 result in no adaptation at all, whereas high values of σ2 fit the training data very well, and result in a dramatic increase of training data loglikelihood and accuracies approaching 100%. count cut-off and σ2 variance values; log-likelihood and accuracy on adaptation data CNN-trn as well as accuracy on held-out data CNN-dev; the background model results (no new features added) are the entries corresponding to the cut-off threshold of Finally, Table 6 presents the results on test data for 1-gram, background and adapted MEMM. As can be seen, the background MEMM outperforms the 1-gram model on both BN test sets by about 35-40% relative. Adaptation improves performance even further by another 20-25% relative. Overall, the adapted models achieve 60% relative reduction in capitalization error over the 1-gram baseline on both BN test sets. An intuitively satisfying result is the fact that the cross-test set performance (CNN adapted model evaluated on ABC data and the other way around) is worse than the adapted one.
6 Conclusions and Future Work. The MEMM tagger is very effective in reducing both in-domain and out-of-domain capitalization error by 35%-45% relative over a 1-gram capitalization model. We have also presented a general technique for adapting MaxEnt probability models. It was shown to be very effective in adapting a background MEMM capitalization model, improving the accuracy by 20-25% relative. An overall 50-60% reduction in capitalization error over the standard 1-gram baseline is achieved. A surprising result is that the adaptation performance gain is not due to adding more, domain-specific features but rather making better use of the background features for modeling the in-domain data. As expected, adding more background training data improves performance but a very small amount of domain specific data also helps significantly if one can make use of it in an effective way. The “There’s no data like more data” rule-of-thumb could be amended by “..., especially if it’s the right data!”. As future work we plan to investigate the best way to blend increasing amounts of less-specific background training data with specific, in-domain data for this and other problems. Another interesting research direction is to explore the usefulness of the MAP adaptation of MaxEnt models for other problems among which we wish to include language modeling, part-of-speech tagging, parsing, machine translation, information extraction, text routing.
Acknowledgments. Special thanks to Adwait Ratnaparkhi for making available the code for his MEMM tagger and MaxEnt trainer.