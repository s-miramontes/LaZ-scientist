1 Introduction. One of the primary problems that NLP researchers who work in new languages or new domains encounter is a lack of available annotated data. Collection of data is neither easy nor cheap. The construction of the Penn Treebank significantly improved performance for English systems dealing in the &quot;traditional&quot; NLP domains (eg parsing, part-of-speech tagging, etc). However, for a new language, a similar investment of effort in time and money is most likely prohibitive, if not impossible. Faced with the costs associated with data acquisition, rationalists may argue that it would be more cost effective to construct systems of handcoded rule lists that capture the linguistic characteristics of the task at hand, rather than spending comparable effort annotating data and expecting the same knowledge to be acquired indirectly by a machine learning system. The question we are trying to address then is: for a given cost assumption, which approach would be the most effective. Although learning curves showing performance relative to amount of training data are common in the machine learning literature, these are inadequate for comparing systems with different sources of training data or supervision. This is especially true when a human rule-based approach and empirical learning are evaluated relative to effort invested. Such a multi-factor cost analysis is long overdue. This paper will conclude with a comprehensive cost model exposition and analysis, and an empirical study contrasting human rule-writing versus annotation-based learning approaches that are sensitive to these cost models.
2 Base Noun Phrase Chunking. The domain in which our experiments are performed is base noun phrase chunking. A significant amount of work has been done in this domain and many different methods have been applied: Church's PARTS (1988) program used a Markov model; Bourigault (1992) used heuristics along with a grammar; Voutilainen's NPTool (1993) used a lexicon combined with a constraint grammar; Juteson and Katz (1995) used repeated phrases; Veenstra (1998), Argamon, Dagan & Krymolowski (1998), Daelemans, van den Bosch & Zavrel (1999) and Tjong Kim Sang & Veenstra (1999) used memory-based systems; Ramshaw & Marcus (1999) and Cardie & Pierce (1998) used rule-based systems, Munoz et al. (1999) used a Winnow-based system, and the XTAG Research Group(1998) used a tree-adjoining grammar. Of all the systems, Ramshaw & Marcus' transformation rule-based system had the best published performance (f-measure 92.0) for several years, and is regarded as the de facto standard for the domain. Although several systems have recently achieved slightly higher published results (Munoz et al. : 92.8, Tjong Kim Sang & Veenstra: 92.37, XTAG Research Group: 92.4), their algorithms are significantly more costly, or not feasible, to implement in an active learning framework. To facilitate contrastive studies, we have evaluated our active learning and cost model comparisons using Ramshaw & Marcus' system as the reference algorithm in these experiments.
3 Active Learning from Annotation. Supervised statistical machine learning systems have traditionally required large amounts of annotated data from which to extract linguistic properties of the task at hand. However, not all data is created equal. A random distribution of annotated data contains much redundant information. By intelligently choosing the training examples which get passed to the learner, it is possible to provide the necessary amount of information with less data. Active learning attempts to perform this intelligent sampling of data to reduce annotation costs without damaging performance. In general, these methods calculate the usefulness of an example by first having the learner classify it, and then seeing how uncertain that classification was. The idea is that the more uncertain the example, the less well modeled this situation is, and therefore, the more useful it would be to have this example annotated. Seung, Opper and Sompolinsky (1992) and Freund et al. (1997) proposed a theoretical queryby-committee approach. Such an approach uses multiple models (or a committee) to evaluate the data, and candidates for annotation (or queries) are drawn from the pool of examples in which the models disagree. Furthermore, Freund et al. prove that, under some situations, the generalization error decreases exponentially with the number of queries. On the experimental side, active learning has been applied to several different problems. Lewis & Gale (1994), Lewis & Catlett (1994) and Liere & Tadepalli (1997) all applied it to text categorization; Engelson & Dagan (1996) applied it to part-of-speech tagging. Each approach has its own way of determining uncertainty in examples. Lewis & Gale used a probabilistic classifier and picked the examples e whose class-conditional a posteriori probability P(Cie) is closest to 0.5 (for a 2-class problem). Engelson & Dagan implemented a committee of learners, and used vote entropy to pick examples which had the highest disagreement among the learners. In addition, Engelson & Dagan also investigate several different selection techniques in depth. To our knowledge, this paper constitutes the first work to apply active learning to base noun phrase chunking, or to apply active learning to a transformation-learning paradigm (Brill, 1995) for any application. Since a transformation-based learner does not give a probabilistic output, we are not able to use Lewis & Gale's method for determining uncertainty. Our experimental framework thus uses the query by committee paradigm with batch selection: In our experiments, the initial corpus C that we used consisted of sections 15-18 of the Wall Street Journal Treebank (Marcus et al., 1993), which is also the training set used by Ramshaw & Marcus (1999). The initial t sentences were the first 100 sentences of the training corpus, and x = 50 sentences were picked at each iteration. Sets of 50 sentences were selected because it takes approximately 15-30 minutes for humans to annotate them, a reasonable amount of work and time for the annotator to spend before taking a break while the machine selects the next set. The parameter m, which denotes the number of models to train, was set at 3, which could be expected to give us reasonable labelling variation over the samples, but also would not cause the processing phase to take a long time. To divide the corpus into the different subsets in Step 3, we tried using two approaches: bagging and n-fold partitioning. In bagging, we randomly sentences selected by active learning and annotated sentences selected sequentially shows that active learning reduces the amount of data needed to reach a given level of performance by approximately a factor of two. Most of the published work on active learning are simulations of an idealized situation. One has a large annotated corpus, and the new tags for the &quot;newly annotated&quot; sentences are simply drawn from what was observed in the annotated corpus, as if the gold standard annotator was producing this feedback in real time, while the test set itself is, of course, not used for this feedback. This is an idealized situation, since it assumes that a true active learning situation would have access to someone who could annotate with perfect consistency to the gold standard corpus annotation conventions. Because our goal is to investigate the relative costs of rule writing versus annotation, it is essential that we use a realistic model of annotation. Therefore, we decided to do a fully-fledged active learning annotation experiment, with real time human supervision, rather than assume the simulated feedback of actual Treebank annotators. We developed an annotation tool that is modeled on MITRE's Alembic Workbench software (Day et al., 1997), but written in Java for platform-independence. To enable data storage and the active learning sample selection to take place on the more powerful machines in our lab rather than the user's home machine, the tool was designed with network support so that it could communicate with our servers over the internet. Our real-time active learning experiment subjects were seven graduate students in computer science. Five of them are native English speakers, but none had any formal linguistics training. The initial training set T is the first 100 sentences of Ramshaw & Marcus' training set. To acquaint the subjects with the Treebank conventions, they were first asked to spend some time in a feedback phase, where they would annotate up to 50 sentences (they were allowed to stop at any time) drawn from the initial 100 sentences in T. The sentences were annotated one at a time, and the Treebank annotation was shown to them after every sentence. On average, the annotators spent around 15 minutes on this feedback phase before deciding that they were comfortable enough with the convention. The active learning phase follows the feedback phase. The f-complement disagreement measure was used to select 50 sentences from the rest of Ramshaw & Marcus' training set and the annotator was instructed to annotate them. The annotated sentences were then sent back to the server. The system chose the next 50 sentences. The experiment consists of 10 iterations, during which the annotators were allowed to make use of the original 100 sentences as a reference corpus. After completing all 10 iterations, they were asked to annotate a further 100 consecutive sentences drawn randomly from the test set. The purpose of this final annotation was to judge how well annotators tag sentences drawn with the true distribution from the test corpus, as we shall see in section 5. On average, the annotators took 17 minutes to annotate each set of 50 sentences, ranging from 8 to 30 minutes. The average amount of time the server took to run the active learning algorithm and select the next batch of sentences was approximately 3 minutes, a rest break for the annotators. The analysis of the results is presented in section 5.
4 Learning by Rules. In previous work, Brill & Ngai (1999) showed that under certain circumstances, it is possible for humans writing rules to perform as well as a stateof-the-art machine learning system for base noun phrase chunking. What that study did not address, however, was the cost of the human labor and/or machine cycles involved to construct such a system, nor the relative cost of obtaining the training data for the machine learning system. This paper will estimate and contrast these costs relative to performance. To investigate the costs of a human rule-writing system, we used a similar framework to that of Brill & Ngai. The system was written as a cgi script which could be accessed across the web from a browser such as Netscape or Internet Explorer. Like Brill & Ngai's 1999 approach, our rules were based on Perl regular expressions. However, instead of explicitly defining rule actions and having different kinds of rules, our rules implicitly define their actions by using different symbols to denote the placement of the base noun phrase-enclosing parentheses prior to and after the application of the rule. Table 1 presents a comparison of our rule format against that of Brill & Ngai's. The rules presented here may be considered less cumbersome and more intuitive. In a way that is similar to Brill & Ngai's system, our rules were translated into Perl regular expressions and evaluated on the corpus. New rules are appended onto the end of the list and each rule applied in order, in the paradigm of a transformation-based rule list. The rule-writing experiments were conducted by a group of 17 advanced computer science students, using the identical test set as in the annotation experiments and the same initial 100 gold standard sentences for both initial bracketing standards guidance and rule-quality feedback throughout their work. The time that the students spent on the task varied widely, from a minimum of 1.5 hours to a maximum of 9 hours, with an average of 5 hours. Because we captured and saved every change the students made to their rule list and logged every mouse click they made while doing the experiment, it was possible for us to trace the performance of the system as a function of time. Figure 2 shows the rule list constructed by one of the subjects. The quantitative results of the rule-writing experiments are presented in the next section.
5 Experiment Results--- Rule Writing vs. Annotation. This section will analyze and compare the performance of systems constructed with hand-built rules with systems that were trained from data selected during real-time active learning. The performance of Ramshaw & Marcus' system trained on the annotations of each subject in the real-time active learning experiments, and the performance achieved by the manually constructed systems of the top 6 rule writers are shown in Figures 3 and 4, depicting the performance achieved by each individual system. The x-axes show the time spent by each human subject (either annotating or writing rules) in minutes; the y-axes show the f-measure performance achieved by the systems built using the given level of supervision. It is important to note that when comparing the curves in Figure 4, experimental conditions across groups were kept as equal as possible, with any known potential biases favoring the rules-writing group. First, both groups began with the identical environments, while initially variable across methods, have already been borne and to the extent that both interface systems port to new languages and domains with relative ease, the incremental development costs for new trials are likely to be relatively low and comparable. Finally, this cost model takes into account the cost of developing or acquiring the So gold standard tagged data (e.g. from the Treebank) to provide initial and/or incremental training feedback to the annotator or rule writer to help force consistency with the gold standard. We have found that both learning modes can benefit from this high quality feedback, but the cost x of developing such a high-quality resource for new languages or domains is unknown, but likely to be higher than the non-expert labor costs employed here. 7 Rules vs. Annotation-based Learning — Advantages and Disadvantages In the previous sections, we investigated the performance differences and resource costs involved for using humans to write rules vs. using them for annotations. In this section, we will further compare these system development paradigms. Annotation-based human participation has a number of significant practical advantages relative to developing a system by manual rule-writing: interdependencies when adding or revising rules, ultimately bounding continued rulesystem growth by cognitive load factors. • Annotation-based learning can more effectively combine the efforts of multiple individuals. The tagged sentences from different data sets can be simply concatenated to form a larger data set with broader coverage. In contrast, it is much more difficult, if not impossible, for a rule writer to resume where another one left off. Furthermore, combining rule lists is very difficult because of the tight and complex interaction between successive rules. Combination of rule writing systems is therefore limited to voting or similar classifier techniques which can be applied to annotation systems as well. • Rule-based learning requires a larger skill set, including not only the linguistic knowledge needed for annotation, but also competence in regular expressions and an ability to grasp the complex interactions within a rule list. These added skill requirements naturally shrink the pool of viable participants and increases their likely cost. • Based on empirical observation, the performance of rule writers tend to exhibit considerably more variance, while systems trained on annotation tend to yield much more consistent results. • Finally, the current performance of annotation-based training is only a lower bound based on the performance of current learning algorithms. Since annotated data can be used by other current or future machine learning techniques, subsequent algorithmic improvements may yield performance improvements without any change in the data. In contrast, the performance achieved by a set of rules is effectively final without additional human revision. The potential disadvantages of annotationbased system development for applications such as base NP chunking are limited. Given the cost models presented in Section 6, one potential negative scenario would be an environment where the machine cost significantly outweighed human labor costs, or where access to active learning and annotation infrastructure was unavailable or costly. However, under normal circumstances where machine analysis of text is pursued, and public domain access to our annotation and active learning toolkits is assumed, such a scenario is unlikely.
8 Conclusion. This paper has illustrated that there are potentially compelling practical and performance advantages to pursuing active-learning based annotation rather than rule-writing to develop base noun phrase chunkers. The relative balance depends ultimately on one's cost model, but given the goal of minimizing total human labor cost, it appears to be consistently more efficient and effective to invest these human resources in systemdevelopment via annotation rather than rule writing.
9 Acknowledgements. The authors would like to thank Jan Hajic, Eric Brill, Radu Florian, and various members of the Natural Language Processing Lab at Johns Hopkins for their valuable feedback regarding this work.