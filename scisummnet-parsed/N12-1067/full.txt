1 Introduction. Progress in natural language processing (NLP) research is driven and measured by automatic evaluation methods. Automatic evaluation allows fast and inexpensive feedback during development, and objective and reproducible evaluation during testing time. Grammatical error correction is an important NLP task with useful applications for second language learning. Evaluation for error correction is typically done by computing F1 measure between a set of proposed system edits and a set of humanannotated gold-standard edits (Leacock et al., 2010). Unfortunately, evaluation is complicated by the fact that the set of edit operations for a given system hypothesis is ambiguous. This is due to two reasons. First, the set of edits that transforms one string into another is not necessarily unique, even at the token level. Second, edits can consist of longer phrases which introduce additional ambiguity. To see how this can affect evaluation, consider the following source sentence and system hypothesis from the recent Helping Our Own (HOO) shared task (Dale and Kilgarriff, 2011) on grammatical error correction: Source: Our baseline system feeds word into PB-SMT pipeline. Hypot. : Our baseline system feeds a word into PB-SMT pipeline. The HOO evaluation script extracts the system edit (c —* a), i.e., inserting the article a. Unfortunately, the gold-standard annotation instead contains the edits (word —* {a word, words}). Although the extracted system edit results in the same corrected sentence as the first gold-standard edit option, the system hypothesis was considered to be invalid. In this work, we propose a method, called MaxMatch (M2), to overcome this problem. The key idea is that if there are multiple possible ways to arrive at the same correction, the system should be evaluated according to the set of edits that matches the gold-standard as often as possible. To this end, we propose an algorithm for efficiently computing the set of phrase-level edits with the maximum overlap with the gold standard. The edits are subsequently scored using F1 measure. We test our method in the context of the HOO shared task and show that our method results in a more accurate evaluation for error correction. The remainder of this paper is organized as follows: Section 2 describes the proposed method; Section 3 presents experimental results; Section 4 discusses some details of grammar correction evaluation; and Section 5 concludes the paper.
2 Method. We begin by establishing some notation. We consider a set of source sentences 5 = {s1, ... , sn} together with a set of hypotheses H = {h1, ... , hn} generated by an error correction system. Let G = {g1, ... , gn} be the set of gold standard annotations for the same sentences. Each annotation gi = {g1i , ... , gr i } is a set of edits. An edit is a triple (a, b, C), consisting of: The remainder of this section describes a method for solving these two steps. We start by describing how to construct an edit lattice from a source-hypothesis pair. Then, we show that finding the optimal sequence of edits is equivalent to solving a shortest path search through the lattice. Finally, we describe how to evaluate the edits using F1 measure. We start from the well-established Levenshtein distance (Levenshtein, 1966), which is defined as the minimum number of insertions, deletions, and substitutions needed to transform one string into another. The Levenshtein distance between a source sentence si = s1i,...,ski and a hypothesis hi = h1i, ... , hl i can be efficiently computed using a two dimensional matrix that is filled using a classic dynamic programming algorithm. We assume that both si and hi have been tokenized. The matrix for the example from Section 1 is shown in Figure 1. By performing a simple breadth-first search, similar to the Viterbi algorithm, we can extract the lattice of all shortest paths that lead from the top-left corner to the bottom-right corner of the Levenshtein matrix. Each vertex in the lattice corresponds to a cell in the Levenshtein matrix, and each edge in the lattice corresponds to an atomic edit operation: inserting a token, deleting a token, substituting a token, or leaving a token unchanged. Each path through the lattice corresponds to a shortest sequence of edits that transform si into hi. We assign a unit cost to each edge in the lattice. We have seen that annotators can use longer phrases and that phrases can include unchanged words from the context, e.g., the gold edit from the example in Section 1 is (4, 5, word, {a word, words}). However, it seems unrealistic to allow an arbitrary number of unchanged words in an edit. In particular, we want to avoid very large edits that cover complete sentences. Therefore, we limit the number of unchanged words by a parameter u. To allow for phrase-level edits, we add transitive edges to the lattice as long as the number of unchanged words in the newly added edit is not greater than u and the edit changes at least one word. Let e1 = (a1, b1, C1) and e2 = (a2, b2, C2) be two edits corresponding to adjacent edges in the lattice, with the first end offset b1 being equal to the second start offset a2. We can combine them into a new edit e3 = (a1, b2, C1 + C2), where C1 + C2 is the concatenation of strings C1 and C2. The cost of a transitive edge is the sum of the costs of its parts. The lattice extracted from the example sentence is shown in Figure 2. Our goal is to find the sequence of edits ei with the maximum overlap with the gold standard. Let L = (V, E) be the edit lattice graph from the last section. We change the cost of each edge whose corsystem feeds a word into PB-SMT pipeline . responding edit has a match in the gold standard to −(u + 1) × |E|. An edit e matches a gold edit g iff they have the same offsets and e’s correction is included in g: Then, we perform a single-source shortest path search with negative edge weights from the start to the end vertex1. This can be done efficiently, for example with the Bellman-Ford algorithm (Cormen et al., 2001). As the lattice is acyclic, the algorithm is guaranteed to terminate and return a shortest path. Theorem 1. The set of edits corresponding to the shortest path has the maximum overlap with the gold standard annotation. Proof. Let e = e1, ... , ek be the edit sequence corresponding to the shortest path and let p be the number of matched edits. Assume that there exists another edit sequence e0 with higher total edge weights but p0 > p matching edits. Then we have where q and q0 denote the combined cost of all nonmatching edits in the two paths, respectively. Because p0 − p ≥ 1, the right hand side is at most −(u + 1)|E|. Because q and q0 are positive and bounded by (u + 1)|E|, the left hand side cannot be smaller than or equal to −(u + 1)|E|. This is a contradiction. Therefore there cannot exist such an edit sequence e0, and e is the sequence with the maximum overlap with the gold-standard annotation. What is left to do is to evaluate the set of edits with respect to the gold standard. This is done by computing precision, recall, and F1 measure (van Rijsbergen, 1979) between the set of system edits {e1, ... , en} and the set of gold edits {g1, ... , gn} for all sentences where we define the intersection between ei and gi as ei ∩ gi = {e ∈ ei  |∃ g ∈ gi(match(e, g))}. (6)
3 Experiments and Results. We experimentally test our M2 method in the context of the HOO shared task. The HOO test data2 consists of text fragments from NLP papers together with manually-created gold-standard corrections (see (Dale and Kilgarriff, 2011) for details). We test our method by re-scoring the best runs of the participating teams3 in the HOO shared task with our M2 scorer and comparing the scores with the official HOO scorer, which simply uses GNU wdiff4 to extract system edits. We obtain each system’s output and segment it at the sentence level according to the gold standard sentence segmentation. The source sentences, system hypotheses, and corrections are tokenized using the Penn Treebank standard (Marcus et al., 1993). The character edit offsets are automatically converted to token offsets. We set the parameter u to 2, allowing up to two unchanged words per edit. The results are shown in Table 1. Note that the M2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. We can see that the M2 scorer results in higher scores than the official scorer for all systems, showing that the official scorer missed some valid edits. For example, the M2 scorer finds 155 valid edits for the UI system compared to 141 found by the official scorer, and 83 valid edits for the NU system, compared to 78 by the official scorer. We manually inspect the output of the scorers and find that the M2 scorer indeed extracts the correct edits matching the gold standard where possible. Examples are shown in Table 2.
4 Discussion. The evaluation framework proposed in this work differs slightly from the one in the HOO shared task. Sentence-by-sentence. We compute the edits between source-hypothesis sentence pairs, while the HOO scorer computes edits at the document level. As the HOO data comes in a sentencesegmented format, both approaches are equivalent, while sentence-by-sentence is easier to work with. Token-level offsets. In our work, the start and end of an edit are given as token offsets, while the HOO data uses character offsets. Character offsets make the evaluation procedure very brittle as a small change, e.g., an additional whitespace character, will affect all subsequent edits. Character offsets also introduce ambiguities in the annotation, e.g., whether a comma is part of the preceding token. Alternative scoring. The HOO shared task defines three different scores: detection, recognition, and correction. Effectively, all three scores are F1 measures and only differ in the conditions on when an edit is counted as valid. Additionally, each score is reported under a “with bonus” alternative, where a system receives rewards for missed optional edits. The F1 measure defined in Section 2.3 is equivalent to correction without bonus. Our method can be used to compute detection and recognition scores and scores with bonus as well.
5 Conclusion. We have presented a novel method, called MaxMatch (M2), for evaluating grammatical error correction. Our method computes the sequence of phrase-level edits that achieves the highest overlap with the gold-standard annotation. Experiments on the HOO data show that our method overcomes deficiencies in the current evaluation method. The M2 scorer is available for download at http://nlp.comp.nus.edu.sg/software/.
Acknowledgments. We thank Chang Liu for comments on an earlier draft. This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.