Introduction. This paper gives an overview of the NAACL-2010 Workshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk. A number of recent papers have evaluated the effectiveness of us ing Mechanical Turk to create annotated data for natural language processing applications. The lowcost, scalable workforce available through Mechan ical Turk (MTurk) and other crowdsourcing sites opens new possibilities for annotating speech and text, and has the potential to dramatically changehow we create data for human language technolo gies. Open questions include: What kind of researchis possible when the cost of creating annotated train ing data is dramatically reduced? What new tasks should we try to solve if we do not limit ourselves to reusing existing training and test sets? Can complex annotation be done by untrained annotators? Howcan we ensure high quality annotations from crowd sourced contributors?To begin addressing these questions, we orga nized an open-ended $100 shared task. Researchers were given $100 of credit on Amazon MechanicalTurk to spend on an annotation task of their choosing. They were required to write a short paper de scribing their experience, and to distribute the datathat they created. They were encouraged to ad dress the following questions: How did you conveythe task in terms that were simple enough for non experts to understand? Were non-experts as good as experts? What did you do to ensure quality? How quickly did the data get annotated? What is the cost per label? Researchers submitted a 1 page proposalto the workshop organizers that described their in tended experiments and expected outcomes. The organizers selected proposals based on merit, and awarded $100 credits that were generously provided by Amazon Mechanical Turk. In total, 35 credits were awarded to researchers. Shared task participants were given 10 days to run experiments between the distribution of the credit and the initial submission deadline. 30 papers were submitted to the shared task track, of which 24 were accepted. 14 papers were submitted to the generaltrack of which 10 were accepted, giving a 77% ac ceptance rate and a total of 34 papers. Shared taskparticipants were required to provide the data col lected as part of their experiments. All of the shared task data is available on the workshop website.
Mechanical Turk. . Amazon?s Mechanical Turk1 is an online marketplace for work. Amazon?s tag line for Mechani cal Turk is artificial artificial intelligence, and thename refers to a historical hoax from the 18th cen 1http://www.mturk.com/ 1 < 1 1-2 2-4 4-8 8-20 20-40 40+ < 1 HIT 1-5 5-10 10-20 20-50 50-100 100-200 200-500 500-1k 1k-5k 5k+ HITs < $1 $1-5 $5-10 10-20 20-50 50-100 100-200 $200+ 5% 17% 22% 26% 19% 10% 3% 100% 1% 6% 9% 13% 19% 17% 12% 13% 5% 4% 1% 100% 11% 36% 22% 15% 11% 4% 2% 0% 100% 0% 5% 10% 15% 20% 25% 30% < 1 1-2 2-4 4-8 8-20 40+ Hours spent on Mechanical Turk per week 0% 5% 10% 15% 20% < 1 HIT 5-10 20-50 100-200 500-1k 5k+ HITs Number of HITs completed per week 0% 10% 20% 30% 40% < $1 $5-10 20-50 100-200 Weekly income from Mechanical Turk Figure 1: Time spent, HITs completed, and amount earned from a survey of 1,000 Turkers by Ipeirotis (2010). tury where a chess-playing automaton appeared tobe able to beat human opponents using a mecha nism, but was, in fact, controlled by a person hidinginside the machine. These hint at the the primary focus of the web service, which is to get people to per form tasks that are simple for humans but difficult for computers. The basic unit of work on MTurk is even called a Human Intelligence Task (HIT). Amazon?s web service provides an easy way to pay people small amounts of money to perform HITs. Anyone with an Amazon account can either submit HITs or work on HITs that were submitted by others. Workers are referred to as ?Turkers? and people designing the HITs are called ?Requesters.? Requesters set the amount that they will pay for each item that is completed. Payments are frequently as low as $0.01. Turkers are free to select whichever HITs interest them.], and to disregard HITs that they find uninteresting or which they deem pay too little.Because of its focus on tasks requiring human in telligence, Mechanical Turk is obviously applicable to the field of natural language processing. Snow et al (2008) used Mechanical Turk to inexpensively collect labels for several NLP tasks including wordsense disambiguation, word similarity, textual en tailment, and temporal ordering of events. Snow et al. had two exciting findings. First, they showed that a strong correlation between non-expert and expertannotators can be achieved by combining the judgments of multiple non-experts, for instance by voting on each label using 10 different Turkers. Cor relation and accuracy of labeling could be furtherimproved by weighting each Turker?s vote by cal ibrating them on a small amount of gold standarddata created by expert annotators. Second, they col lected a staggering number of labels for a very small amount of money. They collected 21,000 labels for just over $25. Turkers put in over 140+ hours worth Why do you complete tasks in MTurk? US India To spend free time fruitfully and get cash (e.g., instead of watching TV) 70% 60% For ?primary? income purposes (e.g., gas, bills, groceries, credit cards) 15% 27% For ?secondary? income purposes, pocket change (for hobbies, gadgets) 60% 37% To kill time 33% 5% The tasks are fun 40% 20% Currently unemployed or part time work 30% 27% Table 1: Motivations for participating on Mechanical Turk from a survey of 1,000 Turkers by Ipeirotis (2010). of human effort to generate the labels. The amount of participation is surprisingly high, given the small payment. Turker demographics Given the amount of work that can get done for so little, it is natural to ask: who would contribute so much work for so little pay, and why? The answers to these questions are often mysterious becauseAmazon does not provide any personal informa tion about Turkers (each Turker is identifiable only through a serial number like A23KO2TP7I4KK2). Ipeirotis (2010) elucidates some of the reasons by presenting a demographic analysis of Turkers. He built a profile of 1000 Turkers by posting a survey toMTurk and paying $0.10 for people to answer questions about their reasons for participating on Me chanical Turk, the amount that they earn each week,and how much time they spend, as well as demo graphic information like country of origin, gender, age, education level, and household income. One suspicion that people often have when theyfirst hear about MTurk is that it is some sort of dig ital sweatshop that exploits workers in third world countries. However, Ipeirotis reports that nearly half 2 (47%) of the Turkers who answered his survey were from the United States, with the next largest group (34%) coming from India, and the remaining 19% spread between 66 other countries. Table 1 gives the survey results for questions relating to why people participate on MechanicalTurk. It shows that most US-based workers use Me chanical Turk for secondary income purposes (to have spending money for hobbies or going out), but that the overwhelming majority of them use it to spend their time more fruitfully (i.e., instead of watching TV). The economic downturn mayhave increased participation, with 30% of the US based Turkers reporting that they are unemployedor underemployed. The public radio show Mar ketplace recently interviewed unemployed Turkers(Rose, 2010). It reports that they earn a little in come, but that they do not earn enough to make a living. Figure 1 confirms this, giving a break down of how much time people spend on Mechanical Turk each week, how many HITs they complete, and how much money they earn. Most Turkers spend less than 8 hours per week on Mechanical Turk, and earn less than $10 per week through the site.
Quality Control. . Ipeirotis (2010) reports that just over half of Turkers have a college education. Despite being reasonably well educated, it is important to keep in mind that Turkers do not have training in specialized subjects like NLP. Because the Turkers are non-experts, and because the payments are generally so low, quality control is an important consideration when creating data with MTurk.Amazon provides three mechanisms to help en sure quality: ? Requesters have the option of rejecting the work of individual Turkers, in which case they are not paid.2 Turkers can also be blocked from doing future work for a requester. 2Since the results are downloadable even if they are rejected, this could allow unscrupulous Requesters to abuse Turkers by rejecting all of their work, even if it was done well. Turkers have message boards at http://www.turkernation.com/,where they discuss Requesters. They even have a Firefox plu gin called Turkopticon that lets them see ratings of how goodthe Requesters are in terms of communicating with Turkers, be ing generous and fair, and paying promptly. Requesters can specify that each HIT should be redundantly completed by several different Turkers. This allows higher quality labels to be selected, for instance, by taking the majority label. Requesters can require that all workers meeta particular set of qualifications, such as suffi cient accuracy on a small test set or a minimum percentage of previously accepted submissions.Amazon provides two qualifications that a Requester can use by default. These are past HIT Approval Rate and Location. The location qualifica tion allows the Requester to have HITs done only byresidents of a certain country (or to exclude Turk ers from certain regions). Additionally, Requesters can design custom Qualification Tests that Turkers must complete before working on a particular HIT. These can be created through the MTurk API, and can either be graded manually or automatically. An important qualification that isn?t among Amazon?s default qualifications is language skills. One might design a qualification test to determine a Turker?s ability to speak Arabic or Farsi before allowing them to do part of speech tagging in those languages, for instance. There are several reasons that poor quality data might be generated. The task may be too complex orthe instructions might not be clear enough for Turk ers to follow. The financial incentives may be too low for Turkers to act conscientiously, and certain HIT designs may allow them to simply randomly click instead of thinking about the task. Mason and Watts (2009) present a study of financial incentives on Mechanical Turk and find, counterintuitively, thatincreasing the amount of compensation for a partic ular task does not tend to improve the quality of theresults. Anecdotally, we have observed that some times there is an inverse relationship between the amount of payment and the quality of work, because it is more tempting to cheat on high-paying HITs ifyou don?t have the skills to complete them. For ex ample, a number of Turkers tried to cheat on an Urdu to English translation HIT by cutting-and-pastingthe Urdu text into an online machine translation sys tem (expressly forbidden in the instructions) because we were paying the comparatively high amount of $1. 3 3.1 Designing HITs for quality control. We suggest designing your HITs in a way that will deter cheating or that will make cheating obvious. HIT design is part of the art of using MTurk. It can?t be easily quantified, but it has a large impact on the outcome. For instance, we reduced cheating on our translation HIT by changing the design so that we displayed images of the Urdu sentences instead of text, which made it impossible to copy-and-paste into an MT system for anyone who could not type in Arabic script. Another suggestion is to include information within the data that you upload to MTurk that will not be displayed to the Turkers, but will be useful to you when reviewing the HITs. For example, we include machine translation output along with the source sentences. Although this is not displayed to Turkers, when we review the Turkers? translations we compare them to the MT output. This allows us to reject translations that are identical to the MT, or which are just random sentences that are unrelated to the original Urdu. We also use a javascript3 to gather the IP addresses of the Turkers and do geolocation to look up their location. Turkers in Pakistan require less careful scrutiny since they are more likely to be bilingual Urdu speakers than those in Romania, for instance.CrowdFlower4 provides an interface for design ing HITs that includes a phase for the Requester toinput gold standard data with known labels. Insert ing items with known labels alongside items which need labels allows a Requester to see which Turkers are correctly replicating the gold standard labels andwhich are not. This is an excellent idea. If it is possi ble to include positive and negative controls in your HITs, then do so. Turkers who fail the controls can be blocked and their labels can be excluded from the final data set. CrowdFlower-generated HITs even display a score to the Turkers to give them feedback on how well they are doing. This provides training for Turkers, and discourages cheating. 3http://wiki.github.com/callison-burch/ mechanical_turk_workshop/geolocation 4http://crowdflower.com/ 3.2 Iterative improvements on MTurk. Another class of quality control on Mechanical Turk is through iterative HITs that build on the output of previous HITs. This could be used to have Turkersjudge whether the results from a previous HIT con formed to the instructions, and whether it is of high quality. Alternately, the second set of Turkers couldbe used to improve the quality of what the first Turkers created. For instance, in a translation task, a sec ond set of US-based Turkers could edit the English produced by non-native speakers. CastingWords,5 a transcription company that usesTurker labor, employs this strategy by having a first pass transcription graded and iteratively improvedin subsequent passes. Little et al (2009) even de signed an API specifically for running iterative tasks on MTurk.6
Recommended Practices. . Although it is hard to define a set of ?best practices? that applies to all HITs, or even to all NLP HITs, we recommend the following guidelines to Requesters.First and foremost, it is critical to convey instruc tions appropriately for non-experts. The instructions should be clear and concise. To calibrate whetherthe HIT is doable, you should first try the task your self, and then have a friend from outside the field try it. This will help to ensure that the instructions are clear, and to calibrate how long each HIT will take (which ought to allow you to price the HITs fairly).If possible, you should insert positive and nega tive controls so that you can quickly screen out bad Turkers. This is especially important for HITs thatonly require clicking buttons to complete. If pos sible, you should include a small amount of gold standard data in each HIT. This will allow you todetermine which Turkers are good, but will also al low you weight the Turkers if you are combiningthe judgments of multiple Turkers. If you are having Turkers evaluate the output of systems, then ran domize the order that the systems are shown in. When publishing papers that use Mechanical Turk as a source of training data or to evaluate the outputof an NLP system, report how you ensured the qual ity of your data. You can do this by measuring the 5http://castingwords.com/ 6http://groups.csail.mit.edu/uid/turkit/ 4inter-annotator agreement of the Turkers against ex perts on small amounts of gold standard data, or by stating what controls you used and what criteria youused to block bad Turkers. Finally, whenever possi ble you should publish the data that you generate on Mechanical Turk (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it.
Related work. . In the past two years, several papers have published about applying Mechanical Turk to a diverse set ofnatural language processing tasks, including: cre ating question-answer sentence pairs (Kaisser andLowe, 2008), evaluating machine translation qual ity and crowdsouring translations (Callison-Burch,2009), paraphrasing noun-noun compouds for Se mEval (Butnariu et al, 2009), human evaluation oftopic models (Chang et al, 2009), and speech tran scription (McGraw et al, 2010; Marge et al, 2010a; Novotney and Callison-Burch, 2010a). Others haveused MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sen timent classification (Hsueh et al, 2009) or doingquixotic things like doing human-in-the-loop min imum error rate training for machine translation (Zaidan and Callison-Burch, 2009).Some projects have demonstrated the super scalability of crowdsourced efforts. Deng et al(2009) used MTurk to construct ImageNet, an anno tated image database containing 3.2 million that arehierarchically categorized using the WordNet ontol ogy (Fellbaum, 1998). Because Mechanical Turkallows researchers to experiment with crowdsourc ing by providing small incentives to Turkers, other successful crowdsourcing efforts like Wikipedia or Games with a Purpose (von Ahn and Dabbish, 2008) also share something in common with MTurk.
Shared Task. . The workshop included a shared task in which participants were provided with $100 to spend on Me chanical Turk experiments. Participants submitted a 1 page proposal in advance describing their intended use of the funds. Selected proposals were provided $100 seed money, to which many participants added their own funds. As part of their participation, each team submitted a workshop paper describing theirexperiments as well as the data collected and de scribed in the paper. Data for the shared papers is available at the workshop website.7This section describes the variety of data types ex plored and collected in the shared task. Of the 24 participating teams, most did not exceed the $100 that they were awarded by a significant amount. Therefore, the variety and extent of data described inthis section is the result of a minimal $2,400 invest ment. This achievement demonstrates the potential for MTurk?s impact on the creation and curation of speech and language corpora. 6.1 Traditional NLP Tasks. An established core set of computational linguistictasks have received considerable attention in the nat ural language processing community. These include knowledge extraction, textual entailment and word sense disambiguation. Each of these tasks requires a large and carefully curated annotated corpus to train and evaluate statistical models. Many of the shared task teams attempted to create new corpora for these tasks at substantially reduced costs using MTurk. Parent and Eskenazi (2010) produce new corpora for the task of word sense disambiguation. The study used MTurk to create unique word definitions for 50 words, which Turkers then also mapped onto existing definitions. Sentences containing these 50words were then assigned to unique definitions ac cording to word sense. Madnani and Boyd-Graber (2010) measured theconcept of transitivity of verbs in the style of Hop per and Thompson (1980), a theory that goes beyond simple grammatical transitivity ? whether verbs take objects (transitive) or not ? to capture the amount of action indicated by a sentence. Videos that portrayedverbs were shown to Turkers who described the ac tions shown in the video. Additionally, sentencescontaining the verbs were rated for aspect, affirma tion, benefit, harm, kinesis, punctuality, and volition.The authors investigated several approaches for elic iting descriptions of transitivity from Turkers. Two teams explored textual entailment tasks. Wang and Callison-Burch (2010) created data for 7http://sites.google.com/site/ amtworkshop2010/ 5recognizing textual entailment (RTE). They submit ted 600 text segments and asked Turkers to identifyfacts and counter-facts (unsupported facts and con tradictions) given the provided text. The resulting collection includes 790 facts and 203 counter-facts.Negri and Mehdad (2010) created a bi-lingual entailment corpus using English and Spanish entail ment pairs, where the hypothesis and text come from different languages. The authors took a publicly available English RTE data set (the PASCAL-RTE3 dataset1) and created an English-Spanish equivalent by having Turkers translating the hypotheses into Spanish. The authors include a timeline of their progress, complete with total cost over the 10 days that they ran the experiments.In the area of natural language generation, Heil man and Smith (2010) explored the potential of MTurk for ranking of computer generated questions about provided texts. These questions can be used to test reading comprehension and understanding. 60 Wikipedia articles were selected, for each of which20 questions were generated. Turkers provided 5 ratings for each of the 1,200 questions, creating a sig nificant corpus of scored questions. Finally, Gordon et al (2010) relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge (factoids) fromnews and Wikipedia articles. Factoids were pro vided by the KNEXT knowledge extraction system. 6.2 Speech and Vision. While MTurk naturally lends itself to text tasks, several teams explored annotation and collection ofspeech and image data. We note that one of the pa pers in the main track described tools for collecting such data (Lane et al, 2010). Two teams used MTurk to collect text annotations on speech data. Marge et al (2010b) identified easy and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from4 different speakers, as well as other types of an notations. Kunath and Weinberger (2010) collectedratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors ob tained multiple annotations for each speech sample, and tracked the native language of each annotator, allowing for an analysis of rating accuracy between native English and non-native English annotators. Novotney and Callison-Burch (2010b) used MTurk to elicit new speech samples. As part of aneffort to increase the accessibility of public knowl edge, such as Wikipedia, the team prompted Turkers to narrate Wikipedia articles. This required Turkers to record audio files and upload them. An additionalHIT was used to evaluate the quality of the narra tions. A particularly creative data collection approach asked Turkers to create handwriting samples and then to submit images of their writing (Tong et al, 2010). Turkers were asked to submit handwrittenshopping lists (large vocabulary) or weather descrip tions (small vocabulary) in either Arabic or Spanish. Subsequent Turkers provided a transcription and atranslation. The team collected 18 images per lan guage, 2 transcripts per image and 1 translation per transcript. 6.3 Sentiment, Polarity and Bias. Two papers investigated the topics of sentiment, po larity and bias. Mellebeek et al (2010) used severalmethods to obtain polarity scores for Spanish sen tences expressing opinions about automative topics. They evaluated three HITs for collecting such data and compared results for quality and expressiveness. Yano et al (2010) evaluated the political bias of blogposts. Annotators labeled 1000 sentences to deter mine biased phrases in political blogs from the 2008 election season. Knowledge of the annotators own biases allowed the authors to study how bias differs on the different ends of the political spectrum. 6.4 Information Retrieval. Large scale evaluations requiring significant humanlabor for evaluation have a long history in the in formation retrieval community (TREC). Grady and Lease (2010) study four factors that influence Turker performance on a document relevance search task. The authors present some negative results on how these factors influence data collection. For further work on MTurk and information retrieval, readers are encouraged to see the SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation.8 8http://www.ischool.utexas.edu/?cse2010/ call.htm 6 6.5 Information Extraction. Information extraction (IE) seeks to identify specific types of information in natural languages. The IE papers in the shared tasks focused on new domains and genres as well as new relation types.The goal of relation extraction is to identify rela tions between entities or terms in a sentence, such asborn in or religion. Gormley et al (2010) automat ically generate potential relation pairs in sentences by finding relation pairs appearing in news articles as given by a knowledge base. They ask Turkers ifa sentence supports a relation, does not support a re lation, or whether the relation makes sense. They collected close to 2500 annotations for 17 different person relation types.The other IE papers explored new genres and domains. Finin et al (2010) obtained named entity an notations (person, organization, geopolitical entity)for several hundred Twitter messages. They conducted experiments using both MTurk and Crowd Flower. Yetisgen-Yildiz et al (2010) explored medical named entity recognition. They selected100 clinical trial announcements from ClinicalTrials.gov. 4 annotators for each of the 100 announce ments identified 3 types of medical entities: medical conditions, medications, and laboratory test. 6.6 Machine Translation. The most popular shared task topic was MachineTranslation (MT). MT is a data hungry task that re lies on huge corpora of parallel texts between two languages. Performance of MT systems dependson the size of training corpora, so there is a con stant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore thepotential of MTurk in the creation of MT paral lel corpora for evaluation and training. Bloodgoodand Callison-Burch replicate the NIST 2009 Urdu English test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that isstill effective for comparing MT systems in an eval uation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasibility of creating cheap corpora for high and low resource languages.Two papers focused on the creation and evaluation of paraphrases. Denkowski et al (2010) generated and evaluated 728 paraphrases for Arabic English translation. MTurk was used to identify correct and fix incorrect paraphrases. Over 1200 high quality paraphrases were created. Buzek et al. (2010) evaluated error driven paraphrases forMT. In this setting, paraphrases are used to sim plify potentially difficult to translate segments of text. Turkers identified 1780 error regions in 1006 English/Chinese sentences. Turkers provided 4821 paraphrases for these regions. External resources can be an important part of an MT system. Irvine and Klementiev (2010) created lexicons for low resource languages. They evaluated translation candidates for 100 English words in 32 languages and solicited translations for 10 additional languages. Higgins et al (2010) expanded namelists in Arabic by soliciting common Arabic nicknames. The 332 collected nicknames were primar ily provided by Turkers in Arab speaking countries (35%), India (46%), and the United States (13%). Finally, Zaidan and Ganitkevitch (2010) explored how MTurk could be used to directly improve an MTgrammar. Each rule in an Urdu to English translation system was characterized by 12 features. Turkers were provided examples for which their feedback was used to rescore grammar productions di rectly. This approach shows the potential of fine tuning an MT system with targeted feedback from annotators.
Future Directions. . Looking ahead, we can?t help but wonder what im pact MTurk and crowdsourcing will have on thespeech and language research community. Keeping in mind Niels Bohr?s famous exhortation ?Pre 7 diction is very difficult, especially if it?s about the future,? we attempt to draw some conclusions and predict future directions and impact on the field. Some have predicted that access to low cost, highly scalable methods for creating language andspeech annotations means the end of work on unsupervised learning. Many a researcher has advocated his or her unsupervised learning approach because of annotation costs. However, if 100 exam ples for any task are obtainable for less than $100,why spend the time and effort developing often infe rior unsupervised methods? Such a radical change is highly debatable, in fact, one of this paper?s authors is a strong advocate of such a position while the other disagrees, perhaps because he himself works on unsupervised methods. Certainly, we can agree that the potential exists for a change in focus in a number of ways.In natural language processing, data drives re search. The introduction of new large and widelyaccessible data sets creates whole new areas of re search. There are many examples of such impact, the most famous of which is the Penn Treebank (Marcus. et al, 1994), which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network (Radev et al, 2009). Other examples include the CoNLL named entity corpus (Sang and Meulder (2003) with 348 citationson Google Scholar), the IMDB movie reviews senti ment data (Pang et al (2002) with 894 citations) and the Amazon sentiment multi-domain data (Blitzer et al. (2007) with 109 citations) . MTurk means that creating similar data sets is now much cheaper and easier than ever before. It is highly likely that new MTurk produced data sets will achieve prominenceand have significant impact. Additionally, the cre ation of shared data means more comparison and evaluation against previous work. Progress is madewhen it can be demonstrated against previous ap proaches on the same data. The reduction of data cost and the rise of independent corpus producers likely means more accessible data. More than a new source for cheap data, MTurk isa source for new types of data. Several of the pa pers in this workshop collected information about the annotators in addition to their annotations. Thiscreates potential for studying how different user demographics understand language and allow for targeting specific demographics in data creation. Be yond efficiencies in cost, MTurk provides access to a global user population far more diverse than those provided by more professional annotation settings. This will have a significant impact on low resource languages as corpora can be cheaply built for a much wider array of languages. As one example, Irvineand Klementiev (2010) collected data for 42 languages without worrying about how to find speakers of such a wide variety of languages. Addition ally, the collection of Arabic nicknames requires a diverse and numerous Arabic speaking population (Higgins et al, 2010). In addition to extending into new languages, MTurk also allows for the creation of evaluation sets in new genres and domains, which was the focus of two papers in this workshop (Fininet al, 2010; Yetisgen-Yildiz et al, 2010). We ex pect to see new research emphasis on low resource languages and new domains and genres. Another factor is the change of data type and itsimpact on machine learning algorithms. With pro fessional annotators, great time and care are paid to annotation guidelines and annotator training. These are difficult tasks with MTurk, which favors simple intuitive annotations and little training. Many papersapplied creative methods of using simpler annota tion tasks to create more complex data sets. This process can impact machine learning in a numberof ways. Rather than a single gold standard, annotations are now available for many users. Learning across multiple annotations may improve sys tems (Dredze et al, 2009). Additionally, even withefforts to clean up MTurk annotations, we can ex pect an increase in noisy examples in data. This will push for new more robust learning algorithms that are less sensitive to noise. If we increase the size of the data ten-fold but also increase the noise, can learning still be successful? Another learning area of great interest is active learning, which has long relied on simulated user experiments. New workevaluated active learning methods with real users us ing MTurk (Baker et al, 2009; Ambati et al, 2010; Hsueh et al, 2009; ?). Finally, the composition ofcomplex data set annotations from simple user in puts can transform the method by which we learncomplex outputs. Current approaches expect exam ples of labels that exactly match the expectation ofthe system. Can we instead provide lower level sim 8 pler user annotations and teach systems how to learn from these to construct complex output? This would open more complex annotation tasks to MTurk. A general trend in research is that good ideascome from unexpected places. Major transformations in the field have come from creative new ap proaches. Consider the Penn Treebank, an ambitious and difficult project of unknown potential. Such large changes can be uncommon since they are often associated with high cost, as was the Penn Treebank.However, MTurk greatly reduces these costs, en couraging researchers to try creative new tasks. Forexample, in this workshop Tong et al (2010) col lected handwriting samples in multiple languages. Their creative data collection may or may not have a significant impact, but it is unlikely that it would have been tried had the cost been very high. Finally, while obtaining new data annotationsfrom MTurk is cheap, it is not trivial. Workshop par ticipants struggled with how to attract Turkers, howto price HITs, HIT design, instructions, cheating de tection, etc. No doubt that as work progresses, so will a communal knowledge and experience of how to use MTurk. There can be great benefit in new toolkits for collecting language data using MTurk, and indeed some of these have already started to emerge (Lane et al, 2010)9. AcknowledgementsThanks to Sharon Chiarella of Amazon?s Mechan ical Turk for providing $100 credits for the shared task, and to CrowdFlower for allowing free use of their tool to workshop participants.Research funding was provided by the NSF under grant IIS-0713448, by the European Commis sion through the EuroMatrixPlus project, and by the DARPA GALE program under Contract No. HR0011-06-2-0001. The views and findings are the authors? alone.