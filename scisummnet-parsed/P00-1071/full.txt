1 Background. Finding the answer to a question by returning a small fragment of a text, where the answer actually lies, is profoundly different from the task of information retrieval (IR) or information extraction (IE). Current IR systems allow us to locate full documents that might contain pertinent information, leaving it to the user to extract the answer from a ranked list of texts. In contrast, IE systems extract the information of interest, provided it has been presented in a predefined, target representation, known as template. The immediate solution of combining IR and IE techniques for question/answering (Q/A) is impractical since IE systems are known to be highly dependent on domain knowledge, and furthermore, the template generation is not performed automatically. Our methodology of finding answers in large collections of documents relies on natural language processing (NLP) techniques in novel ways. First, we perform the processing of the question by combining syntactic information, resulting from a shallow parse, with semantic information that characterizes the question (e.g. question type, question focus). Secondly, the search for the answer is based on a novel form of indexing, called paragraph indexing (Moldovan and Mihalcea 2000). Finally, in order to extract answers and to evaluate their correctness, we use a battery of abductive techniques (Hobbs et al.1993), some based on empirical methods, some on lexicosemantic information. The principles that have guided our paragraph indexing and the abductive inference of the answers are reported in (Harabagiu and Maiorano 1999). 2 Overview of the LASSO Q/A System The architecture of LASSO (Moldovan, Harabagiu et. al 1999) comprises three modules: Question Processing module, Paragraph Indexing module and Answer Processing module. Given a question, of open-ended nature, expressed in natural language, we first process the question by creating a representation of the information requested. Thus we automatically find (a) the question type from the taxonomy of questions built into the system, (b) the expected answer type from the semantic analysis of the question, and most importantly, (c) the question focus defined as the main information required by that question. Furthermore, the Question Processing module also identifies the keywords from the question, which are passed to the Paragraph Indexing module, as illustrated by Figure 1. what questions, as what is ambiguous and it says nothing about the information asked by the question. The same applies to many other question types. The problem was solved by defining a concept named focus. A focus is a word or a sequence of words which define the question and disambiguate the question by indicating what the question is looking for. For example, for the question What is the largest city in Germany?, the focus is largest city. Knowing the focus and the question type it becomes easier to determine the type of the answer sought, namely: the name of the largest city in Germany. The focus is also important in determining the list of keywords for query formation. Often, many question words do not appear in the answer, and that is because their role is just to form the context of the question. For example, in the question In 1990, what day of the week did Christmas fall on?, the focus is day of the week, a concept that is unlikely to occur in the answer. In such situations, the focus should not be included in the list of keywords considered for detecting the answer. The process of extracting keywords is based on a set of ordered heuristics. Each heuristic returns a set of keywords that are added in the same order to the question keywords. We have implemented eight different heuristics. Initially, only the keywords returned by the first six heuristics are considered. If further keywords are needed in the retrieval loop, keywords provided by the other two heuristics are added. When keywords define an exceedingly specific query, they are dropped in the reversed order in which they have been entered. The heuristics are: Table 2 lists two questions from the TREC8 competition together with their associated keywords. The Table also illustrates the trace of keywords until the paragraphs containing the answer were found. For question 26, the paragraphs containing the answers could not be found before dropping many of the initial keywords. In contrast, the answer for question 13 was found when the verb rent was added to the Boolean query. Q-26 What is the name of the &quot;female&quot; counterpart to El Nino, which results in cooling temperatures and very dry weather ?
4 Paragraph Indexing. The Information Retrieval Engine for LASSO is related to the Zprise IR search engine available from NIST. There were several features of the Zprise IR engine which were not conducive to working within the design of LASSO. Because of this, a new IR engine was generated to support LASSO without the encumbrance of these features. The index creation was, however, kept in its entirety. The Zprise IR engine was built using a cosine vector space model. This model does not allow for the extraction of those documents which include all of the keywords, but extracts documents according to the similarity measure between the document and the query as computed by the cosine of the angle between the vectors represented by the document and the query. This permits documents to be retrieved when only one of the keywords is present. Additionally, the keywords present in one retrieved document may not be present in another retrieved document. LASSO's requirements are much more rigid. LASSO requires that documents be retrieved only when all of the keywords are present in the document. Thus, it became necessary to implement a more precise determinant for extraction. For the early work, it was determined that a Boolean discriminate would suffice provided that the operators AND and OR were implemented. It was also necessary to provide the ability to organize queries through the use of parentheses. We opted for the Boolean indexing as opposed to vector indexing (Buckley et al.1998) because Boolean indexing increases the recall at the expense of precision. That works well for us since we control the retrieval precision with the PARAGRAPH operator which provides document filtering. In addition, the Boolean indexing requires less processing time than vector indexing, and this becomes important when the collection size increases. To facilitate the identification of the document sources, the engine was required to put the document id in front of each line in the document. The index creation includes the following steps: normalize the SGML tags, eliminate extraneous characters, identify the words within each document, stem the terms (words) using the Porter stemming algorithm, calculate the local (document) and global (collection) weights, build a comprehensive dictionary of the collection, and create the inverted index file. The number of documents that contain the keywords returned by the Search Engine may be large since only weak Boolean operators were used. A new, more restrictive operator was introduced: PARAGRAPH n. This operator searches like an AND operator for the words in the query with the constraint that the words belong only to some n consecutive paragraphs, where n is a controllable positive integer. The parameter n selects the number of paragraphs, thus controlling the size of the text retrieved from a document considered relevant. The rationale is that most likely the information requested is found in a few paragraphs rather than being dispersed over an entire document. Paragraph ordering is performed by a radix sort that involves three different scores: the largest Same_word_sequence-score, the largest Distance-score and the smallest Missing_keyword-score. The definition of these scores is based on the notion of paragraphwindow. Paragraph-windows are determined by the need to consider separately each match of the same keyword in the same paragraph. For example, if we have a set of keywords fkl, Id, k3, k41 and in a paragraph kl and Id are matched each twice, whereas k3 is matched only once, and k4 is not matched, we are going to have four different windows, defined by the keywords: [kl-matchl, Id-matchl, k3], [k1-match2,0-match1, k3], [kl-matchl, Idmatch2, k3], and [k1-match2, k2-match, k3]. A window comprises all the text between the lowest positioned keyword in the window and the highest position keyword in the window. For each paragraph window we compute the following scores: The radix sorting takes place across all the window scores for all paragraphs.
5 Answer Processing. The Answer Processing module identifies and extracts the answer from the paragraphs that contain the question keywords. Crucial to the identification of the answer is the recognition of the answer type. Since almost always the answer type is not explicit in the question or the answer, we need to rely on lexicosemantic information provided by a parser to identify named entities (e.g. names of people or organizations, monetary units, dates and temporal/locative expressions, products and others). The recognition of the answer type, through the semantic tag returned by the parser, creates a candidate answer. The extraction of the answer and its evaluation are based on a set of heuristics. The Parser The parser combines information from broad coverage lexical dictionaries with semantic information that contributes to the identification of the named entities. Since part-ofspeech tagging is an intrinsic component of a parser, we have extended Brill's part-ofspeech tagger in two ways. First, we have acquired new tagging rules and secondly, we have unified the dictionaries of the tagger with semantic dictionaries derived from the Gazetteers and from WordNet (Miller 1995). In addition to the implementation of grammar rules, we have implemented heuristics capable of recognizing names of persons, organizations, locations, dates, currencies and products. Similar heuristics recognize named entities successfully in IE systems. Having these capabilities proved to be useful for locating the possible answers within a set of candidate paragraphs. Answer Extraction The parser enables the recognition of the answer candidates in the paragraph. Each expression tagged by the parser with the answer type becomes one of the answer candidates for a paragraph. Similar to the paragraphwindows used in ordering the paragraphs, we establish an answer-window for each answer candidate. To evaluate the correctness of each answer candidate, a new evaluation metric is computed for each answer-window. We use the following scores: The overall score for a given answer candidate is computed by: Currently the combined score represents an un-normalized measure of answer correctness. The answer extraction is performed by choosing the answer candidate with the highest score. Some of the scores approximate very simple abductions. For example, the recognition of keywords or other question words in an apposition determines the Punctuation_sign-score, the Same_parse_subtreescore, the Comma_3_words-score and the Same_sentence-score to go up. Moreover, the same sequence score gives higher plausibility to answer candidates that contain in their window sequences of question words that follow the same orders in the question. This score approximates the assumption that concepts are lexicalized in the same manner in the question and in the answer. However, the combined score allows for keywords and question words to be matched in the same order. Table 3 illustrates some of the scores that were attributed to the candidate answers LASSO has extracted successfully. Currently we compute the same score for both short and long answers, as we analyze in the same way the answer windows.
6 Performance evaluation. Several criteria and metrics may be used to measure the performance of a QA system. In TREC-8, the performance focus was on accuracy. Table 4 summarizes the scores provided by NIST for our system. The metric used by NIST for accuracy is described in (Voorhees and Tice 1999). Another important performance parameter is the processing time to answer a question. On the average, the processing time per question is 61 sec., and the time ranges from 1 sec. to 540 sec. There are four main components of the overall time: (1) question processing time, (2) paragraph search time, (3) paragraph ordering time, and (4) answer extraction time. Table 5 summarizes the relative time spent on each processing component. The answer extraction dominates the processing time while the question processing part is negligible.
7 Conclusions. In principle, the problem of finding one or more answers to a question from a very large set of documents can be addressed by creating a context for the question and a knowledge representation of each document and then match the question context against each document representation. This approach is not practical yet since it involves advanced techniques in knowledge representation of open text, reasoning, natural language processing, and indexing that currently are beyond the technology state of the art. On the other hand, traditional information retrieval and extraction techniques alone can not be used for question answering due to the need to pinpoint exactly an answer in large collections of open domain texts. Thus, a mixture of natural language processing and information retrieval methods may be the solution for now. In order to better understand the nature of the QA task and put this into perspective, we offer in Table 6 a taxonomy of question answering systems. It is not sufficient to classify only the types of questions alone, since for the same question the answer may be easier or more difficult to extract depending on how the answer is phrased in the text. Thus we classify the QA systems, not the questions. We provide a taxonomy based on three critePercentage of questions in top 5 NIST score Short answer Long answer ria that we consider important for building question answering systems: (1) knowledge base, (2) reasoning, and (3) natural language processing and indexing techniques. Knowledge bases and reasoning provide the medium for building question contexts and matching them against text documents. Indexing identifies the text passages where answers may lie, and natural language processing provides a framework for answer extraction. Out of the 153 questions that our system has answered, 136 belong to Class 1, and 17 to Class 2. Obviously, the questions in Class 2 are more difficult as they require more powerful natural language and reasoning techniques. As we look for the future, in order to address questions of higher classes we need to handle real-time knowledge acquisition and classification from different domains, coreference, metonymy, special-purpose reasoning, semantic indexing and other advanced techniques.