1 Motivation. Conceptual natural language processing typically involves case frame instantiation to recognize events and role objects in text. For example, an NLP system designed for a business domain might use case frames to recognize business activities such as mergers, acquisitions, or joint ventures. The case frames would contain slots for thematic roles that are associated with each event. For example, case frames for business activities might contain slots for the agents (e.g., companies or people who merge or acquire others) and the objects (e.g., companies that are acquired or products that are being developed). Unfortunately, acquiring a good set of case frames for a domain can be a major undertaking. Case frames are often lexically indexed so that each case frame is tailored for a specific set of linguistic expressions and their expectations. For example, one case frame might be activated by the phrase &quot;joint venture&quot; and contain slots to recognize the partner cornpanies and objects of the joint venture (e.g., child company or product). A different case frame might be activated by the word &quot;acquisition&quot; and contain slots to recognize the agent (e.g., the acquiring company or person) and the object of the acquisition. Devising the right set of role assignments for a case frame can be surprisingly difficult. Determining the necessary thematic roles for an event is relatively straightforward, but anticipating how they will be manifested syntactically can be tricky. For example, consider some of the manually defined case frames that were used to recognize terrorist events in the UMass MUC-4 system (Lehnert et al., 1992a). The ATTACK case frame shows a very common situation where multiple conceptual roles map to the same syntactic role. When &quot;attacked&quot; is used as a passive verb, the subject may be either a victim or a physical target, and the object of the preposition &quot;by&quot; may be the agent or instrument. It is easy for a person to miss one of these possibilities when defining the case frame manually. The ACCUSATION case frame shows that the same conceptual role can be filled by multiple syntactic roles. For example, the person accused of a crime may be the direct object of &quot;blamed&quot; (e.g., &quot;The government blamed John Smith for the crime&quot;) or may be the object of the preposition &quot;on&quot; (e.g., &quot;The government blamed the crime on John Smith&quot;). The SABOTAGE case frame illustrates that a multitude of prepositional arguments may be necessary for some case frames. Prepositional arguments are especially difficult for a person to anticipate when defining case frames by hand. It is virtually impossible for a person to correctly and completely anticipate all of the arguments that are necessary for a large set of case frames for a domain. Omitting an important argument will result in the failure to recognize role objects in certain syntactic constructions. In practice, people often turn to the corpus to look for argument structures that they might have missed. For example, the UMass/MUC-4 terrorism case frames were developed by applying an initial set of case frames to hundreds of sample texts and looking for places where the case frames failed to recognize desired information. But this approach is extremely timeconsuming unless the answers are known in advance (i.e., the information that should have been extracted), which is unrealistic for most applications. It should be possible, however, to learn case frame structures automatically from a text corpus. Toward this end, we have been developing a corpus-based approach to conceptual case frame acquisition. Our approach builds upon earlier work on corpus-based methods for generating extraction patterns (Riloff, 1996b) and semantic lexicons (Riloff and Shepherd, 1997). Our new system constructs conceptual case frames by learning semantic preferences for extraction patterns and merging syntactically compatible patterns into more complex structures. The resulting case frames can have slots for multiple role objects and each slot has a set of learned selectional restrictions for its role object. The first section of this paper begins with background about AutoSlog-TS, a corpus-based system for generating extraction patterns automatically, and the extraction patterns that it generates. The following section presents a new corpus-based algorithm that uses the extraction patterns as a building block for constructing conceptual case frame structures. We then show several examples of case frames that were generated automatically using this method. Finally, we present experimental results that compare the performance of the case frames with the extraction patterns. Our results show that the conceptual case frames produce substantially fewer false hits than the extraction patterns.
2 AutoSlog-TS: generating simple extraction patterns. In the past few years, several systems have been developed to generate structures for information extraction automatically. However, these systems usually need special training resources that are expensive to obtain. One of the first such systems was AutoSlog (Riloff, 1993; Riloff, 1996a), which generates extraction patterns from annotated text. The patterns produced by AutoSlog achieved 98% of the performance of hand-crafted extraction patterns, but AutoSlog requires a training corpus that is manually tagged with domain-specific annotations. Another early system, PALKA (Kim and Moldovan, 1993), requires domain-specific frames with keyword lists, CRYSTAL (Soderland et al., 1995) requires an annotated training corpus, RAPIER (Calif and Mooney, 1997) requires filled templates, and LIEP (Huffman, 1996) requires keywords and annotated training examples. PALKA and CRYSTAL also require semantic lexicons, while LIEP uses domain-specific concept recognizers. AutoSlog-TS (Riloff, 1996b) is a derivative of AutoSlog that was designed to obviate the need for special training data. AutoSlog-TS generates extraction patterns using only a &quot;preclassified&quot; training corpus: one set of texts that are relevant to the domain, and one set of texts that are irrelevant. The texts do not need to be annotated in any way. AutoSlog-TS generates the same simple extraction patterns that AutoSlog generates. Each pattern is activated by a keyword in a specific linguistic context. For example, one extraction pattern may be triggered by the word &quot;murdered&quot; in passive verb constructions, while a different extraction pattern may be triggered by &quot;murdered&quot; in active verb constructions. Each pattern extracts information from a syntactic constituent in the current clause: the subject, the direct object, or a prepositional phrase. AutoSlog-TS generates extraction patterns by making two passes over the corpus. In the first pass, AutoSlog-TS uses AutoSlog's heuristics in an exhaustive fashion to generate a set of patterns that collectively extract every noun phrase in the corpus. In the second pass, AutoSlog-TS computes statistics to determine which extraction patterns are most strongly correlated with the relevant training texts. The patterns are ranked so that those most strongly associated with the domain appear at the top. Figure 1 shows the top 20 extraction patterns produced by AutoSlog-TS for the MUC-4 terrorism domain (MUC-4 Proceedings, 1992). The ranked list is then presented to a human to decide which patterns should be kept. For example, the pattern &quot;<subject> exploded&quot; should be retained because it is likely to extract relevant information about bombings. However, the pattern &quot;<subject> said&quot; should be discarded because it is not likely to extract information about terrorism and will probably extract a lot of irrelevant information. The human reviewer assigns a conceptual role to each accepted pattern to characterize its extractions. For example, the pattern &quot;<subject> was murdered&quot; would be assigned The extraction patterns learned by AutoSlog-TS (and AutoSlog) have two serious limitations. First, each pattern extracts only one item, which causes the output to be artificially fragmented. For example, the sentence &quot;Guerrillas kidnapped the mayor in Bogota&quot; produces three extractions (Guerrillas, the mayor, and Bogota), each in a separate structure. This fragmented representation causes unnecessary work for subsequent components that need to piece the information back together. Second, the patterns do not include semantic constraints so they produce many spurious extractions.' Theoretically, conceptual case frames should overcome both of these limitations. Multi-slot case frames will allow several role objects associated with the same event to be instantiated as part of the same structure. This produces a more coherent representation, which is more natural for subsequent event or discourse processing. Furthermore, if each slot has selectional restrictions associated with its legal role objects, then the case frames should produce fewer false hits (i.e., spurious extractions). In the next section, we describe a corpus-based algorithm that constructs conceptual case frames empirically by learning semantic preferences for each extraction pattern and using these preferences to assign conceptual roles automatically. (Consequently, the human reviewer no longer needs to assign roles to the extraction patterns manually.) Extraction patterns with compatible syntactic constraints are then ISemantic constraints could be associated with the conceptual roles assigned by the human reviewer, but our goal is to assign both the conceptual roles and selectional restrictions automatically. merged to produce multi-slot case frames with selectional restrictions. The conceptual case frames should be more reliable at identifying relevant information (our experimental results support this hypothesis), and the case frames can instantiate multiple role objects in a single structure to simplify subsequent discourse processing.
3 Generating conceptual case frames from extraction patterns. The algorithm for building conceptual case frames begins with extraction patterns and a semantic lexicon for the domain. The semantic lexicon is a dictionary of words that belong to relevant semantic categories. We used AutoSlog-TS to generate the extraction patterns and a corpus-based algorithm to generate the semantic lexicon.2 The corpus-based algorithm that we used to build the semantic lexicon (Riloff and Shepherd, 1997) requires five &quot;seed words&quot; as input for each semantic category, and produces a ranked list of words that are statistically associated with each category. First, the algorithm looks for all sentences in which a seed word is used as the head noun of a noun phrase. For each such occurrence of a seed word, the algorithm collects a small context window around the seed word. The context window consists of the closest noun to the left of the seed word, and the closest noun to its right. The context windows for all seed words that belong to the same category are then combined, and each word is assigned a category score. The category score is (essentially) the conditional probability that the word appears in a category context. The words are ranked by this score and the top five are dynamically added to the seed word list. This bootstrapping process dynamically grows the seed word list so that each iteration produces a larger category context. After several iterations, the final list of ranked words usually contains many words that belong to the category, especially near the top. The ranked list is presented to a user, who scans down the list and removes any words that do not belong to the category. For more details of this algorithm, see (Riloff and Shepherd, 1997). A flowchart for the case frame generation process appears in Figure 2. AutoSlog-TS produces a ranked list of extraction patterns and our semantic lexicon generator produces a ranked list of words for each category. Generating these lists is fully automatic, but a human must review them to decide which extraction patterns and category words to keep. This is the only part of the process that involves human interaction. Next, the extraction patterns are applied to the texts to generate a semantic profile for each pattern. The semantic profile shows the semantic categories that were extracted by each pattern, based on the head noun of each extraction. Figure 3 shows the semantic profile for the pattern &quot;attack on <nounphrase>&quot; . PFreq is the number of times that the extraction pattern fired, SFreq is the number of times that the pattern extracted the given semantic category, and Prob is the estimated probability of the pattern extracting the given semantic category (SFreq/PFreq). Note that many extractions will not be labeled with any semantic category if the head noun is unknown (i.e., not in the semantic lexicon). Figure 3 shows that attacks are often carried out on buildings, civilians, dates, government officials, locations, military people, and vehicles. It seems obvious that attacks will occur on people and on physical targets, but a person might not realize that attacks will also occur on dates (e.g., Monday) and on locations (e.g., a neighborhood). This example shows how the corpus-based approach can identify semantic preferences that a person might not anticipate. Also, note that the semantic profile shows no instances of attacks on terrorists or weapons, which makes sense in this domain. The semantic profile is used to select semantic preferences that are strong enough to become selectional restrictions. We use the following formula to identify strong semantic preferences: The first test selects semantic categories that are extracted with high frequency, under the assumption that this reflects a real association with the category. The second case selects semantic categories that represent a relatively high percentage of the extractions even though the frequency might be low (e.g., 2 out of 4 extractions). In our experiments, we chose F1=3, F2=2, and P=0.1. We used fairly lenient criteria because (a) patterns can often extract several types of objects that belong to different semantic categories, and (b) many extractions contain unknown words. Also, remember that the semantic lexicon is reliable because it was reviewed by a person, so it is usually meaningful when a pattern extracts a semantic category even once. The thresholds are needed only to eliminate noise, which can be caused by misparsed sentences or polysemous words. The semantic preferences are used to assign conceptual roles to each extraction pattern. At this point, one additional piece of input is needed: a list of conceptual roles and associated semantic categories for the domain. The conceptual roles identify the types of information that need to be recognized. Figure 4 shows the conceptual roles used for the terrorism domain. Each extraction pattern is expanded to include a set of conceptual roles based on its semantic preferences. These conceptual roles are assigned automatically based on a pattern's semantic profile. This process eliminates the need for a human to assign roles to the extraction patterns by hand, as had been necessary when using AutoSlog or AutoSlog-TS by themselves. For example, the pattern &quot;machinegunned <direct-obj>&quot; had strong semantic preferences for BUILDING, CIVILIAN, LOCATION, and VEHICLE, so it was expanded to have three conceptual roles with four selectional restrictions. The expanded extraction pattern for &quot;machinegunned <direct-obj>&quot; is: Only semantic categories that were associated with a pattern are included as selectional restrictions. For example, the GOVOFFICIAL category also represents possible terrorism victims, but it was not strongly associated with the pattern. Our rationale is that an individual pattern may have a strong preference for only a subset of the categories that can be associated with a role. For example, the pattern &quot;<subject> was ambushed&quot; showed a preference for VEHICLE extractions but not BUILDING extractions, which makes sense because it is hard to imagine ambushing a building. Including only VEHICLE as its selectional restriction for targets might help eliminate incorrect building extractions. One could argue that this pattern is not likely to find building extractions anyway so the selectional restriction will not matter, but the selectional restriction might help filter out incorrect extractions due to misparses or metaphor (e.g., &quot;The White House was ambushed by reporters.&quot;). Ultimately, it is an empirical question whether it is better to include all of the semantic categories associated with a conceptual role or not. Finally, we merge the expanded extraction patterns into multi-slot case frames. All extraction patterns that share the same trigger word and compatible syntactic constraints are merged into a single structure. For example, we would merge all patterns triggered by a specific verb in its passive voice. For example, the patterns &quot;<subject> was kidnapped&quot;, &quot;was kidnapped by <noun-phrase>&quot;, and &quot;was kidnapped in <noun-phrase>&quot; would be merged into a single case frame. Similarly, we would merge all patterns triggered by a specific verb in its active voice. For example, we would merge patterns for the active form of &quot;destroyed&quot; that extract the subject of &quot;destroyed&quot;, its direct object, and any prepositional phrases that are associated with it. We also merge syntactically compatible patterns that are triggered by the same noun (e.g., &quot;assassination&quot;) or by the same infinitive verb structure (e.g., &quot;to kill&quot;). When we merge extraction patterns into a case frame, all of the slots are simply unioned together.
4 Examples. In this section, we show several examples of case frames that were generated automatically by our system. Figure 5 shows a simple case frame triggered by active forms of the verb &quot;ambushed&quot;. The subject is extracted as a perpetrator and has a selectional restriction of TERRORIST. The direct object is extracted as a target and has a selectional restriction of VEHICLE. Note that the case frame does not contain a victim slot, even though it is theoretically possible to ambush people. During training, the &quot;ambushed <direct-obj>&quot; pattern extracted 13 people, 11 of whom were recognized as MILITARYPEOPLE. Since our domain roles only list civilians and government officials as legitimate terrorism victims3, a victim slot was not created. This example shows how the case frames are tailored for the domain empirically. The case frame in Figure 7 illustrates how a semantic category can show up in multiple places. This case frame will handle phrases like &quot;the guerrillas detonated a bomb&quot;, as well as &quot;the bomb detonated&quot;. Both constructions are very common in the training corpus so the system added slots for both possibilities. It would be easy for a human to overlook some of these variations when creating case frames by hand. The case frame in Figure 8 is activated by the noun &quot;attack&quot; and includes slots for a variety of prepositional phrases. The same preposition can recognize different types of information (e.g., &quot;on&quot; can recognize targets, victims, locations, and dates). And the same role can be filled by different prepositions (e.g., targets can be extracted from &quot;on&quot;, &quot;against&quot;, or &quot;at&quot;). This example again shows the power of corpus-based methods to identify common constructions empirically. Anticipating all of these prepositional arguments would be difficult for a person. A disadvantage of this automated method is that inappropriate slots sometimes end up in the case frames. For example, Figure 9 shows a case frame that is activated by passive forms of the verb &quot;killed&quot;. Some of the slots are correct: the subject is assigned to the victim slot and objects of the preposition &quot;by&quot; are assigned to the perpetrator and instrument slots. However, the remaining slots do not make sense. The location slot is the result of polysemy; many person names are also location names, such as &quot;Flores&quot;. The date slot was produced by incorrect parses of date expressions. The perpetrator (subject) and victim (pp (by)) slots were caused by incorrect role assignments. The list of domain roles assumes that terrorists are always perpetrators and civilians are always victims, but of course this is not true. Terrorists can be killed and civilians can be killers. The previous example illustrates some of the problems that can occur when generating case frames automatically. Currently, we are assuming that each semantic category will be uniquely associated with a conceptual role, which may be an unrealistic assumption for some domains. One avenue for future work is to develop more sophisticated methods for mapping semantic preferences to conceptual roles. One could also have a human review the case frames and manually remove inappropriate slots. For now, we chose to avoid additional human interaction and used the case frames exactly as they were generated.
5 Evaluation. The purpose of the selectional restrictions is to constrain the types of information that can be instantiated by each slot. Consequently, we hoped that the case frames would be more reliably instantiated than the extraction patterns, thereby producing fewer false hits. To evaluate the case frames, we used the same corpus and evaluation metrics as previous experiments with AutoSlog and AutoSlogTS (Riloff, 1996b) so that we can draw comparisons between them. For training, we used the 1500 MUC4 development texts to generate the extraction patterns and the semantic lexicon. AutoSlog-TS generated 44,013 extraction patterns in its first pass. After discarding the patterns that occurred only once, the remaining 11,517 patterns were applied to the corpus for the second pass and ranked for manual review. We reviewed the top 2168 patterns5 and kept 306 extraction patterns for the final dictionary. We built a semantic lexicon for nine categories associated with terrorism: BUILDING, CIVILIAN, GOVOFFICIAL, MILITARYPEOPLE, LOCATION, TERRORIST, DATE, VEHICLE, WEAPON. We reviewed the top 500 words for each category. It takes about 30 minutes to review a category assuming that the reviewer is familiar with the domain. Our final semantic dictionary contained 494 words. In total, the review process required approximately 6 person-hours: 1.5 hours to review the extraction patterns plus 4.5 hours to review the words for 9 semantic categories. From the extraction patterns and semantic lexicon, our system generated 137 conceptual case frames. One important question is how to deal with unknown words during extraction. This is especially important in the terrorism domain because many of the extracted items are proper names, which cannot be expected to be in the semantic lexicon. We allowed unknown words to fill all eligible slots and then used a precedence scheme so that each item was instantiated by only one slot. Precedence was based on the order of the roles shown in Figure 4. This is not a very satisfying solution and one of the weaknesses of our current approach. Handling unknown words more intelligently is an important direction for future research. We compared AutoSlog-TS' extraction patterns with the case frames using 100 blind texts6 from the MUC-4 test set. The MUC-4 answer keys were used to score the output. Each extracted item was scored as either correct, mislabeled, duplicate, or spurious. An item was correct if it matched against the answer keys. An item was mislabeled if it matched against the answer keys but was extracted as the wrong type of object (e.g., if a victim was extracted as a perpetrator). An item was a duplicate if it was coreferent with an item in the answer keys. Correct items extracted more than once were scored as duplicates, as well as correct but underspecified extractions such as &quot;Kennedy&quot; instead of &quot;John F. Kennedy&quot;.7 An item was spurious if it did not appear in the answer keys. All items extracted from irrelevant texts were spurious. Finally, items in the answer keys that were not extracted were counted as missing. Correct + missing equals the total number of items in the answer keys.8 Table 1 shows the results8 for AutoSlog-TS' extraction patterns, and Table 2 shows the results for the case frames. We computed Recall (R) as correct I (correct + missing), and Precision (P) as (correct duplicate) I (correct + duplicate + mislabeled + spurious). The extraction patterns and case frames achieved similar recall results, although the case frames missed seven correct extractions. However the case frames produced substantially fewer false hits, producing 82 fewer spurious extractions. Note that perpetrators exhibited by far the lowest precision. The reason is that the perpetrator slot received highest precedence among competing slots for unknown words. Changing the precedence scheme produces a bubble effect where many incorrect extractions shift to the primary default category. The case frames therefore have the potential for even higher precision if the unknown words are handled better. Expanding the semantic lexicon is one option, and additional work may suggest ways to choose slots for unknown words more intelligently.
6 Conclusions. We have shown that conceptual case frames can be generated automatically using unannotated text as input, coupled with a few hours of manual review. Our results for the terrorism domain show that the case frames achieve similar recall levels as the extraction patterns, but with substantially fewer false hits. Our results are not directly comparable to the MUC-4 results because the MUC-4 systems contained additional components, such as domainspecific discourse analyzers that resolved coreferent noun phrases, merged event descriptions, and filtered out irrelevant information. The work presented here only addresses the initial stage of information extraction. However, in previous work we showed that AutoSlog-TS achieved performance comparable to AutoSlog (Riloff, 1996b), which performed very well in the MUC-4 evaluation (Lehnert et al., 1992b). Since the conceptual case frames achieved comparable recall and higher precision than AutoSlog-TS' extraction patterns, our results suggest that the case frames performed well relative to previous work on this domain. Several other systems learn extraction patterns that can also be viewed as conceptual case frames with selectional restrictions (e.g., PALKA (Kim and Moldovan, 1993) and CRYSTAL (Soderland et al., 1995)). The case frames learned by our system are not necessarily more powerful then those generated by other systems. The advantage of our approach is that it requires no special training resources. Our technique requires only preclassified training texts and a few hours of manual filtering to build the intermediate dictionaries. Given preclassified texts, it is possible to build a dictionary of conceptual case frames for a new domain in one day. Another advantage of our approach is its highly empirical nature; a corpus often reveals important patterns in a domain that are not necessarily intuitive to people. By using corpus-based methods to generate all of the intermediate dictionaries and the final case frame structures, the most important words, role assignments, and semantic preferences are less likely to be missed. Our empirical approach aims to exploit the text corpus to automatically acquire the syntactic and semantic role assignments that are necessary to achieve good performance in the domain.