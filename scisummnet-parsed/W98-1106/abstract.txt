TIME 52 process eliminates the need for a human to assign roles to the extraction patterns by hand, as had been necessary when using AutoSlog or AutoSlog-TS by themselves. For example, the pattern &quot;machinegunned <direct-obj>&quot; had strong semantic preferences for CIVILIAN, LOCATION, so was expanded to have three conceptual roles with four selectional restrictions. The expanded extraction pattern for &quot;machinegunned <direct-obj>&quot; is: &quot;machinegunned <direct-obj>&quot; -+ VEHICLE Only semantic categories that were associated with a pattern are included as selectional restric- For example, the also represents possible terrorism victims, but it was not strongly associated with the pattern. Our rationale is that an individual pattern may have a strong preference for only a subset of the categories that can be associated with a role. For example, the pattern &quot;<subject> was ambushed&quot; showed a preference for but not which makes sense because it is hard to imagine ama building. Including only as selectional restriction for targets might help eliminate incorrect building extractions. One could argue that this pattern is not likely to find building extractions anyway so the selectional restriction will not matter, but the selectional restriction might help filter out incorrect extractions due to misparses or (e.g., &quot;The White House by reporters.&quot;). Ultimately, it is an empirical question whether it is better to include all of the semantic categories associated with a conceptual role or not. Finally, we merge the expanded extraction patterns into multi-slot case frames. All extraction patterns that share the same trigger word and compatible syntactic constraints are merged into a single structure. For example, we would merge all patterns triggered by a specific verb in its passive voice. For example, the patterns &quot;<subject> was kidnapped&quot;, &quot;was kidnapped by <noun-phrase>&quot;, and &quot;was kidnapped in <noun-phrase>&quot; would be merged into a single case frame. Similarly, we would merge all patterns triggered by a specific verb in its active voice. For example, we would merge patterns for the active form of &quot;destroyed&quot; that extract the subject of &quot;destroyed&quot;, its direct object, and any prepositional phrases that are associated with it. We also merge syntactically compatible patterns that are triggered by the same noun (e.g., &quot;assassination&quot;) or by the same infinitive verb structure (e.g., &quot;to kill&quot;). When merge extraction patterns into a case frame, of the slots are simply unioned together. 4 Examples In this section, we show several examples of case frames that were generated automatically by our system. Figure 5 shows a simple case frame triggered by active forms of the verb &quot;ambushed&quot;. The subject extracted as a has a selectional of direct object is exas a has a selectional restriction that the case frame does not contain even though it is theoretically possible to ambush people. During training, the &quot;ambushed <direct-obj>&quot; pattern extracted 13 people, 11 of were recognized as Since our domain roles only list civilians and government as legitimate terrorism a victim slot was not created. This example shows how the case frames are tailored for the domain empirically. Caseframe: (active_verb ambushed) VEHICLE Figure 5: Case frame for active forms of &quot;ambushed&quot; Figure 6 shows a case frame triggered by active of &quot;blew_up&quot; This case frame extracts information from an entire sentence into a single struc- The subject object (tara prepositional phrase location) all be extracted together. Caseframe: (active_verb blew_up) subject VEHICLE Figure 6: Case frame for active forms of &quot;blew_up&quot; The case frame in Figure 7 illustrates how a semantic category can show up in multiple places. This case frame will handle phrases like &quot;the guerrillas detonated a bomb&quot;, as well as &quot;the bomb detonated&quot;. Both constructions are very common in the training corpus so the system added slots for both possibilities. It would be easy for a human to overlook some of these variations when creating case frames by hand. The case frame in Figure 8 is activated by the noun &quot;attack&quot; and includes slots for a variety of prepositional phrases. The same preposition can recognize different types of information (e.g., &quot;on&quot; can victims, locations, the same role can be filled by different prepositions military victims were classified as military incidents, not terrorism, according to the MUC-4 guidelines. represent lexicalized expressions in our phrasal lexicon. 53 Caseframe: (active_verb detonated) subject instrument subject WEAPON Figure 7: Case frame for active forms of &quot;detonated&quot; be extracted from &quot;on&quot;, &quot;against&quot;, or &quot;at&quot;). This example again shows the power of corpus-based methods to identify common constructions empirically. Anticipating all of these prepositional arguments would be difficult for a person. Caseframe: (noun attack) VEHICLE CIVILIAN GOVOFFICIAL BUILDING CIVILIAN locationpp(at) Figure 8: Case frame for noun forms of &quot;attack&quot; A disadvantage of this automated method is that inappropriate slots sometimes end up in the case frames. For example, Figure 9 shows a case frame that is activated by passive forms of the verb &quot;killed&quot;. Some of the slots are correct: the subis assigned to the and objects of the preposition &quot;by&quot; are assigned to the perpetrator and However, the remaining slots do sense. The is the result of polysemy; many person names are also location names, as &quot;Flores&quot;. The was produced by inparses of date expressions. The and (by)) slots were caused by incorrect role assignments. The list of domain roles assumes that terrorists are always perpetrators and civilians are always victims, but of course this is not true. Terrorists can be killed and civilians can be killers. killed) subject pp(by) pp(by) Figure 9: Case frame for passive forms of &quot;killed&quot; The previous example illustrates some of the problems that can occur when generating case frames automatically. Currently, we are assuming that each semantic category will be uniquely associated with a conceptual role, which may be an unrealistic assumption for some domains. One avenue for future work is to develop more sophisticated methods for mapping semantic preferences to conceptual roles. One could also have a human review the case frames and manually remove inappropriate slots. For now, we chose to avoid additional human interaction and used the case frames exactly as they were generated. The purpose of the selectional restrictions is to constrain the types of information that can be instantiated by each slot. Consequently, we hoped that the case frames would be more reliably instantiated than the extraction patterns, thereby producing fewer false hits. To evaluate the case frames, we used the same corpus and evaluation metrics as previous experiments with AutoSlog and AutoSlog- TS (Riloff, 1996b) so that we can draw comparisons between them. For training, we used the 1500 MUC- 4 development texts to generate the extraction patterns and the semantic lexicon. AutoSlog-TS generated 44,013 extraction patterns in its first pass. After discarding the patterns that occurred only once, the remaining 11,517 patterns were applied to the corpus for the second pass and ranked for manual We reviewed the top 2168 and kept 306 extraction patterns for the final dictionary. We built a semantic lexicon for nine categories aswith terrorism: CIVILIAN, GOV- OFFICIAL, MILITARYPEOPLE, LOCATION, TERROR- DATE, VEHICLE, WEAPON. reviewed the top 500 words for each category. It takes about 30 minutes to review a category assuming that the reviewer is familiar with the domain. Our final semantic dictionary contained 494 words. In total, the review process required approximately 6 person-hours: 1.5 hours to review the extraction patterns plus 4.5 hours to review the words for 9 semantic categories. From the extraction patterns and semantic lexicon, our system generated 137 conceptual case frames. important question is how to deal with unknown words during extraction. This is especially important in the terrorism domain because many of extracted items are proper names, which cannot be expected to be in the semantic lexicon. We allowed unknown words to fill all eligible slots and then used a precedence scheme so that each item was instantiated by only one slot. Precedence was based on the order of the roles shown in Figure 4. This is not a very satisfying solution and one of the weaknesses of our current approach. Handling unknown words more intelligently is an important direction for future research. We compared AutoSlog-TS' extraction patterns decided to review the top but continued down the list until there were no more ties. 54 Slot cor mis mlb dup spu R P Perp 25 31 10 18 84 .45 .31 Victim 44 23 16 24 62 .66 .47 Target 31 22 17 23 66 .58 .39 Instr 16 15 7 17 23 .52 .52 Total 116 91 50 82 235 .56 .41 Table 1: AutoSlog-TS results the case frames using 100 blind from the MUC-4 test set. The MUC-4 answer keys were used to score the output. Each extracted item was scored either mislabeled, duplicate, spurious. item was it matched against the answer An item was it matched against the answer keys but was extracted as the wrong type of object (e.g., if a victim was extracted as a perpe- An item it was coreferent with an item in the answer keys. Correct items extracted more than once were scored as duplicates, as well as correct but underspecified extractions such as instead of &quot;John F. An item it did not appear in the answer keys. All items extracted from irrelevant texts were spurious. Finally, items in the answer keys that were not were counted as Correct + missthe total number of items in the answer 1 shows the for AutoSlog-TS' extraction patterns, and Table 2 shows the results for case frames. We computed (R) cor- I (correct + missing), (P) (correct duplicate) I (correct + duplicate + misla- + spurious). extraction patterns and case frames achieved similar recall results, although the case frames missed seven correct extractions. However the case frames produced substantially fewer false hits, producing 82 fewer spurious extractions. Note that perpetrators exhibited by far the lowest precision. The reason is that the perpetrator slot received highest precedence among competing slots for unknown words. Changing the precedence relevant texts and 25 irrelevant texts from each of the TST3 and TST4 test sets. rationale for scoring coreferent phrases as duplicates instead of spurious is that the extraction pattern or case frame was instantiated with a reference to the correct answer. In other words, the pattern (or case frame) did the right thing. Resolving coreferent phrases to produce the best answer is a problem for subsequent discourse analysis, which is not addressed by the work presented here. caveat is that the MUC-4 answer keys contain some &quot;optional&quot; answers. We scored these as correct if they were extracted but they were never scored as missing, which is how the &quot;optional&quot; items were scored in MUC-4. Note that the number of possible extractions can vary depending on the output of the system. reimplemented AutoSlog-TS to use a different sentence analyzer, so these results are slightly different from those reported in (Riloff, 19966). Slot cor mis mlb dup spu R P Perp 26 30 4 17 71 .46 .36 Victim 38 28 24 12 26 .58 .50 Target 28 25 3 29 48 .53 .53 Instr 17 14 2 19 8 .55 .78 Total 109 97 33 77 153 .53 .50 Table 2: Case frame results scheme produces a bubble effect where many incorrect extractions shift to the primary default category. The case frames therefore have the potential for even higher precision if the unknown words are handled better. Expanding the semantic lexicon is one option, and additional work may suggest ways to choose slots for unknown words more intelligently. 6 Conclusions We have shown that conceptual case frames can be generated automatically using unannotated text as input, coupled with a few hours of manual review. Our results for the terrorism domain show that the case frames achieve similar recall levels as the extraction patterns, but with substantially fewer false hits. Our results are not directly comparable to the MUC-4 results because the MUC-4 systems contained additional components, such as domainspecific discourse analyzers that resolved coreferent noun phrases, merged event descriptions, and filtered out irrelevant information. The work presented here only addresses the initial stage of information extraction. However, in previous work we showed that AutoSlog-TS achieved performance comparable to AutoSlog (Riloff, 1996b), which performed very well in the MUC-4 evaluation (Lehnert et al., 1992b). Since the conceptual case frames achieved comparable recall and higher precision than AutoSlog-TS' extraction patterns, our results suggest that the case frames performed well relative to previous work on this domain. Several other systems learn extraction patterns that can also be viewed as conceptual case frames with selectional restrictions (e.g., PALKA (Kim and Moldovan, 1993) and CRYSTAL (Soderland et al., 1995)). The case frames learned by our system are not necessarily more powerful then those generated by other systems. The advantage of our approach is that it requires no special training resources. Our technique requires only preclassified training texts and a few hours of manual filtering to build the intermediate dictionaries. Given preclassified texts, it is possible to build a dictionary of conceptual case frames for a new domain in one day. Another advantage of our approach is its highly empirical nature; a corpus often reveals important patterns in a domain that are not necessarily intuitive to people. By using corpus-based methods to generate all of the intermediate dictionaries and 55 the final case frame structures, the most important words, role assignments, and semantic preferences are less likely to be missed. Our empirical approach aims to exploit the text corpus to automatically acquire the syntactic and semantic role assignments that are necessary to achieve good performance in the domain.