Introduction. Finding information about people in the World Wide Web is one of the most common activities of Internetusers. Person names, however, are highly ambigu ous. In most cases, the results for a person name search are a mix of pages about different peoplesharing the same name. The user is then forced ei ther to add terms to the query (probably losing recall and focusing on one single aspect of the person), orto browse every document in order to filter the infor mation about the person he is actually looking for. In an ideal system the user would simply type aperson name, and receive search results clustered ac cording to the different people sharing that name. And this is, in essence, the WePS (Web People Search) task we have proposed to SemEval-2007 participants: systems receive a set of web pages(which are the result of a web search for a per son name), and they have to cluster them in as many sets as entities sharing the name. This task has close links with Word Sense Disambiguation (WSD), which is generally formulated as the taskof deciding which sense a word has in a given context. In both cases, the problem addressed is the resolution of the ambiguity in a natural language expression. A couple of differences make our problem different. WSD is usually focused on open class words (common nouns, adjectives, verbs andadverbs). The first difference is that boundaries be tween word senses in a dictionary are often subtle or even conflicting, making binary decisions harderand sometimes even useless depending on the ap plication. In contrast, distinctions between people should be easier to establish. The second differenceis that WSD usually operates with a dictionary con taining a relatively small number of senses that can be assigned to each word. Our task is rather a case of Word Sense Discrimination, because the number of ?senses? (actual people) is unknown a priori, and it is in average much higher than in the WSD task(there are 90,000 different names shared by 100 mil lion people according to the U.S. Census Bureau). There is also a strong relation of our proposedtask with the Co-reference Resolution problem, fo cused on linking mentions (including pronouns) ina text. Our task can be seen as a co-reference resolution problem where the focus is on solving inter document co-reference, disregarding the linking of all the mentions of an entity inside each document. An early work in name disambiguation (Baggaand Baldwin, 1998) uses the similarity between doc uments in a Vector Space using a ?bag of words? representation. An alternative approach by Mann and Yarowsky (2003) is based on a rich feature space of automatically extracted biographic information. Fleischman and Hovy (2004) propose a Maximum Entropy model trained to give the probability that 64 two names refer to the same individual 1.The paper is organized as follows. Section 2 provides a description of the experimental methodology, the training and test data provided to the par ticipants, the evaluation measures, baseline systemsand the campaign design. Section 3 gives a description of the participant systems and provides the evaluation results. Finally, Section 4 presents some con clusions.
Experimental Methodology. . 2.1 Data. Following the general SemEval guidelines, we have prepared trial, training and test data sets for the task, which are described below. 2.1.1 Trial dataFor this evaluation campaign we initially deliv ered a trial corpus for the potential participants. The trial data consisted of an adapted version of the WePS corpus described in (Artiles et al, 2006). The predominant feature of this corpus is a high number of entities in each document set, due to the fact that the ambiguous names were extracted from the most common names in the US Census. This corpus did not completely match task specifications because it did not consider documents with internal ambiguity, nor it did consider non-person entities; but it was, however, a cost-effective way of releasing data toplay around with. During the first weeks after releasing this trial data to potential participants, some annotation mistakes were noticed. We preferred, how ever, to leave the corpus ?as is? and concentrate our efforts in producing clean training and test datasets, rather than investing time in improving trial data. 2.1.2 Training data In order to provide different ambiguity scenarios, we selected person names from different sources: US Census. We reused the Web03 corpus (Mann, 2006), which contains 32 names randomly picked from the US Census, and was well suited for the task. Wikipedia. Another seven names were sampledfrom a list of ambiguous person names in the En glish Wikipedia. These were expected to have a1For a comprehensive bibliography on person name disam biguation refer to http://nlp.uned.es/weps few predominant entities (popular or historical), and therefore a lower ambiguity than the previous set.ECDL. Finally, ten additional names were ran domly selected from the Program Committee listing of a Computer Science conference (ECDL 2006).This set offers a scenario of potentially low am biguity (computer science scholars usually have a stronger Internet presence than other professionalfields) with the added value of the a priori knowl edge of a domain specific type of entity (scholar) present in the data. All datasets consist of collections of web pages obtained from the 100 top results for a person name query to an Internet search engine 2. Note that 100 is an upper bound, because in some occasions the URL returned by the search engine no longer exists.The second and third datasets (developed explic itly for our task) consist of 17 person names and 1685 associated documents in total (99 documentsper name in average). Each web page was down loaded and stored for off-line processing. We also stored the basic metadata associated to each search result, including the original URL, title, position in the results ranking and the corresponding snippet generated by the search engine.In the process of generating the corpus, the selection of the names plays an important role, poten tially conditioning the degree of ambiguity that will be found later in the Web search results. The reasonsfor this variability in the ambiguity of names are diverse and do not always correlate with the straight forward census frequency. A much more decisivefeature is, for instance, the presence of famous en tities sharing the ambiguous name with less popular people. As we are considering top search results, these can easily be monopolized by a single entity that is popular in the Internet. After the annotation of this data (see section 2.1.4.) we found our predictions about the averageambiguity of each dataset not to be completely ac curate. In Table 1 we see that the ECDL-06 average ambiguity is indeed relatively low (except for the documents for ?Thomas Baker? standing as the most ambiguous name in the whole training). Wikipedia names have an average ambiguity of 23,14 entities2We used the Yahoo! API from Yahoo! Search Web Ser vices (http://developer.yahoo.com/search/web/). 65 Name entities documents discarded Wikipedia names John Kennedy 27 99 6 George Clinton 27 99 6 Michael Howard 32 99 8 Paul Collins 37 98 6 Tony Abbott 7 98 9 Alexander Macomb 21 100 14 David Lodge 11 100 9 Average 23,14 99,00 8,29 ECDL-06 Names Edward Fox 16 100 36 Allan Hanbury 2 100 32 Donna Harman 7 98 6 Andrew Powell 19 98 48 Gregory Crane 4 99 17 Jane Hunter 15 99 59 Paul Clough 14 100 35 Thomas Baker 60 100 31 Christine Borgman 7 99 11 Anita Coleman 9 99 28 Average 15,30 99,20 30,30 WEB03 Corpus Tim Whisler 10 33 8 Roy Tamashiro 5 23 6 Cynthia Voigt 1 405 314 Miranda Bollinger 2 2 0 Guy Dunbar 4 51 34 Todd Platts 2 239 144 Stacey Doughty 1 2 0 Young Dawkins 4 61 35 Luke Choi 13 20 6 Gregory Brennan 32 96 38 Ione Westover 1 4 0 Patrick Karlsson 10 24 8 Celeste Paquette 2 17 2 Elmo Hardy 3 55 15 Louis Sidoti 2 6 3 Alexander Markham 9 32 16 Helen Cawthorne 3 46 13 Dan Rhone 2 4 2 Maile Doyle 1 13 1 Alice Gilbreath 8 74 30 Sidney Shorter 3 4 0 Alfred Schroeder 35 112 58 Cathie Ely 1 2 0 Martin Nagel 14 55 31 Abby Watkins 13 124 35 Mary Lemanski 2 152 78 Gillian Symons 3 30 6 Pam Tetu 1 4 2 Guy Crider 2 2 0 Armando Valencia 16 79 20 Hannah Bassham 2 3 0 Charlotte Bergeron 5 21 8 Average 5,90 47,20 18,00 Global average 10,76 71,02 26,00 Table 1: Training Data per name, which is higher than for the ECDL set. The WEB03 Corpus has the lowest ambiguity (5,9 entities per name), for two reasons: first, randomly picked names belong predominantly to the long tail of unfrequent person names which, per se, have low ambiguity. Being rare names implies that in averagethere are fewer documents returned by the search engine (47,20 per name), which also reduces the pos sibilities to find ambiguity. 2.1.3 Test data For the test data we followed the same process described for the training. In the name selection wetried to maintain a similar distribution of ambiguity degrees and scenario. For that reason we ran domly extracted 10 person names from the English Wikipedia and another 10 names from participantsin the ACL-06 conference. In the case of the US census names, we decided to focus on relatively com mon names, to avoid the problems explained above. Unfortunately, after the annotation was finished (once the submission deadline had expired), wefound a major increase in the ambiguity degrees (Ta ble 2) of all data sets. While we expected a raise in the case of the US census names, the other two cases just show that there is a high (and unpredictable) variability, which would require much larger data sets to have reliable population samples. This has made the task particularly challenging for participants, because naive learning strategies (such as empirical adjustment of distance thresholds to optimize standard clustering algorithms) might be misleaded by the training set. 2.1.4 AnnotationThe annotation of the data was performed separately in each set of documents related to an ambiguous name. Given this set of approximately 100 documents that mention the ambiguous name, the an notation consisted in the manual clustering of eachdocument according to the actual entity that is re ferred on it.When non person entities were found (for in stance, organization or places named after a person) the annotation was performed without any special rule. Generally, the annotator browses documents following the original ranking in the search results; after reading a document he will decide whether thementions of the ambiguous name refer to a new en tity or to a entity previously identified. We asked the annotators to concentrate first on mentions that strictly contained the search string, and then to pay attention to the co-referent variations of the name. For instance ?John Edward Fox? or ?Edward Fox Smith? would be valid mentions. ?Edward J. Fox?, however, breaks the original search string, and we do not get into name variation detection, so it will be considered valid only if it is co-referent to a valid 66 Name entities documents discarded Wikipedia names Arthur Morgan 19 100 52 James Morehead 48 100 11 James Davidson 59 98 16 Patrick Killen 25 96 4 William Dickson 91 100 8 George Foster 42 99 11 James Hamilton 81 100 15 John Nelson 55 100 25 Thomas Fraser 73 100 13 Thomas Kirk 72 100 20 Average 56,50 99,30 17,50 ACL06 Names Dekang Lin 1 99 0 Chris Brockett 19 98 5 James Curran 63 99 9 Mark Johnson 70 99 7 Jerry Hobbs 15 99 7 Frank Keller 28 100 20 Leon Barrett 33 98 9 Robert Moore 38 98 28 Sharon Goldwater 2 97 4 Stephen Clark 41 97 39 Average 31,00 98,40 12,80 US Census Names Alvin Cooper 43 99 9 Harry Hughes 39 98 9 Jonathan Brooks 83 97 8 Jude Brown 32 100 39 Karen Peterson 64 100 16 Marcy Jackson 51 100 5 Martha Edwards 82 100 9 Neil Clark 21 99 7 Stephan Johnson 36 100 20 Violet Howard 52 98 27 Average 50,30 99,10 14,90 Global average 45,93 98,93 15,07 Table 2: Test Data mention. In order to perform the clustering, the annotatorwas asked to pay attention to objective facts (bi ographical dates, related names, occupations, etc.) and to be conservative when making decisions. Thefinal result is a complete clustering of the docu ments, where each cluster contains the documentsthat refer to a particular entity. Following the pre vious example, in documents for the name ?Edward Fox? the annotator found 16 different entities with that name. Note that there is no a priori knowledge about the number of entities that will be discovered in a document set. This makes the task specially difficult when there are many different entities and a high volume of scattered biographical information to take into account. In cases where the document does not offer enough information to decide whether it belongs to a cluster or is a new entity, it is discarded from the evaluation process (not from the dataset). Another common reason for discarding documents was theabsence of the person name in the document, usu ally due to a mismatch between the search engine cache and the downloaded URL. We found that, in many cases, different entities were mentioned using the ambiguous name within asingle document. This was the case when a doc ument mentions relatives with names that contain the ambiguous string (for instance ?Edward Fox? and ?Edward Fox Jr.?). Another common case ofintra-document ambiguity is that of pages contain ing database search results, such as book lists from Amazon, actors from IMDB, etc. A similar case is that of pages that explicitly analyze the ambiguity of a person name (Wikipedia ?disambiguation? pages). The way this situation was handled, in terms of the annotation, was to assign each document to as many clusters as entities were referred to on it with the ambiguous name. 2.2 Evaluation measures. Evaluation was performed in each document set (web pages mentioning an ambiguous person name) of the data distributed as test. The human annotation was used as the gold standard for the evaluation.Each system was evaluated using the standard pu rity and inverse purity clustering measures Purity isrelated to the precision measure, well known in In formation Retrieval. This measure focuses on the frequency of the most common category in eachcluster, and rewards the clustering solutions that in troduce less noise in each cluster. Being C the set of clusters to be evaluated, L the set of categories (manually annotated) and n the number of clustered elements, purity is computed by taking the weighted average of maximal precision values: Purity = ? i |Ci| n max Precision(Ci, Lj)where the precision of a cluster Ci for a given cat egory Lj is defined as: Precision(Ci, Lj) = |Ci ? Lj | |Ci|Inverse Purity focuses on the cluster with maximum recall for each category, rewarding the clus tering solutions that gathers more elements of each category in a corresponding single cluster. Inverse Purity is defined as: 67 Inverse Purity = ? i |Li| n max Precision(Li, Cj)For the final ranking of systems we used the har monic mean of purity and inverse purity F?=0,5 . The F measure is defined as follows: F = 1 ? 1Purity + (1? ?) 1 Inverse PurityF?=0,2 is included as an additional measure giv ing more importance to the inverse purity aspect. The rationale is that, for a search engine user, it should be easier to discard a few incorrect web pages in a cluster containing all the informationneeded, than having to collect the relevant infor mation across many different clusters. Therefore, achieving a high inverse purity should be rewarded more than having high purity. 2.3 Baselines. Two simple baseline approaches were applied to the test data. The ALL-IN-ONE baseline provides aclustering solution where all the documents are assigned to a single cluster. This has the effect of always achieving the highest score in the inverse purity measure, because all classes have their docu ments in a single cluster. On the other hand, the purity measure will be equal to the precision of thepredominant class in that single cluster. The ONE IN-ONE baseline gives another extreme clusteringsolution, where every document is assigned to a dif ferent cluster. In this case purity always gives its maximum value, while inverse purity will decrease with larger classes. 2.4 Campaign design. The schedule for the evaluation campaign was set by the SemEval organisation as follows: (i) release task description and trial data set; (ii) release of training and test; (iii) participants send their answers to the task organizers; (iv) the task organizers evaluate the answers and send the results. The task description and the initial trial data set were publicly released before the start of the official evaluation.The official evaluation period started with the simultaneous release of both training and test data, to gether with a scoring script with the main evaluation measures to be used. This period spanned five weeksin which teams were allowed to register and down load the data. During that period, results for a giventask had to be submitted no later than 21 days af ter downloading the training data and no later than 7days after downloading the test data. Only one sub mission per team was allowed. Training data included the downloaded webpages, their associated metadata and the human clustering of each document set, providing a develop ment test-bed for the participant?s systems. We also specified the source of each ambiguous name in the training data (Wikipedia, ECDL conference and US Census). Test data only included the downloaded web pages and their metadata. This section of thecorpus was used for the systems evaluation. Partici pants were required to send a clustering for each test document set. Finally, after the evaluation period was finishedand all the participants sent their data, the task orga nizers sent the evaluation for the test data.
Results of the evaluation campaign. . 29 teams expressed their interest in the task; thisnumber exceeded our expectations for this pilot ex perience, and confirms the potential interest of theresearch community in this highly practical prob lem. Out of them, 16 teams submitted results within the deadline; their results are reported below. 3.1 Results and discussion. Table 3 presents the macro-averaged results ob tained by the sixteen systems plus the two baselineson the test data. We found macro-average 3 preferable to micro-average 4 because it has a clear inter pretation: if the evaluation measure is F, then we should calculate F for every test case (person name) and then average over all trials. The interpretation of micro-average F is less clear. The systems are ranked according to the scores obtained with the harmonic mean measure F?=0,5 of 3Macro-average F consists of computing F for every test set (person name) and then averaging over all test sets. 4Micro-average F consists of computing the average P and IP (over all test sets) and then calculating F with these figures. 68 Macro-averaged Scores F-measures rank team-id ? =,5 ? =,2 Pur Inv Pur 1 CU COMSEM ,78 ,83 ,72 ,88. 2 IRST-BP ,75 ,77 ,75 ,80. 3 PSNUS ,75 ,78 ,73 ,82. 5 SHEF ,66 ,73 ,60 ,82. 6 FICO ,64 ,76 ,53 ,90. 7 UNN ,62 ,67 ,60 ,73. 8 ONE-IN-ONE ,61 ,52 1,00 ,47. 9 AUG ,60 ,73 ,50 ,88. 10 SWAT-IV ,58 ,64 ,55 ,71. 11 UA-ZSA ,58 ,60 ,58 ,64. 12 TITPI ,57 ,71 ,45 ,89. 13 JHU1-13 ,53 ,65 ,45 ,82. 14 DFKI2 ,50 ,63 ,39 ,83. 15 WIT ,49 ,66 ,36 ,93. 16 UC3M 13 ,48 ,66 ,35 ,95. 17 UBC-AS ,40 ,55 ,30 ,91. 18 ALL-IN-ONE ,40 ,58 ,29 1,00. Table 3: Team rankingpurity and inverse purity. Considering only the par ticipant systems, the average value for the ranking measure was 0, 60 and its standard deviation 0, 11. Results with F?=0,2 are not substantially different (except for the two baselines, which roughly swappositions). There are some ranking swaps, but gen erally only within close pairs.The good performance of the ONE-IN-ONE baseline system is indicative of the abundance of singleton entities (entities represented by only one doc ument). This situation increases the inverse purity score for this system giving a harmonic measure higher than the expected.
Conclusions. . The WEPS task ended with considerable success in terms of participation, and we believe that a careful analysis of the contributions made by participants(which is not possible at the time of writing this report) will be an interesting reference for future re search. In addition, all the collected and annotated dataset will be publicly available 5 as a benchmark for Web People Search systems.At the same time, it is clear that building a re liable test-bed for the task is not simple. First ofall, the variability across test cases is large and un predictable, and a system that works well with the 5http://nlp.uned.es/wepsnames in our test bed may not be reliable in practi cal, open search situations. Partly because of that,our test-bed happened to be unintentionally challenging for systems, with a large difference be tween the average ambiguity in the training and test datasets. Secondly, it is probably necessary to think about specific evaluation measures beyond standard clustering metrics such as purity and inverse purity,which are not tailored to the task and do not be have well when multiple classification is allowed.We hope to address these problems in a forthcom ing edition of the WEPS task.
Acknowledgements. . This research was supported in part by the National Science Foundation of United States under GrantIIS-00325657 and by a grant from the Spanish government under project Text-Mess (TIN2006-15265C06). This paper does not necessarily reflect the po sition of the U.S. Government.