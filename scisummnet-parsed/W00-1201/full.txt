1 Introduction. Ever since the success of HMMs' application to part-of-speech tagging in (Church, 1988), machine learning approaches to natural language processing have steadily become more widespread. This increase has of course been due to their proven efficacy in many tasks, but also to their engineering efficacy. Many machine learning approaches let the data speak for itself (data ipsa loguuntur), as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems. The success of statistical methods in particular has been quite evident in the area of syntactic parsing, most recently with the outstanding results of (Charniak, 2000) and (Collins, 2000) on the now-standard English test set of the Penn rkeebank (Marcus et al., 1993). A significant trend in parsing models has been the incorporation of linguistically-motivated features; however, it is important to note that &quot;linguistically-motivated&quot; does not necessarily mean language-dependent&quot;--often, it means just the opposite. For example, almost all statistical parsers make use of lexicalized nonterminals in some way, which allows lexical items' indiosyncratic parsing preferences to be modeled, but the paring between head words and their parent nonterminals is determined almost entirely by the training data, thereby making this feature—which models preferences of particular words of a particular language—almost entirely languageindependent. In this paper, we will explore the use of two parsing models, which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese IYeebank. We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing. We also discuss directions for future work.
2 Models and Modifications. We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)). Both parsing models discussed in this paper inherit a great deal from this model, so we briefly describe its &quot;progenitive&quot; features here, describing only how each of the two models of this paper differ in the subsequent two sections. The lexicalized PCFG that sits behind Model 2 of (Collins, 1997) has rules of the form where P, L, Ri and H are all lexicalized nonterminals, and P inherits its lexical head from its distinguished head child, H. In this generative model, first P is generated, then its head-child H, then each of the left- and right-modifying nonterminals are generated from the head outward. The modifying nonterminals Li and Ri are generated conditioning on P and H, as well as a distance metric (based on what material intervenes between the currently-generated modifying nonterminal and H) and an incremental subcat frame feature (a multiset containing the complements of H that have yet to be generated on the side of H in which the currentlygenerated nonterminal falls). Note that if the modifying nonterminals were generated completely independently, the model would be very impoverished, but in actuality, by including the distance and subcat frame features, the model captures a crucial bit of linguistic reality, viz., that words often have welldefined sets of complements and adjuncts, dispersed with some well-defined distribution in the right hand sides of a (context-free) rewriting system. The BBN model is also of the lexicalized PCFG variety. In the BBN model, as with Model 2 of (Collins, 1997), modifying nonterminals are generated conditioning both on the parent P and its head child H. Unlike Model 2 of (Collins, 1997), they are also generated conditioning on the previously generated modifying nonterminal, or and there is no subcat frame or distance feature. While the BBN model does not perform at the level of Model 2 of (Collins, 1997) on Wall Street Journal text, it is also less language-dependent, eschewing the distance metric (which relied on specific features of the English Treebank) in favor of the &quot;bigrams on nonterminals&quot; model. This section briefly describes the top-level parameters used in the BBN parsing model. We use p to denote the unlexicalized nonterminal corresponding to P in (1), and similarly for i, ri and h. We now present the toplevel generation probabilities, along with examples from Figure 1. For brevity, we omit the smoothing details of BBN's model (see (Miller et al., 1998) for a complete description); we note that all smoothing weights are computed via the technique described in (Bikel et al., 1997). The probability of generating p as the root label is predicted conditioning on only +TOP+, which is the hidden root of all parse trees: The probability of generating a head node h with a parent p is The probability of generating a left-modifier ii is when generating the NP for NP(Apple—NNP), and the probability of generating a right modwhen generating the NP for NP(Microsoft— NNP).1 The probabilities for generating lexical elements (part-of-speech tags and words) are as follows. The part of speech tag of the head of the entire sentence, th, is computed conditioning only on the top-most symbol p:2 Part of speech tags of modifier constituents, and tri, are predicted conditioning on the modifier constituent i or ri, the tag of the head constituent, th, and the word of the head constituent, wh The head word of the entire sentence, wh, is predicted conditioning only on the top-most symbol p and th: Head words of modifier constituents, wii and wri , are predicted conditioning on all the context used for predicting parts of speech in (7), as well as the parts of speech themsleves and P (wri I tri , ri, WO- (9) The original English model also included a word feature to help reduce part-of-speech ambiguity for unknown words, but this component of the model was removed for Chinese, as it was language-dependent. The probability of an entire parse tree is the product of the probabilities of generating all of the elements of that parse tree, The hidden nonterminal +BEGIN+ is used to provide a convenient mechanism for determining the initial probability of the underlying Markov process generating the modifying nonterminals; the hidden nonterminal +END+ is used to provide consistency to the underlying Markov process, i.e., so that the probabilities of all possible nonterminal sequences sum to 1. 2This is the one place where we altered the original model, as the lexical components of the head of the entire sentence were all being estimated incorrectly, causing an inconsistency in the model. We corrected the estimation of th and wh in our implementation. where an element is either a constituent label, a part of speech tag or a word. We obtain maximum-likelihood estimates of the parameters of this model using frequencies gathered from the training data. The model of (Chiang, 2000) is based on stochastic TAG (Resnik, 1992; Schabes, 1992). In this model a parse tree is built up not out of lexicalized phrase-structure rules but by tree fragments (called elementary trees) which are lexicalized in the sense that each fragment contains exactly one lexical item (its anchor). In the variant of TAG we use, there are three kinds of elementary tree: initial, (predicative) auxiliary, and modifier, and three composition operations: substitution, adjunction, and sister-adjunction. Figure 2 illustrates all three of these operations, al is an initial tree which substitutes at the leftmost node labeled NP,I. ; 13 is an auxiliary tree which adjoins at the node labeled VP. See (Joshi and Schabes, 1997) for a more detailed explanation. Sister-adjunction is not a standard TAG operation, but borrowed from D-Tree Grammar (Rambow et al., 1995). In Figure 2 the modifier tree 7 is sister adjoined between the nodes labeled VB and NI4. Multiple modifier trees can adjoin at the same place, in the spirit of (Schabes and Shieber, 1994). In stochastic TAG, the probability of generating an elementary tree depends on the elementary tree itself and the elementary tree it attaches to. The parameters are as follows: where a ranges over initial trees, /3 over auxiliary trees, 7 over modifier trees, and n over nodes. Pi(a) is the probability of beginning a derivation with a; Ps(a j ) is the probability of substituting a at 71; Pa(O I n) is the probability of adjoining 13 at 77; finally, Pa(NONE j 7) is the probability of nothing adjoining at n. Our variant adds another set of parameters: This is the probability of sister-adjoining -y between the ith and i + 1 th children of 77 (allowing for two imaginary children beyond the leftmost and rightmost children). Since multiple modifier trees can adjoin at the same location, P.,a(-y) is also conditioned on a flag f which indicates whether -y is the first modifier tree (i.e., the one closest to the head) to adjoin at that location. For our model we break down these probabilities further: first the elementary tree is generated without its anchor, and then its anchor is generated. See (Chiang, 2000) for more details. During training each example is broken into elementary trees using head rules and argument/adjunct rules similar to those of (Collins, 1997). The rules are interpreted as follows: a head is kept in the same elementary tree in its parent, an argument is broken off into a separate initial tree, leaving a substitution node, and an adjunct is broken off into a separate modifier tree. A different rule is used for extracting auxiliary trees; see (Chiang, 2000) for details. Xia (1999) describes a similar process, and in fact our rules for the Xinhua corpus are based on hers. The primary language-dependent component that had to be changed in both models was the head table, used to determine heads when training. We modified the head rules described in (Xia, 1999) for the Xinhua corpus and substituted these new rules into both models. The (Chiang, 2000) model had the following additional modifications. was eliminated, causing parts of speech for unknown words to be predicted solely on the head relations in the model. • The default beam size in the probabilistic CKY parsing algorithm was widened. The default beam pruned away chart entries whose scores were not within a factor of e-5 of the top-ranked subtree; this settings and lower unknown word threshold than the defaults. *3 of the 400 sentences were not parsed due to timeouts and/or pruning problems. t3 of the 348 sentences did not get parsed due to pruning problems, and 2 other sentences had length mismatches (scoring program errors). tight limit was changed to e-9. Also, the default decoder pruned away all but the top 25-ranked chart entries in each cell; this limit was expanded to 50.
3 Experiments and Results. The Chinese Treebank consists of 4185 sentences of Xinhua newswire text. We blindly separated this into training, devtest and test sets, with a roughly 80/10/10 split, putting files 001-270 (3484 sentences, 84,873 words) into the training set, 301-325 (353 sentences, 6776 words) into the development test set and reserving 271-300 (348 sentences, 7980 words) for testing. See Table 1 for results. In order to put the new Chinese Treebank results into context with the unmodified (English) parsing models, we present results on two test sets from the Wall Street Journal: WSJ-all, which is the complete Section 23 (the de facto standard test set for English parsing), and WSJ-small, which is the first 400 sentences of Section 23 and which is roughly comparable in size to the Chinese test set. Furthermore, when testing on WSJ-small, we trained on a subset of our English training data roughly equivalent in size to our Chinese training set (Sections 02 and 03 of the Penn Treebank); we have indicated models trained on all English training with &quot;-all&quot;, and models trained with the reduced English training set with &quot;-small&quot;. Therefore, by comparing the WSJ-small results with the Chinese results, one can reasonably gauge the performance gap between English parsing on the Penn Treebank and Chinese parsing on the Chinese Treebank. The reader will note that the modified BBN model does significantly poorer than (Chiang, 2000) on Chinese. While more investigation is required, we suspect part of the difference may be due to the fact that currently, the BBN model uses language-specific rules to guess part of speech tags for unknown words.
4 Conclusions and Future Work. There is no question that a great deal of care and expertise went into creating the Chinese Treebank, and that it is a source of important grammatical information that is unique to the Chinese language. However, there are definite similarities between the grammars of English and Chinese, especially when viewed through the lens of the statistical models we employed here. In both languages, the nouns, adjectives, adverbs, and verbs have preferences for certain arguments and adjuncts, and these preferences—in spite of the potentially vastlydifferent configurations of these items—are effectively modeled. As discussed in the introduction, lexical items' idiosyncratic parsing preferences are modeled by lexicalizing the grammar formalism, using a lexicalized PCFG in one case and a lexicalized stochastic TAG in the other. Linguistically-reasonable independence assumptions are made, such as the independence of grammar productions in the case of the PCFG model, or the independence of the composition operations in the case of the LTAG model, and we would argue that these assumptions are no less reasonable for the Chinese grammar than they are for that of English. While results for the two languages are far from equal, we believe that further tuning of the head rules, and analysis of development test set errors will yield significant performance gains on Chinese to close the gap. Finally, we fully expect that absolute performance will increase greatly as additional highquality Chinese parse data becomes available.
5 Acknowledgements. This research was funded in part by NSF grant SBR-89-20230-15. We would greatly like to acknowledge the researchers at BBN who allowed us to use their model: Ralph Weischedel, Scott Miller, Lance Ramshaw, Heidi Fox and Sean Boisen. We would also like to thank Mike Collins and our advisors Aravind Joshi and Mitch Marcus.