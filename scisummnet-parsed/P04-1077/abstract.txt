GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83 1. Pearson’s Spearman’s of automatic evaluation measures vs. 4, and 12 are maximum of 1, 4, and 12 grams, NIST is the NIST ROUGE-L is LCS-based F-measure 1), ROUGE-W is weighted LCS-based F-measure = 1). ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit, ROUGEis skip-bigram-based F-measure 1) with maximum skip distance of N, PER is position independent word error rate, and WER is word error rate. GTM 10, 20, and 30 are general text matcher exponents of 1.0, 2.0, and 3.0. (Note, only 4, and 12 are shown here to preserve space.) limit and with skip distant limits of 0, 4, and 9. Correlation analysis based on two different correlastatistics, Pearson’s Spearman’s with respect to adequacy and fluency are shown in Table 1. Pearson’s correlation measures the and direction of a between any two variables, i.e. automatic metric score and human assigned mean coverage score in our case. It ranges from +1 to -1. A correlation of 1 means that there is a perfect positive linear relationship between the two variables, a correlation of -1 means that there is a perfect negative linear relationship between them, and a correlation of 0 means that there is no linear relationship between them. Since we would like to use automatic evaluation metric not only in comparing systems a quick overview of the Pearson’s coefficient, see: http://davidmlane.com/hyperstat/A34739.html. but also in in-house system development, a good linear correlation with human judgment would enable us to use automatic scores to predict corresponding human judgment scores. Therefore, Pearson’s correlation coefficient is a good measure to look at. correlation coefficient 6is also a measure of correlation between two variables. It is a non-parametric measure and is a special case of the Pearson’s correlation coefficient when the values of data are converted into ranks before computing the coefficient. Spearman’s correlation coefficient does not assume the correlation between the variables is linear. Therefore it is a useful correlation indicator even when good linear correlation, for example, according to Pearson’s correlation coefficient between two variables could a quick overview of the Spearman’s coefficient, see: http://davidmlane.com/hyperstat/A62436.html. not be found. It also suits the NIST MT evaluation scenario where multiple systems are ranked according to some performance metrics. To estimate the significance of these correlation statistics, we applied bootstrap resampling, generating random samples of the 919 different sentence segments. The lower and upper values of 95% confidence interval are also shown in the table. Dark (green) cells are the best correlation numbers in their categories and light gray cells are statistically equivalent to the best numbers in their categories. Analyzing all runs according to the adequacy and fluency table, we make the following observations: Applying the stemmer achieves higher correlation with adequacy but keeping case information achieves higher correlation with fluency except for (only is shown). For example, Pearson’s correlation of ROUGE-S* with adequacy increases from 0.85 (Case) to 0.95 while its Pearson’s with fluency drops from 0.84 (Case) to 0.78 (Stem). We will focus our discussions on the Stem set in adequacy and Case set in fluency. Pearson's values in the Stem set of the Adequacy Table, indicates that ROUGE- L and ROUGE-S with a skip distance longer than 0 correlate highly and linearly with adequacy and NIST. ROUGE-S* achieves best correlation with a Pearson’s 0.95. Measures favoring consecutive matches, i.e. and 12, ROUGE-W, GTM20 and 30, ROUGE-S0 (bigram), and WER have lower Pear- Among them WER (0.48) that tends to penalize small word movement is the worst performer. One interesting observation is that longer lower correlation with adequacy. generally agree with Pearhave more equivalents. Pearson's values in the Stem of the Fluency Table, indicates that has the highest correlation (0.93) with fluency. However, it is statistically indistinguishable with 95% confidence from all other metrics shown in the Case set of the Fluency Table except for WER and GTM10. GTM10 has good correlation with human judgments in adequacy but not fluency; while GTM20 and GTM30, i.e. GTM with exponent larger than 1.0, has good correlation with human judgment in fluency but not adequacy. ROUGE-L and ROUGE-S*, 4, and 9 are good automatic evaluation metric candidates since they as well as fluency correlation and outperform and 12 significantly in adequacy. Among them, ROUGE-L is the best metric in both adequacy and fluency correlation with human judgment according to Spearman’s correlation coefficient and is statistically indistinguishable from the best metrics in both adequacy and fluency correlation with human judgment according to Pearson’s correlation coefficient. In this paper we presented two new objective automatic evaluation methods for machine translation, ROUGE-L based on longest common subsequence (LCS) statistics between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence ngrams automatically while this is a free parameter To give proper credit to shorter common sequences that are ignored by LCS but still retain the flexibility of non-consecutive matches, we proposed counting skip bigram co-occurrence. The skip-bigram-based ROUGE-S* (without skip disrestriction) had the best Pearson's correlation of 0.95 in adequacy when all words were lower case and stemmed. ROUGE-L, ROUGE-W, ROUGE-S*, ROUGE-S4, and ROUGE-S9 were performers to measuring fluency. However, they have the advantage that we can apthem on sentence level while longer would not differentiate any sentences with length shorter than 12 words (i.e. no 12-gram matches). We plan to explore their correlation with human judgments on sentence-level in the future. We also confirmed empirically that adequacy and fluency focused on different aspects of machine translations. Adequacy placed more emphasis on terms co-occurred in candidate and reference translations as shown in the higher correlations in Stem set than Case set in Table 1; while the reverse was true in the terms of fluency. The evaluation results of ROUGE-L, ROUGE- W, and ROUGE-S in machine translation evaluation are very encouraging. However, these measures in their current forms are still only applying string-to-string matching. We have shown that better correlation with adequacy can be reached by applying stemmer. In the next step, we plan to extend them to accommodate synonyms and paraphrases. For example, we can use an existing thesaurus such as WordNet (Miller 1990) or creating a customized one by applying automated synonym set discovery methods (Pantel and Lin 2002) to identify potential synonyms. Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches. In this scenario, statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased. ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004). In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed NIST.