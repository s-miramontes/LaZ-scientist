1 Introduction. The optimal way to analyze linguistic data into its primitive elements is rarely obvious but often crucial. Identifying phones and words in speech has been a major focus of research. Automatically finding words in text, the problem addressed here, is largely unsolved for languages such as Chinese and Thai, which are written without spaces (but see Fung Sz Wu, 1994; Sproat et al., 1996). Spaces in texts of languages like English offer an easy first approximation to minimal content-bearing units. However, this approximation mis-analyzes non-compositional compounds (NCCs) such as &quot;kick the bucket&quot; and &quot;hot dog.&quot; NCCs are compound words whose meanings are a matter of convention and cannot be synthesized from the meanings of their space-delimited components. Treating NCCs as multiple words degrades the performance of machine translation (MT), information retrieval, natural language generation, and most other NLP applications. NCCs are usually not translated literally to other languages. Therefore, one way to discover NCCs is to induce and analyze a translation model between two languages. This paper is about an informationtheoretic approach to this kind of ontological discovery. The method is based on the insight that treatment of NCCs as multiple words reduces the predictive power of translation models. Whether a given sequence of words is an NCC can be determined by comparing the predictive power of two translation models that differ on whether they treat the word sequence as an NCC. Searching a space of data models in this manner has been proposed before, e.g. by Brown et al. (1992) and Wang et al. (1996), but their particular methods have been limited by the computational expense of inducing data models and the typically vast number of potential NCCs that need to be tested. The method presented here overcomes this limitation by making independence assumptions that allow hundreds of NCCs to be discovered from each pair of induced translation models. It is further accelerated by heuristics for gauging the a priori likelihood of validation for each candidate NCC. The predictive power of a translation model depends on what the model is meant to predict. This paper considers two different applications of translation models, and their corresponding objective functions. The different objective functions lead to different mathematical formulations of predictive power, different heuristics for estimating predictive power, and different classifications of word sequences with respect to compositionality. Monolingual properties of NCCs are not considered by either objective function. So, the method will not detect phrases that are translated word-for-word despite non-compositional semantics, such as the English metaphors &quot;ivory tower&quot; and &quot;banana republic,&quot; which translate literally into French. On the other hand, the method will detect word sequences that are often paraphrased in translation, but have perfectly compositional meanings in the monolingual sense. For example, &quot;tax system&quot; is most often translated into French as &quot;regime fiscale.&quot; Each new batch of validated NCCs raises the value of the objective function for the given application, as demonstrated in Section 8. You can skip ahead to Table 4 for a random sample of the NCCs that the method validated for use in a machine translation task. The NCC detection method makes some assumptions about the properties of statistical translation models, but no assumptions about the data from which the models are constructed. Therefore, the method is applicable to parallel data other than parallel texts. For example, Section 8 applies the method to orthographic and phonetic representations of English words to discover the NCCs of English orthography.
2 Translation Models. A translation model can be constructed automatically from texts that exist in two languages (bitexts) (Brown et al., 1993; Melamed, 1997). The more accurate algorithms used for constructing translation models, including the EM algorithm, alternate between two phases. In the first phase, the algorithm finds and counts the most likely links between word tokens in the two halves of the bitext. Links connect words that are hypothesized to be mutual translations. In the second phase, the algorithm estimates translation probabilities by dividing the link counts by the total number of links. Let S and T represent the distributions of linked words in the source and target' texts. A simple translation model is just a joint probability distribution Pr(s, t), which indicates the probability that a randomly selected link in the bitext links
3 Objective Functions. The decision whether a given sequence of words should count as an NCC can be made automatically, if it can be expressed in terms of an explicit objective function for the given application. The first application I will consider is statistical machine translation involving a directed translation model and a target language model, of the sort advocated by Brown et al. (1993). If only the translation model may be varied, then the objective function for this application should be based on how well the translation model predicts the distribution of words in the target language. In information theory, one such objective function is called mutual information. Mutual information measures how well one random variable predicts another3: When Pr(s, t) is a text translation model, mutual information indicates how well the model can predict the distribution of words in the target text given the distribution of words in the source text, and vice versa. This objective function may also be used for optimizing cross-language information retrieval, where translational distributions must be estimated either for queries or for documents before queries and documents can be compared (Oard & Dorr, 1996). Figure 1 shows a simple example of how recognition of NCCs increases the mutual information of translation models. The English word &quot;balance&quot; is most often translated into French as &quot;equilibre&quot; and &quot;sheet&quot; usually becomes &quot;feuille.&quot; However, a &quot;balance sheet&quot; is a &quot;bilan.&quot; A translation model that doesn't recognize &quot;balance sheet&quot; as an NCC would distribute the translation probabilities of &quot;bilan&quot; over multiple English words, as shown in the Incorrect Model. The Incorrect Model is uncertain about how &quot;bilan&quot; should be translated. On the other hand, the Correct Model, which recognizes &quot;balance sheet&quot; as an NCC is completely certain about its translation. As a result, the mutual information pf the Incorrect Model is
4 Predictive Value Functions. An explicit objective function immediately leads to a simple test of whether a given sequence of words should be treated as an NCC: Induce two translation models, a trial translation model that involves the candidate NCC and a base translation model that does not. If the value of the objective function is higher in the trial model than in the base model, then the NCC is valid; otherwise it is not. In theory, this test can be repeated for each sequence of words in the text. In practice, texts contain an enormous number of word sequences (Brown et al., 1992), only a tiny fraction of which are NCCs, and it takes considerable computational effort to induce each translation model. Therefore, it is necessary to test many NCCs on each pair of translation models. Suppose we induce a trial translation model from texts E and F involving a number of NCCs in the language S of E, and compare it to a base translation model without any of those NCCs. We would like to keep the NCCs that caused a net increase in the objective function I and discard those that caused a net decrease. We need some method of assigning credit for the difference in the value of I between the two models. More precisely, we need a function ir(s) over the words s E S such that The predictive value function ir(s) represents the contribution of s to the objective function of the whole translation model. I will write simply i(s) when T is clear from the context. Comparison of predictive value functions across translation models can only be done under Let i and i' be the predictive value functions for source words in the base translation model and in the trial translation model, respectively. Under Assumption 1, the net change in the objective function effected by each candidate NCC xy is If Azy > 0, then xy is a valid NCC for the given application. Assumption 1 would likely be false if either x or y was a part of any candidate NCC other than sy. Therefore, NCCs that are tested at the same time must satisfy the mutual exclusion condition: No word s E S may participate in more than one candidate NCC at the same time. Assumption 1 may not be completely safe even with this restriction, due to the imprecise nature of translation model construction algorithms.
5 Iteration. The mutual exclusion condition implies that multiple tests must be performed to find the majority of NCCs in a given text. Furthermore, Equation 4 allows testing of only two-word NCCs. Certainly, longer NCCs exist. Given parallel texts E and F, the following algorithm runs multiple NCC tests and allows for recognition of progressively longer NCCs: Fortunately, the objective function in Equations 1 3. Induce a base translation model between E is already a summation over source words. So, its and F. The algorithm can also be run in &quot;two-sided&quot; mode, so that it looks for NCCs in E and in F on alternate iterations. This mode enables the translation model to link NCCs in one language to NCCs in the other. In its simplest form, the algorithm only considers adjacent words as candidate NCCs. However, function words are translated very inconsistently, and it is difficult to model their translational distributions accurately. To make discovery of NCCs involving function words more likely, I consider content words that are separated by one or two functions words to be adjacent. Thus, NCCs like &quot;blow ... whistle&quot; and &quot;icing ... cake&quot; may contain gaps. Fusing NCCs with gaps may fuse some words incorrectly, when the NCC is a frozen expression. For example, we would want to recognize that &quot;icing ... cake&quot; is an NCC when we see it in new text, but not if it occurs in a sentence like &quot;Mary ate the icing off the cake.&quot; It is necessary to determine whether the gap in a given NCC is fixed or not. Thus, the price for this flexibility provided by NCC gaps is that, before Step 7, the algorithm fills gaps in proposed NCCs by looking through the text. Sometimes, NCCs have multiple possible gap fillers, for example &quot;make up {my,your,his,their} mind.&quot; When the gap filling procedure finds two or three possible fillers, the most frequent filler is used, and the rest are ignored in the hope that they will be discovered on the next iteration. When there are more than three possible fillers, the NCC retains the gap. The token fuser (in Steps 2 and 7) knows to shift all words in the NCC to the location of the leftmost word. E.g. an instance of the previous example in the text might be fused as &quot;make_up_< GAP >_mind his.&quot; In principle, the NCC discovery algorithm could iterate until Axy < 0 for all bigrams. This would be a classic case of over-fitting the model to the training data. NCC discovery is more useful if it is stopped at the point where the NCCs discovered so far would maximize the application's objective function on new data. A domain-independent method to find this point is to use held-out data or, more generally, to cross-validate between different subsets of the training data. Alternatively, when the applications involves human inspection, e.g. for bilingual lexicography, a suitable stopping point can be found by manually inspecting validated NCCs.
6 Credit Estimation. Sections 3 and 4 describe how to carry out NCC validity tests, but not how to choose which NCCs to test. Making this choice at random would make the discovery process too slow, because the vast majority of word sequences are not valid NCCs. The discovery process can be greatly accelerated by testing only candidate NCCs for which Equation 4 is likely to be positive. This section presents a way to guess whether Axy > 0 for a candidate NCC xy before inducing a translation model that involves this NCC. To do so, it is necessary to estimate i'(x), i'(y), and i' (xy), using only the base translation model. First, a bit of notation. Let LC and RC denote word contexts to the left and to the right. Let (x : RC = y) be the set of tokens of x whose right context is y, and vice versa for (y : LC = X). Now, i'(x) and i'(y), can be estimated under Assumption 2 When x occurs without y in its context, it will be linked to the same target words by the trial translation model as by the base translation model, and likewise for y without x. Estimating ii(xy) is more difficult because it requires knowledge of the entire translational distributions of both x and y, conditioned on all the contexts of x and y. Since we wish to consider hundreds of candidate NCCs simultaneously, and contexts from many megabytes of text, all this information would not fit on disk, let alone in memory. The best we can do is approximate with lower-order distributions that are easier to compute. The approximation begins with Assumption 3 implies that for all t E T Pr(xy, t) = Pr(x : RC = y, t) Pr(y : LC = x, t) (8) The approximation continues with Under Assumptions 3 and 4, we can estimate i'(xy) as shown in Figure 2. The final form of Equation 5 (in Figure 2) allows us to partition all the terms in Equation 4 into two sets, one for each of the components of the candidate NCC: (11) where terms in All the Equation 12 depend only on the probability distributions Pr(x, t), Pr(x : RC = y, t) and Pr(x : RC 0 y, t). All the terms in Equation 13 depend only on Pr(y, t), Pr(y : LC = x, t) and Pr(y : LC 0 x, t). These distributions can be computed efficiently by memory-external sorting and streamed accumulation.
7 Bag-of-Words Translation. In bag-of-words translation, each word in the source text is simply replaced with its most likely translation. No target language model is involved. For this application, it is sufficient to predict only the maximum likelihood translation of each source word. The rest of the translational distribution can be ignored. Let mr(s) be the most likely translation of each source word s, according to the translation model: Again, I will write simply m(s) when T is clear from the context. The objective function V for this application follows by analogy with the mutual information function / in Equation 1: The Kronecker ö function is equal to one when its arguments are identical and zero otherwise. The form of the objective function again permits easy distribution of its value over the s E S: The formula for estimating the net change in the objective function due to each candidate NCC remains the same: It is easier to estimate the values of v' using only the base translation model, than to estimate the values of since only the most likely translations need to be considered, instead of entire translational distributions. v' (a;) and v' (y) are again estimated under Assumption 2: v'(xy) can be estimated without making the strong assumptions 3 and 4. Instead, I use the weaker Assumption 5 Let tx and ty be the most frequent translations of x and y in each other's presence, in the base translation model. The most likely translation of xy in the trial translation model will be the more frequent of tx and ti,. This quantity can be computed exactly at a reasonable computational expense.
8 Experiments. To demonstrate the method's applicability to data other than parallel texts, and to illustrate some of its interesting properties, I describe my last experiment first. I applied the mutual information objective function and its associated predictive value function to a data set consisting of spellings and pronunciations of 17381 English words. Table 1 shows the NCCs of English spelling that the algorithm discovered on the first 10 iterations. The table reveals some interesting behavior of the algorithm. The NCCs &quot;er,&quot; &quot;ng&quot; and &quot;ow&quot; were validated because this data set represents the sounds usually produced by these letter combinations with one phoneme. The NCC &quot;es&quot; most often appears in word-final position, where the &quot;e&quot; is silent. However, when &quot;es&quot; is not word-final, the &quot;e&quot; is usually not silent, and the most frequent following letter is &quot;s&quot;, which is why the NCC &quot;ess&quot; was validated. NCCs like &quot;tio&quot; and &quot;ough&quot; are built up over multiple iterations, sometimes out of pairs of previously discovered NCCs. The other two experiments were carried out on transcripts of Canadian parliamentary debates, known as the Hansards. French and English versions of these texts were aligned by sentence using the method of Gale & Church (1991). Morphological variants in both languages were stemmed to a canonical form. Thirteen million words (in both languages combined) were used for training and another two and a half million were used for testing. All translation models were induced using the method of Melamed (1997). Six iterations of the NCC discovery algorithm were run in &quot;two-sided&quot; mode, using the objective function /, and five iterations were run using the objective function V. Each iteration took approximately 78 hours on a 167MHz UltraSPARC processor, running unoptimized Perl code. Tables 2 and 3 chart the NCC discovery process. The NCCs proposed for the V objective function were much more likely to be validated than those proposed for I, because the predictive value function v' is much easier to estimate a priori than the predictive value function i'. In 3 iterations on the English side of the bitext, 192 NCCs were validated for I and 1432 were validated for V. Of the 1432 NCCs validated for V, 84 NCCs consisted of 3 words, 3 consisted of 4 words and 2 consisted of 5 words. The French NCCs were longer on average, due to the frequent &quot;N de N&quot; construction for noun compounds. The first experiment on the Hansards involved the mutual information objective function I and its associated predictive value function in Equation 3. The first step in the experiment was the construction of 5 new versions of the test data, in addition to the original version. Version k of the test data was constructed by fusing all NCCs validated up to iteration k on the training data. The second step was to induce a translation model from each version of the test data. There was no opportunity to measure the impact of NCC recognition under the objective function I on any real application, but Figure 3 shows that the mutual information of successive test translation models rose as desired. The second experiment was based on the simpler objective function V and its associated predictive value function in Equation 16. The impact of NCC recognition on the bag-of-words translation task was measured directly, using Bitext-Based Lexicon Evaluation (BiBLE: Melamed, 1995). BiBLE is a family of evaluation algorithms for comparing different translation methods objectively and automatically. The algorithms are based on the observation that if translation method A is better than translation method B, and each method produces a translation from one half of a held-out test bitext, then the other half of that bitext will be more similar to the translation produced by A than to the translation produced by B. In the present experiment, the translation method was always bag-of-words translation, but using different translation models. The similarity of two texts was measured in terms of word precision and word recall in aligned sentence pairs, ignoring word order. I compared the 6 base translation models induced in 6 iterations of the algorithm in Section 5.5 The first model is numbered 0, to indicate that it did not recognize any NCCs. The 6 translation models were evaluated on the test bitext (E, F) using the following BiBLE algorithm: The BiBLE algorithm compared the 6 models in both directions of translation. The results are detailed in Figures 4 and 5. Figure 6 shows F-measures that are standard in the information retrieval literature: The absolute recall and precision values in these figures are quite low, but this is not a reflection of the quality of the translation models. Rather, it is an expected outcome of BiBLE evaluation, which is quite harsh. Many translations are not word for word in real bitexts and BiBLE does not even give credit for synonyms. The best possible performance on this kind of BiBLE evaluation has been estimated at 62% precision and 60% recall (Melamed, 1995). The purpose of BiBLE is internally valid comparison, rather than externally valid benchmarking. On a sufficiently large test bitext, BiBLE can expose the slightest differences in translation quality. The number of NCCs validated on each iteration was never more than 2.5% of the vocabulary size. Thus, the curves in Figures 4 and 5 have a very small range, but the trends are clear. A qualitative assessment of the NCC discovery method can be made by looking at Table 4. It contains a random sample of 50 of the English NCCs accumulated in the first five iterations of the algorithm in Section 5, using the simpler objective function V. All of the NCCs in the table are noncompositional with respect to the objective function V. Many of the NCCs, like &quot;red tape&quot; and &quot;blaze the trail,&quot; are true idioms. Some NCCs are incomplete. E.g. &quot;flow-&quot; has not yet been recognized as a non-compositional part of &quot;flow-through share,&quot; and likewise for &quot;head&quot; in &quot;rear its ugly head.&quot; These NCCs would likely be completed if the algorithm were allowed to run for more iterations. Some of the other entries deserve more explanation. First, &quot;Della Noce&quot; is the last name of a Canadian Member of Parliament. Every occurrence of this name in the French training text was tokenized as &quot;Della noce&quot; with a lowercase &quot;n,&quot; because &quot;noce&quot; is a common noun in French meaning &quot;marriage,&quot; and the tokenization algorithm lowercases all capitalized words that are found in the lexicon. When this word occurs in the French text without &quot;Della,&quot; its English translation is &quot;marriage,&quot; but when it occurs as part of the name, its translation is &quot;Noce.&quot; So, the French bigram &quot;Della Noce&quot; is noncompositional with respect to the objective function V. It was validated as an NCC. On a subsequent iteration, the algorithm found that the English bigram &quot;Della Noce&quot; was always linked to one French word, the NCC &quot;Della_noce,&quot; so it decided that the English &quot;Della Noce&quot; must also be an NCC. This is one of the few non-compositional personal names in the Hansards. Another interesting entry in the table is the last one. The capitalized English words &quot;Generic&quot; and &quot;Association&quot; are translated with perfect consistency to &quot;Generic&quot; and &quot;association,&quot; respectively, in the training text. The translation of the middle two words, however, is non-compositional. When &quot;Pharmaceutical&quot; and &quot;Industry&quot; occur together, they are rendered in the French text without translation as &quot;Pharmaceutical Industry.&quot; When they occur separately, they are translated into &quot;pharmaceutique&quot; and &quot;industrie.&quot; Thus, the English bigram &quot;Pharmaceutical Industry&quot; is an NCC, but the words that always occur around it are not part of the NCC. Similar reasoning applies to &quot;ship unprocessed uranium.&quot; The bigram < ship, unprocessed > is an NCC because its components are translated noncompositionally whenever they co-occur. However, &quot;uranium&quot; is always translated as &quot;uranium,&quot; so it is not a part of the NCC. This NCC demonstrates that valid NCCs may cross the boundaries of grammatical constituents.
9 Related Work. In their seminal work on statistical machine translation, Brown et al. (1993) implicitly accounted for NCCs in the target language by estimating &quot;fertility&quot; distributions for words in the source language. A source word s with fertility n could generate a sequence of 71 target words, if each word in the sequence was also in the translational distribution of s and the target language model assigned a sufficiently high probability to the sequence. However, Brown et al. 's models do not account for NCCs in the source language. Recognition of source-language NCCs would certainly improve the performance of their models, but Brown et al. warn that ... one must be discriminating in choosing multi-word cepts. The caution that we have displayed thus far in limiting ourselves to cepts with fewer than two words was motivated primarily by our respect for the featureless desert that multi-word cepts offer a priori. (Brown et al., 1993) The heuristics in Section 6 are designed specifically to find the interesting features in that featureless desert. Furthermore, translational equivalence relations involving explicit representations of targetlanguage NCCs are more useful than fertility distributions for applications that do translation by table lookup. Many authors (e.g. Daille et al., 1994; Smadja et al., 1996) define &quot;collocations&quot; in terms of monolingual frequency and part-of-speech patterns. Markedly high frequency is a necessary property of NCCs, because otherwise they would fall out of use. However, at least for translationrelated applications, it is not a sufficient property. Non-compositional translation cannot be detected reliably without looking at translational distributions. The deficiency of criteria that ignore translational distributions is illustrated by their propensity to validate most personal names as &quot;collocations.&quot; At least among West European languages, translations of the vast majority of personal names are perfectly compositional. Several authors have used mutual information and similar statistics as an objective function for word clustering (Dagan et at., 1993; Brown et at., 1992; Pereira et at., 1993; Wang et at., 1996), for automatic determination of phonemic baseforms (Lucassen & Mercer, 1984), and for language modeling for speech recognition (Ries et at., 1996). Although the applications considered in this paper are different, the strategy is similar: search a space of data models for the one with maximum predictive power. Wang et at. (1996) also employ parallel texts and independence assumptions that are similar to those described in Section 6. Like Brown et at. (1992), they report a modest improvement in model perplexity and encouraging qualitative results. Unfortunately, their estimation method cannot propose more than ten or so word-pair clusters before the translation model must be re-estimated. Also, the particular clustering method that they hoped to improve using parallel data is not very robust for low frequencies. So, like Smadja et at., they were forced to ignore all words that occur less than five times. If appropriate objective functions and predictive value functions can be found for these other tasks, then the method in this paper might be applied to them. There has been some research into matching compositional phrases across bitexts. For example, Kupiec (1993) presented a method for finding translations of whole noun phrases. Wu (1995) showed how to use an existing translation lexicon to populate a database of &quot;phrasal correspondences&quot; for use in example-based MT. These compositional translation patterns enable more sophisticated approaches to MT. However, they are only useful if they can be discovered reliably and efficiently. Their time may come when we have a better understanding of how to model the human translation process.
10 Conclusion. It is well known that two languages are more informative than one (Dagan et at., 1991). I have argued that texts in two languages are not only preferable but necessary for discovery of noncompositional compounds for translation-related applications. Given a method for constructing statistical translation models, NCCs can be discovered by maximizing the models' information-theoretic predictive value over parallel data sets. This paper presented an efficient algorithm for such ontological discovery. Proper recognition of NCCs resulted in improved performance on a simple MT task. Lists of NCCs derived from parallel data may be useful for NLP applications that do not involve parallel data. Translation-oriented NCC lists can be used directly in applications that have a human in the loop, such as computer-assisted lexicography, computer-assisted language learning, and corpus linguistics. To the extent that translation-oriented definitions of compositionality overlap with other definitions, NCC lists derived from parallel data may benefit other applications where NCCs play a role, such as information retrieval (Evans & Zhai, 1996) and language modeling for speech recognition (Ries et at., 1996). To the extent that different applications have different objective functions, optimizing these functions can benefit from an understanding of how they differ. The present work was a step towards such understanding, because &quot;an explication of a monolingual idiom might best be given after bilingual idioms have been properly understood&quot; (Bar-Hillel, 1964, p. 48). The NCC discovery method makes few assumptions about the data sets from which the statistical translation models are induced. As demonstrated in Section 8, the method can find NCCs in English letter strings that are aligned with their phonetic representations. We hope to use this method to discover NCCs in other kinds of parallel data. A natural next target is bitexts involving Asian languages. Perhaps the method presented here, combined with an appropriate translation model, can make some progress on the word identification problem for languages like Chinese and Japanese. 2 wrongful conviction erreur judiciaire 2 weak sister parent pauvre 2 of both the users and providers of transportation des utilisateurs et des transporteurs 2 understand the motivation saisir le motif 2 swimming pool piscine 2 ship unprocessed uranium expedier de l'uranium non raffine 2 by reason of insanity pour cause d'alienation mentale 2 l'agence de Presse libre du Québec l'agence de Presse libre du Québec 2 do cold weather research etudier l'effet du froid 2 the bread basket of the nation le grenier du Canada 2 turn back the boatload of European Jews renvoyer tout ces juifs europeens 2 Generic Pharmaceutical Industry Association Generic Pharmaceutical Industry Association