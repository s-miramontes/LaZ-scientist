1 Introduction. Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query. Landauer and Littman used SVD analysis (or Latent Semantic Indexing) on the Canadian Hansards, parliamentary debates that are published in both English and French, in order to estimate a kind of soft thesaurus. They then showed that these estimates could be used to retrieve documents appropriately in the bilingual condition where the query and the document were written in different languages. We have been most interested in the terminology application. How does Microsoft, or some other software vendor, want &quot;dialog box,&quot; &quot;text box,&quot; and &quot;menu box&quot; to be translated in their manuals? Considerable time is spent on terminology questions, many of which have already been solved by other translators working on similar texts. It ought to be possible for a translator to point at an instance of &quot;dialog box&quot; in the English version of the Microsoft Windows manual and see how it was translated in the French version of the same manual. Alternatively, the translator can ask for a bilingual concordance as shown in Figure 1. A PCbased terminology reuse tool is being developed to do just exactly this. The tool depends crucially on the results of an alignment program to determine which parts of the source text correspond with which parts of the target text. In working with the translators at AT&T Language Line Services, a commercial translation service, we discovered that we needed to completely redesign our alignment programs in order to deal more effectively with texts supplied by Language Line's customers. All too often the texts are not available in electronic form, and may need to be scanned in and processed by an OCR (optical character recognition) device. Even if the texts are available in electronic form, it may not be worth the effort to clean them up by hand. Real texts are not like the Hansards; real texts are much smaller and not nearly as clean as the ideal texts that have been used in previous studies. To deal with these robustness issues, Church (1993) developed a character-based alignment method called char_align. The method was intended as a replacement for sentence-based methods (e.g., (Brown et al., 1991a; Gale and Church, 1991b; Kay and Rosenschein, 1993)), which are very sensitive to noise. This paper describes a new program, called word_align, that starts with an initial &quot;rough&quot; alignment (e.g., the output of char_a/ign or a sentence-based alignment method), and produces improved alignments by exploiting constraints at the word-level. The alignment algorithm consists of two steps: (1) estimate translation probabilities, and (2) use these probabilities to search for most probable alignment path. The two steps are described in the following section.
2 The alignment Algorithm. The translation probabilities are estimated using a method based on Brown et al. 's Model 2 (1993), which is summarized in the following subsection, 2.1.1. Then, in subsection 2.1.2, we describe modifications that achieve three goals: (1) enable word_align to accept input which may not be aligned by sentence (e.g. char_al:gn's output), (2) reduce the number of parameters that need to be estimated, and (3) prepare the ground for the second step, the search for the best alignment (described in section 2.2). In the context of their statistical machine translation project (Brown et al., 1990), Brown et al. estimate Pr(f le), the probability that f, a sentence in one language (say French), is the translation of e, a sentence in the other language (say English). Pr(f le) is computed using the concept of altgnment, denoted by a, which is a set of connections between each French word in f and the corresponding English word in e. A connection, which we will write f e as con- specifies that position j in f is connected to position i in e. If a French word in f does not correspond to any English word in e, then it is connected to the special word null (position 0 in e). Notice that this model is directional, as each French position is connected to exactly one position in the English sentence (which might be the null word), and accordingly the number of connections in an alignment is equal to the length of the French sentence. However, an English word may be connected to several words in the French sentence, or not connected at all. Using alignments, the translation probability for a pair of sentences is expressed as where A is the set of all combinatorially possible alignments for the sentences f and e (calligraphic font will be used to denote sets). In their paper, Brown et al. present a series of 5 models of Pr(f le). The first two of these 5 models are summarized here. Model 1 Model 1 assumes that Pr(f, ale) depends primarily on t( f le), the probability that an occurrence of the English word e is translated as the French word f. That is, (2) where Cf e, an irrelevant constant, accounts for certain dependencies on sentence lengths, which are not important for our purposes here. Except for Cf e' most of the notation is borrowed from Brown et al.. The variable, j, is used to refer to a position in a French sentence, and the variable, i, is used to refer to a position in an English sentence. The expression, f, is used to refer to the French word in position j of a French sentence, and e, is used to refer to the English word in position i of an English sentence. An alignment, a, is a set of pairs (j, i), each of which connects a position in a French sentence with a corresponding position in an English sentence. The expression, ai , is used to refer to the English position that is connected to the French position j, and the expression, is used to refer to the English word in position ai. The variable, m, is used to denote the length of the French sentence and the variable, I, is used to denote the length of the English sentence. There are quite a number of constraints that could be used to estimate Pr(f, ale). Model 1 depends primarily on the translation probabilities, t(f(e), and does not make use of constraints involving the positions within an alignment. These constraints will be exploited in Model 2. Brown et al. estimate t(fle) on the basis of a training set, a set of English and French sentences that have been aligned at the sentence level. Those values of t(fle) that maximize the probability of the training set are called the maximum likelihood estimates. Brown et a/. show that the maximum likelihood estimates satisfy It follows from the definition of Model 1 that the probability of a connection satisfies: Recall that h refers to the French word in position j of the French sentence f of length m, and that ei refers to the English word in position i of the English sentence e of length I. Also, remember that position 0 is reserved for the null word. Equations 3 and 4 are used iteratively to estimate t(f le). That is, we start with an initial guess for (f le). We then evaluation the right hand side of equation 4, and compute the probability of the connections in the training set. Then we evaluate equation 3, obtain new estimates for the translation probabilities, and repeat the process, until it converges. This iterative process is known as the EM algorithm and has been shown to converge to a stationary point (Baum, 1972; Dempster et al., 1977). Moreover, Brown et a/. show that Model 1 has a unique maximum, and therefore, in this special case, the EM algorithm is guaranteed to converge to the maximum likelihood solution, and does not depend on the initial guess. Model 2 Model 2 improves upon model 1 by making use of the positions within an alignment. For instance, it is much more likely that the first word of an English sentence will be connected to a word near the beginning of the corresponding French sentence, than to some word near the end of the French sentence. Model 2 enhances Model 1 with the assumption that the probability of a connection, con, depends also on j and i (the positions in f and e), as well as on m and 1 (the lengths of the two sentences). This dependence is expressed through the term a(iIj, m,1), which denotes the probability of connecting position j in a French sentence of length m with position i in an English sentence of length I. Since each French position is connected to exactly one English position, the constraint El=c) where e C'f is an irrelevant constant. , As in Model 1, equation 3 holds for the maximum likelihood estimates of the translation probabilities. The corresponding equation for the maxwhere CO/tile and CON.,, denote sets of connections: the set CO.Arf,, contains all connections in the training data between f and e, and the set CON.,, contains all connections between some French word and e. The probability of a connection, con'.e • , is the sum of the probabilities of all alignments that contain it. Notice that equation 3 satisfies the constraint: E t(fle) = 1, for each English word e. where CONV denotes the set of connections in the training data between positions j and i in French and English sentences of lengths in and 1, respectively. Similarly, CO.A7.3 denotes the set of connections between position j and some English position, in sentences of these lengths. Instead of equation 4, we obtain the following equation for the probability of a connection: Notice that Model 1 is a special case of Model 2, where a(i1j,m,l) is held fixed at As before, the EM algorithm is used to compute maximum likelihood estimates for t(fle) and a(i1j, m, 1) (using first equation 7, and then equations 3 and 6). However, in this case, Model 2 does not have a unique maximum, and therefore the results depend on the initial guesses. Brown et al. therefore use Model 1 to obtain estimates for t(f le) which do not depend on the initial guesses. These values are then used as the initial guesses of t(f)e) in Model 2. As mentioned in the introduction, we are interested in aligning corpora that are smaller and noisier than the Hansards. This implies severe practical constraints on the word alignment algorithm. As mentioned earlier, we chose to start with the output of char_align because it is more robust than alternative sentence-based methods. This choice, of course, requires certain modifications to the model of Brown et al. to accommodate as input an initial rough alignment (such as produced by char_align) instead of pairs of aligned sentences. It is also useful to reduce the number of parameters that we are trying to estimate, because we have much less data and much more noise. The paragraphs below describe our modifications which are intended to meet these somewhat different requirements. The two major modifications are: (a) replacing the sentence-by-sentence alignment with a single global alignment for the entire corpus, and (b) replacing the set of probabilities a(ijj, in,!) with a small set of offset probabilities. Word_align starts with an initial rough alignment, I, which maps French positions to English positions (if the mapping is partial, we use linear extrapolation to make it complete). Our goal is to find a global alignment. A, which is more accurate than I. To achieve this goal, we first use I to determine which connections will be considered for A. Let coni,i denote a connection between position j in the French corpus and position i in the English corpus (the super-scripts in cof e are omitted, as there is no notion of sentences). We assume that coni,i is a possible connection only if i falls within a limited window which is centered around 1(j), such that: where w is a predetermined parameter specifying the size of the window (we typically set w to 20 words). Connections that fall outside this window are assumed to have a zero probability. This assumption replaces the assumption of Brown et al. that connections which cross boundaries of aligned sentences have a zero probability. In this new framework, equation 3 becomes: where CONL, and CON. ,e are taken from the set of possible connections, as defined by (8). Turning to Model 2, the parameters of the form a(i1j,m,l) are somewhat more problematic. First, since there are no sentence boundaries, there are no direct equivalents for i, j, in and 1. Secondly, there are too many parameters to be estimated, given the limited size of our corpora (one parameter for each combination of i, j, in and 1). Fortunately, these parameters are highly redundant. For example, it is likely that a(i1j, m, 1) will be very close to a(i + 11j+ 1, m, /) and a(i1j, m + 1,1+1). In order to deal with these concerns, we replace probabilities of the form a(iIj, in, 1) with a small set of offset probabilities. We use k to denote the offset between i, an English position which corresponds to the French position j, and the English position which the input alignment I connects to j: k = i —I(j). An offset probability, o(k), is the probability of having an offset k for some arbitrary connection. According to (8), k ranges between —w and w. Thus, instead of equation 6, we have where CON is the set of all connections and COArk is the set of all connections with offset k. Instead of equation 7, we have The last three equations are used in the EM algorithm in an iterative fashion as before to estimate the translation probabilities and the offset probabilities. Table 1 and Figure 2 show some values that were estimated in this way. The input consisted of a pair of Microsoft Windows manuals in English (125,000 words) and its equivalent in French (143,000 words). Table 1 shows four French words and the four most likely translations, sorted by t(el f)1 . Note that the correct translation(s) are usually near the front of the list, though there is a tendency for the program to be confused by collocates such as &quot;information about&quot;. Figure 2 shows the probability estimates for offsets from the initial alignment I. Note that smaller offsets are more likely than larger ones, as we would expect. Moreover, the distribution is reasonably close to normal, as indicated by the dotted line, which was generated by a Gaussian with a mean of 0 and standard deviation of 102. We have found it useful to make use of three filters to deal with robustness issues. Empirically, we found that both high frequency and low frequency words caused difficulties and therefore connections involving these words are filtered out. The thresholds are set to exclude the most frequent function words and punctuations, as well as words with less than 3 occurrences. In addition, following a similar filter by Brown et al., small values of t( f le) are set to 0 after each iteration of the EM algorithm because these small values often correspond to inappropriate translations. Finally, connections to null are ignored. Such connections model French words that are often omitted in the English translation. However, because of OCR errors and other sources of noise, it was decided that this phenomenon was too difficult to model. Some words will not be aligned because of these heuristics. It may not be necessary, however, to align all words in order to meet the goal of helping translators (and lexicographers) with difficult terminology. The EM algorithm produces two sets of maximum likelihood probability estimates: translation probabilities, 1(1 le), and offset probabilities, o(k). Brown et al. select their preferred alignment simply by choosing the most probable alignment according to the maximum likelihood probabilities, relative to the given sentence alignment. In the terms of our 'In this example, French is used as the source language and English as the target. 2The center of the estimated distribution seems more fiat than in a normal distribution. This might be explained by a higher tendency for local changes of word order within phrases than for order changes among phrases. This is merely a hypothesis, though, which requires further testing. Unfortunately, this method does not model the dependence between connections for French words that are near one another. For example, the fact that the French position j was connected to the English position i will not increase the probability that j + 1 will be connected to an English position near i. The absence of such dependence can easily confuse the program, mainly in aligning adjacent occurrences of the same word, which are common in technical texts. Brown et al. introduce such dependence in their Model 4. We have selected a simpler alternative defined in terms of offset probabilities. The first step in finding the most probable alignment is to determine the relevant connectzons for each French position. Relevant connections are required to be reasonably likely, that is, their translation probability (t(f le)) should exceed some minimal threshold. Moreover, they are required to fall within a window between 1(j) — w and 1(j) + w in the English corpus, as in the previous step (parameter estimation). We call a French position relevant if it has at least one relevant connection. Each alignment A then consists of exactly one connection for each relevant French position (the irrelevant positions are ignored). To model the dependency between connections in an alignment, we assume that the offset of a connection is determined relative to the preceding connection in A, instead of relative to the initial alignment, I. For this purpose, we define A' (j) as a linear extrapolation from the preceding connection in A: where previ is the last French position before j which is aligned by A and NE and NF are the lengths of the English and French corpora. A'(j) thus predicts the connection of j, knowing the connection of jprey and assuming that the two languages have the same word order. Instead of (12), the most probable alignment maximizes French word zone fermer informations insertion English translations (with probabilities) box (0.58) area (0.28) want (0.04) In (0.02) close (0.44) when (0.08) Close (0.07) selected (0.06) information (0.66) about (0.15) For (0.12) see (0.04) insertion (0.61) point (0.23) Edit (0.06) To (0.05) We approximate the offset probabilities, o(k), relative to A', using the maximum likelihood estimates which were computed relative to I (as described in Section 2.1.2). We use a dynamic programming algorithm to find the most probable alignment. This enables us to know the value A/ 1 when dealing with position j. To avoid connections with very low probability (due to a large offset) we require that i( f, lei ) • o(i — (j)) exceeds a pre-specified threshold T3. If the threshold is not exceeded, the connection is dropped from the alignment, and i(file) • o(i — (j)) for that connection is set to T when computing (14). T can therefore be interpreted as a global setting of the probability that a random position will be connected to the null 'In fact, the threshold on t(hle,), which is used to determine the relevant connections (described in the previous subsection), is used just as an efficient early application of the threshold T. This early application is possible when t(hle,)- o(k,„„x) < T, where kmax is the value of k with maximal o(k). English word'. A similar dynamic programming approach was used by Gale and Church for word alignment (Gale and Church, 1991a), to handle dependency between connections.
3 Evaluation. Word_olign was first evaluated on a representative sample of Canadian Hansards (160,000 words in English and French). The sample was kindly provided by Simard et al., along with alignments of sentence boundaries as determined by their panel of 8 judges (Simard et al., 1992). Ten iterations of the EM algorithm were computed to estimate the parameters of the model. The window size was set to 20 words in each direction, and the minimal threshold for t(fle) was set to 0.005. We considered connections whose source and target words had frequencies between 3 and 1700 (1700 is the highest frequency of a content word in the corpus. We thus excluded as many 'As mentioned earlier, we do not estimate directly translation probabilities for the null English word. function words as possible, but no content words). In this experiment, we used French as the source language and English as the target language. Figure 3 presents the alignment error rate of word_align. It is compared with the error rate of word_align's input, i.e. the initial rough alignment which is produced by char_align. The errors are sampled at sentence boundaries, and are measured as the relative distance between the output of the alignment program and the &quot;true&quot; alignment, as defined by the human judges5. The histograms present errors in the range of -20-20, which covers about 95% of the data.6. It can be seen that word_align decreases the error rate significantly (notice the different scales of the vertical axes). In 55% of the cases, there is no error in word_align's output (distance of 0), in 73% the distance from the correct alignment is at most 1, and in 84% the distance is at most 3. A second evaluation of word_align was performed on noisy technical documents, of the type typically available for AT&T Language Line Services. We used the English and French versions of a manual of monitoring equipment (about 65,000 words), both scanned by an OCR device. We sampled the English vocabulary with frequency between three and 450 occurrences, the same vocabulary that was used for alignment. We sampled 100 types from the top fifth by frequency of the vocabulary (quintile), 80 types from the second quintile, 60 from the third, 40 from the fourth, and 20 from the bottom quintile. We used this stratified sampling because we wanted to make more accurate statements about our error rate by tokens than we would have obtained from random sampling, or even from equal weighting of the quintiles. After choosing the 300 types from the vocabulary list, one token for each type was chosen at random from the corpus. By hand, the best corresponding position in the French version was chosen, to be compared with word_align's output. Table 2 summarizes the results of the second experiment. The figures indicate the expected relative frequency of each offset from the correct alignment. This relative frequency was computed according to the word frequencies in the stratified sample. As shown in the table, for 60.5% of the tokens the alignment is accurate, and in 84% the offset from the correct alingment is at most 3. These figures demonstrate the usefulness of word_align for constructing bilingual lexicons, and its impact on 'As explained earlier, word_align produces a partial alignment. For the purpose of the evaluation, we used linear interpolation to get alignments for all the positions in the sample. 6Recall that the window size we used is 20 words in each direction, which means that word_align cannot recover from larger errors in char_align. char align errors (in words) Figure 3: Word_align reduces the variance (average square error) by a factor of 5 over char_align alone (notice the vertical scales). the quality of bilingual concordances (as in Figure 1). Indeed, using bilingual concordances which are based on word_align's output, the translators at AT&T Language Line Services are now producing bilingual terminology lexicons at a rate of 60-100 terms per hour! This is compared with the previous rate of about 30 terms per hour using char_al:gn's output, and an extremely lower rate before alignment tools were available.
4 Conclusions. Compared with other word alignment algorithms (Brown et al., 1993; Gale and Church, 1991a), word_align does not require sentence alignment as input, and was shown to produce useful alignments for small and noisy corpora. Its robustness was achieved by modifying Brown et al. 's Model 2 to handle an initial &quot;rough&quot; alignment, reducing the number of parameters and introducing a dependency between alignments of adjacent words. Taking the output of char_align as input, word_align produces significantly better, wordlevel, alignments on the kind of corpora that are typically available to translators. This improvement increased the rate of constructing bilingual terminology lexicons at AT&T Language Line Services by a factor of 2-3. In addition, the alignments may also be helpful to developers of lexicons for machine translation systems. Word_align thus provides an example how a model such as Brown et al. 's Model 2, that was originally designed for research in statistical machine translation, can be modified to achieve practical, though less ambitious, goals in the near term.