1 Introduction. Distributional models of semantics have seen considerable success at simulating a wide range of behavioral data in tasks involving semantic cognition and also in practical applications. For example, they have been used to model judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landauer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as a Foreign Language (TOEFL). This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been directed at the most effective ways of constructing representations for individual words, there has been far less consensus regarding the representation of larger constructions such as phrases and sentences. The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Smolensky, 1990; Plate, 1995). More recently, several proposals have been put forward for computing the meaning of word combinations in vector spaces. This renewed interest is partly due to the popularity of distributional methods and their application potential to tasks that require an understanding of larger phrases or complete sentences. For example, Mitchell and Lapata (2010) introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v. Different composition models arise, depending on how f is chosen. Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001). Alternatively, assuming that composition is a linear function of the tensor product of u and v, gives rise to models based on multiplication. One of the most sophisticated proposals for semantic composition is that of Clark et al. (2008) and the more recent implementation of Grefenstette and Sadrzadeh (2011a). Using techniques from logic, category theory, and quantum information they develop a compositional distributional semantics that brings type-logical and distributional vector space models together. In their framework, words belong to different type-based categories and different categories exist in different dimensional spaces. The category of a word is decided by the number and type of adjoints (arguments) it can take and the composition of a sentence results in a vector which exists in sentential space. Verbs, adjectives and adverbs act as relational functions, are represented by matrices, and modify the properties of nouns, that are represented by vectors (see also Baroni and Zamparelli (2010) for a proposal similar in spirit). Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008). Socher et al. (2011a) and Socher et al. (2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences. The network is given a list of word vectors as input and a binary tree representing their syntactic structure. Then, it computes an n-dimensional representation p of two n-dimensional children and the process is repeated at every parent node until a representation for a full tree is constructed. Parent representations are computed essentially by concatenating the representations of their children. During training, the model tries to minimize the reconstruction errors between the n-dimensional parent vectors and those representing their children. This model can also compute compositional representations when the tree structure is not given, e.g., by greedily inferring a binary tree. Although the type of function used for vector composition has attracted much attention, relatively less emphasis has been placed on the basic distributional representations on which the composition functions operate. In this paper, we examine three types of distributional representation of increasing sophistication and their effect on semantic composition. These include a simple semantic space, where a word’s vector represents its co-occurrence with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional tuples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010), and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008). Word embeddings are distributed representations, low-dimensional and real-valued. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. Using these representations, we construct several compositional models, based on addition, multiplication, and recursive neural networks. We assess the effectiveness of these models using two evaluation protocols. The first one involves modeling similarity judgments for short phrases gathered in human experiments (Mitchell and Lapata, 2010). The second one is paraphrase detection, i.e., the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011a). We find that shallow approaches are as good as more computationally intensive alternatives. They achieve considerable semantic expressivity without any learning, sophisticated linguistic processing, or access to very large corpora. Our contributions in this work are three-fold: an empirical comparison of a broad range of compositional models, some of which are introduced here for the first time; the use of an evaluation methodology that takes into account the full spectrum of compositionality from phrases to sentences; and the empirical finding that relatively simple compositional models can be used to perform competitively on the paraphrase detection and phrase similarity tasks.
2 Modeling. The elementary objects that we operate on are vectors associated with words. We instantiate these word representations following three distinct semantic space models which we describe in Section 2.1 below. Analogously, in Section 2.2 we consider three methods of vector composition, i.e., how a phrase or a sentence can be represented as a vector using the vectors of its constituent words. Combining different vector representations and composition methods gives rise to several compositional models whose performance we evaluate in Sections 3 and 4. For all of our experiments we employ column vectors from a Cartesian, finitely-dimensional space. The dimensionality will depend on the source of the vectors involved. Similarly, the component values inside each source’s vectors are not to be interpreted in the same manner. Nonetheless, they have in common that they originate from distributive corpus statistics. meaning is commonly represented in a highdimensional space, where each component corresponds to some contextual element in which the word is found. The contextual elements can be words themselves, or larger linguistic units such as sentences or documents, or even more complex linguistic representations such as the argument slots of predicates. A semantic space that is often employed in studying compositionality across a variety of tasks (Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011a) uses a context window of five words on either side of the target word, and 2,000 vector dimensions. These are the common context words in the British National Corpus (BNC), a corpus of about 100 million tokens. Their values are set to the ratio of the probability of the context word given the target word to the probability of the context word overall. More formally, let us consider the BNC as a set of sentences: ni ) from the BNC’s vocabulary VocBNC. Then f reqw is the amount of times that each word w ∈ VocBNC appears in the BNC. Mitchell and Lapata (2010) collect the M most frequent non-stoplist words in the set ctxttop = {w(top)1, , wM(top) } and let them consitute the word vectors’ dimensions. Each dimension’s value is obtained from a co-occurrence count: for w ∈ VocBNC and j = 1,...,M. Using these counts, they define word vectors component-wise. for j = 1,...,M, where totalCount is the total number of words in the BNC. This space is relatively simple, it has few parameters, requires no preprocessing other than tokenization and involves no syntactic information or parameter learning. Despite its simplicity, it is a good starting point for studying representations for compositional models as a baseline against which to evaluate more elaborate models. Neural Language Model Another perhaps less well-known approach to meaning representation is to represent words as continuous vectors of parameters. Such word vectors can be obtained with an unsupervised neural language model (NLM, Bengio (2001); Collobert and Weston (2008)) which jointly learns an embedding of words into a vector space and uses these vectors to predict how likely a word is, given its context. We induced word embeddings with Collobert and Weston (2008)’s neural language model. The model is discriminative and non-probabilistic. Each word i ∈ D (the vocabulary) is embedded into a d-dimensional space using a lookup table LTW(·): where W ∈ Rd×|D |is a matrix of parameters to be learned. Wi ∈ Rd is the i-th column of W and d is the word vector size to be chosen by the user. The parameters W are automatically trained during the learning process using backpropagation. Specifically, at each training update, the model reads an n-gram x = (w1,...,wn) from the corpus. The n-gram is paired with a corrupted n-gram x˜ = (w1,..., ˜wn) where ˜wn =6 wn is chosen uniformly from the vocabulary. The model concatenates the learned embeddings of the n words and predicts a score for the n-gram sequence using the learned embeddings as features. The training criterion is that n-grams that are present in the training corpus must have a score at least some margin higher than the corrupted n-grams. The model learns via gradient descent over the neural network parameters and the embedding lookup table. Word vectors are stored in a word embedding matrix which captures syntactic and semantic information from co-occurrence statistics. As these representations are learned, albeit in an unsupervised manner, one would hope that they capture word meanings more succinctly, compared to the simpler distributional representations that are merely based on co-occurrence. We trained the neural language model on the BNC. We optimized the model’s parameters on a word similarity task using 4% of the BNC as development data. Specifically, we used WordSim353, a benchmark dataset (Finkelstein et al., 2001), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs. We experimented with vectors of varying dimensionality (ranging from 50 to 200, with a step size of 50). The size of the target word’s context window was 2, 3 and 4 in turn. The rate at which embeddings were learned ranged from 3.4 x 10−10 to 6.7 x 10−10 to 10−9. We ran each training process for 1.1 x 108 to 2.7 x 108 iterations (ca. 2 days). We obtained the best results with 50 dimensions, a context window of size 4, and a embedding learning rate of 10−9. The NLM with these parameters was then trained for 1.51x109 iterations (ca. 2 weeks). Figure 1 illustrates a two-dimensional projection of the embeddings for the 500 most common words in the BNC. We only show two out of the actual 50 dimensions involved, but one can already begin to see clusterings of a syntactic and semantic nature. In one corner, for example, we encounter a grouping of possessive pronouns together with the possessive clitic ’s. The singular ones my, her and his are closely positioned, as are the plural ones our, your and their. Also, there is a clustering of sociopolitical terms, such as international, country, national, government, and council. Distributional Memory Tensor Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. In their framework distributional information is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns give rise to different semantic spaces appropriate for capturing different semantic problems. In this way, the same distributional information can be shared across tasks such as word similarity or analogical learning. More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w,v and a connecting link-word l. This representation operates over a dependency-parsed corpus and the scores c are obtained via counting the occurrences of tuples, and weighting the raw counts by mutual information. Table 1 presents examples of tensor entries. These were taken from a distributional memory tensor1 that Baroni and Lenci obtained via preprocessing several corpora: the web-derived ukWac corpus of about 1.915 billion words, a mid-2009 dump of the English Wikipedia containing about 820 million words, and the BNC. Extracting a 3-dimensional tensor from the BNC alone would create very sparse representations. We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional subspace, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). We view the 3-dimensional tensor as a mapping which assigns each target word w a non-zero value c, given the context (l,v). All wordcontext combinations not listed in T are implicitly assigned a zero value. Now we consider two possible approaches for obtaining vectors, depending on their application. First, we let the D most frequent contexts constitute the D dimensions that each word vector will have. Table 2 shows the 11 contexts (l,v) that appear most frequently in T. Thus, each target word’s vector is defined component-wise as: for j = 1,...,D. This approach is used when a fixed vector dimensionality is necessary. A more dynamic approach is possible when very few words w1,...,wn are involved in a test. Their representations can then have a denser format, that is, with no zero-valued components. For this we identify the set of contexts common to the words involved, ctxtdyn = {(l(dyn) Each context (l,v) again constitutes a vector dimension. The dimensionality varies strongly depending on the selection of words, but if n does not exceed 4, the dimensionality |ctxtdyn |will typically be substantial enough. In this approach, each word’s vector consists of the values c found along with that word and its context in the tensor. In our experiments we compose word vectors to create representations for phrase vectors and sentence vectors. The phrases we are interested in consist of two words each: an adjective and a noun like black hair, a compound noun made up of two nouns such as oil industry, or a verbal phrase with a transitive verb and an object noun, e.g., pour tea. Conceiving of a phrase phr = (w1,w2) as a binary tuple of words, we obtain its vector from its words’ vectors either by addition: In the same way we acquire a vector senVeci representing a sentence Seni = (w(i) ni ) from the vectors for w1,...,wni. We simply sum the existing word vectors, that is, vectors obtained via the respective corpus for words that are not on our stoplist: And do the same with point-wise multiplication: The multiplication model in (13) can be seen as an instantiation of the categorical compositional framework put forward by Clark et al. (2008). In fact, a variety of multiplication-based models can be derived from this framework; and comparisons against component-wise multiplication on phrase similarity tasks yield comparable results (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b). We thus opt for the model (13) as an example of compositional models based on multiplication due to its good performance across a variety of tasks, including language modeling and prediction of reading difficulty (Mitchell, 2011). Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (2011a)’s model. They use the Stanford parser (Klein and Manning, 2003) to create a binary parse tree for each input phrase or sentence. This tree is then used as the basis for a deep recursive autoencoder (RAE). The aim is to construct a vector representation for the tree’s root bottom-up where the leaves contain word vectors. The latter can in theory be provided by any type of semantic space, however Socher et al. use word embeddings provided by the neural language model (Collobert and Weston, 2008). Given the binary tree input structure, the model computes parent representations p from their children (c1,c2) using a standard neural network layer: where [c1;c2] is the concatenation of the two children, f is an element-wise activation function such as tanh, b is a bias term, and W E Rnx2n is an encoding matrix that we want to learn during training. One way of assessing how well p represents its direct children is to decode their vectors in a reconstruction layer: During training, the goal is to minimize the reconstruction errors of all input pairs at nonterminal nodes p in a given parse tree by computing the square of the Euclidean distance between the original input and its reconstruction: Socher et al. (2011a) extend the standard recursive autoencoder sketched above in two ways. Firstly, they present an unfolding autoencoder that tries to reconstruct all leaf nodes underneath each node rather than only its direct children. And secondly, instead of transforming the two children directly into a parent p, they introduce another hidden layer inbetween. We obtained three compositional models per representation resulting in nine compositional models overall. Plugging different representations into the additive and multiplicative models is relatively straightforward. The RAE can also be used with arbitrary word vectors. Socher et al. (2011a) obtain best results with 100-dimensional vectors which we also used in our experiments. NLM vectors were trained with this dimensionality on the BNC for 7.9 x 108 iterations (with window size 4 and an embedding learning rate of 10−9). We constructed a simple distributional space with M = 100 dimensions, i.e., those connected to the 100 most frequent co-occurrence words. In the case of vectors obtained from Baroni and Lenci (2010)’s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). To represent phrases, we used vectors of dynamic dimensionality, since these form a richer and denser representation. The sentences considered in Section 4 are too large for this approach and all word vectors must be members of the same vector space. Hence, these sentence vectors have fixed dimensionality D = 100, consisting of the “most significant” 100 dimensions, i.e., those reflecting the 100 most frequent contexts.
3 Experiment 1: Phrase Similarity. Our first experiment focused on modeling similarity judgments for short phrases gathered in human experiments. Distributional representations of individual words are commonly evaluated on tasks based on their ability to model semantic similarity relations, e.g., synonymy or priming. Thus, it seems appropriate to evaluate phrase representations in a similar manner. Specifically, we used the dataset from Mitchell and Lapata (2010) which contains similarity judgments for adjective-noun, noun-noun and verb-object phrases, respectively.2 Each item is a phrase pair phr1, phr2 which has a human rating from 1 (very low similarity) to 7 (very high similarity). Using the composition models described above, we compute the cosine similarity of phr1 and phr2: Model similarities were evaluated against the human similarity ratings using Spearman’s p correlation coefficient. Table 3 summarizes the performance of the various models on the phrase similarity dataset. Rows in the table correspond to different vector representations: the simple distributional semantic space (SDS) from Mitchell and Lapata (2010), Baroni and Lenci’s (2010) distributional memory tensor (DM) and the neural language model (NLM), for each phrase combination: adjective noun (Adj-N), nounnoun (N-N) and verb object (V-Obj). For each phrase type we report results for each compositional model, namely additive (+), multiplicative (0) and recursive autoencoder (RAE). The table also shows the dimensionality of the input vectors next to the vector representation. As can be seen, for SDS the best performing model is multiplication, as it is mostly for DM. With regard to NLM, vector addition yields overall better results. In general, neither DM or NLM in any compositional configuration are able to outperform SDS with multiplication. All models in Table 3 are significantly correlated with the human similarity judgments (p < 0.01). Spearman’s p differences of 0.3 or more are significant at the 0.01 level, using a ttest (Cohen and Cohen, 1983).
4 Experiment 2: Paraphrase Detection. Although the phrase similarity task gives a fairly direct insight into semantic similarity and compositional representations, it is somewhat limited in scope as it only considers two-word constructions rather than naturally occurring sentences. Ideally, we would like to augment our evaluation with a task which is based on large quantities of natural data and for which vector composition has practical consequences. For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al. (2004). The corpus consists of sentence pairs Seni1,Seni2 and labels indicating whether they are in a paraphrase relationship or not. The vector representations obtained from our various compositional models were used as features for the paraphrase classification task. The MSRPC dataset contains 5,801 sentence pairs, we used the standard split of 4,076 training pairs (67.5% of which are paraphrases) and 1,725 test pairs (66.5% of which are paraphrases). In order to judge whether two sentences have the same meaning we employ Fan et al. (2008)’s liblinear classifier. For each of our three vector sources and three different compositional methods, we create the following features: (a) a vector representing the pair of input sentences either via concatenation (“con”) or subtraction (“sub”); (b) a vector encoding which words appear therein (“enc”); and (c) a vector made up of the following four other pieces of information: the cosine similarity of the sentence vectors, the length of Seni1, the length of Seni2, and the unigram overlap among the two sentences. In order to encode which words appear in each sentence and how often, we define a vector wdCounti for sentence Seni and enumerate all words occuring in the MSRPC: giving the word count vectors nMSRPC dimensions. Thus the k-th component of wdCounti is the frequency with which the word w(MSRPC) appears in for k = 1,...,nMSRPC. Even though nMSRPC may be large, the computer files storing our feature vectors do not explode in size because wdCount contains many zeros and the classifier allows a sparse notation of (non-zero) feature values. Regarding the last four features, we measured the similarity between sentences the same way as we did with phrases in section 3. Note that this is the cosine of the angle between senVeci1 and senVeci2. This enables us to observe the similarity or dissimilarity of two sentences independent of their sentence length. Even though each contained word increases or decreases the norm of the resulting sentence vector, this does not distort the overall similarity value, due to normalization. The lengths of Seni1 and Seni2 are simply the number of words they contain. The unigram overlap feature value may be viewed as the cardinality of the intersection of each sentence’s multisetbag-of-words. The latter is encoded in the alreadyintroduced wdCount vectors. Therefore, In order to establish which features work best for each representation and composition method, we exhaustively explored all combinations on a development set (20% of the original MSRPC training set). Tables 4 (accuracy) and 5 (F1) show our results on the test set with the best feature combinations for each model (shown in parentheses). Each row corresponds to a different type of composition and each column to a different word representation model. As can be seen, the distributional memory (DM) is the best performing representation for the additive composition model. The neural language model (NLM) gives best results for the recursive autoencoder (RAE), although the other two representations come close. And finally the simple distributional semantic space (SDS) works best with multiplication. Also note that the best performing models, namely DM with addition and SDS with multiplication, use a basic feature space consisting only of the cosine similarity of the composed sentence vectors, the length of the two sentences involved, and their unigram word overlap. Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008). More recently, the addition of syntactic features based on dependency parse trees (Wan et al., 2006; Das and Smith, 2009) has been shown to substantially boost performance. The model of Das and Smith (2009), for example, uses quasi-synchronous dependency grammar to model the structure of the sentences involved in the comparison and their correspondences. Socher et al. (2011a) obtain an accuracy that is higher than previously published results. This model is more sophisticated than the one we used in our experiments (see Table 4 and 5). Rather than using the output of the RAE as features for the classifier, it applies dynamic pooling, a procedure that takes a similarity matrix as input (e.g., created by sentences with differing lengths) and maps it to a matrix of fixed size that represents more faithfully the global similarity structure.3 Overall, we observe that our own models do as well as some of the models that employ WordNet and more sophisticated syntactic features. With regard to F1, we are comparable with Das and Smith (2009) and Socher et al. (2011a) without using elaborate features, or any additional manipulations over and above the output of the composition functions 3Without dynamic pooling, their model yields an accuracy of 74.2. which if added could increase performance.
5 Discussion. In this paper we systematically compared three types of distributional representation and their effect on semantic composition. Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). These representations vary in many respects: the amount of preprocessing and linguistic information involved (the third-order tensor computes semantic representations over parsed corpora), whether the semantic space is the by-product of a learning process (in the neural language model the parameters of the lookup table must be learned), and data requirements (the third-order tensor involves processing billions of words). These representations served as input to three composition methods involving addition, multiplication and a deep recursive autoencoder. Again these methods differ in terms of how they implement compositionality: addition and multiplication are commutative and associative operations and thus ignore word order and, more generally, syntactic structure. In contrast, the recursive autoencoder is syntax-aware as it operates over a parse tree. However, the composed representations must be learned with a neural network. We evaluated nine models on the complementary tasks of phrase similarity and paraphrase detection. The former task simplifies the challenge of finding an adequate method of composition and places more emphasis on the representation, whereas the latter poses, in a sense, the ultimate challenge for composition models. It involves entire sentences exhibiting varied syntactic constructions and in the limit involves genuine natural language undertanding. Across both tasks our results deliver a consistent message: simple is best. Despite being in theory more expressive, the representations obtained by the neural language model and the third-order tensor cannot match the simple semantic space on the phrase similarity task. In this task syntax-oblivious composition models are superior to the more sophisticated recursive autoencoder. The latter performs better on the paraphrase detection task when its output is fed to a classifier. The simple semantic space may not take word order or sentence structure into account, but nevertheless achieves considerable semantic expressivity: it is on par with the third-order tensor without having access to as much data (3 billion words) or a syntactically parsed corpus. What do these findings tell us about the future of compositional models for distributional semantics? The problem of finding the right methods of vector composition cannot be pursued independent of the choice of lexical representation. Having tested many model combinations, we argue that in a good model of distributive semantics representation and composition must go hand in hand, i.e., they must be mutually learned. Acknowledgments We are grateful to Jeff Mitchell for his help with the re-implementation of his models. Thanks to Frank Keller and Micha Elsner for their input on earlier versions of this work and to Richard Socher for technical assistance. We acknowledge the support of EPSRC through project grant EP/I032916/1.