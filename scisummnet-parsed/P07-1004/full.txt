1 Introduction. In statistical machine translation (SMT), translation is modeled as a decision process. The goal is to find the translation t of source sentence s which maximizes the posterior probability: arg max p(t  |s) = arg max p(s  |t) · p(t) (1) This decomposition of the probability yields two different statistical models which can be trained independently of each other: the translation model p(s  |t) and the target language model p(t). State-of-the-art SMT systems are trained on large collections of text which consist of bilingual corpora (to learn the parameters of p(s  |t)), and of monolingual target language corpora (for p(t)). It has been shown that adding large amounts of target language text improves translation quality considerably. However, the availability of monolingual corpora in the source language does not help improve the system’s 25 performance. We will show how such corpora can be used to achieve higher translation quality. Even if large amounts of bilingual text are given, the training of the statistical models usually suffers from sparse data. The number of possible events, i.e. phrase pairs or pairs of subtrees in the two languages, is too big to reliably estimate a probability distribution over such pairs. Another problem is that for many language pairs the amount of available bilingual text is very limited. In this work, we will address this problem and propose a general framework to solve it. Our hypothesis is that adding information from source language text can also provide improvements. Unlike adding target language text, this hypothesis is a natural semi-supervised learning problem. To tackle this problem, we propose algorithms for transductive semi-supervised learning. By transductive, we mean that we repeatedly translate sentences from the development set or test set and use the generated translations to improve the performance of the SMT system. Note that the evaluation step is still done just once at the end of our learning process. In this paper, we show that such an approach can lead to better translations despite the fact that the development and test data are typically much smaller in size than typical training data for SMT systems. Transductive learning can be seen as a means to adapt the SMT system to a new type of text. Say a system trained on newswire is used to translate weblog texts. The proposed method adapts the trained models to the style and domain of the new input.
2 Baseline MT System. The SMT system we applied in our experiments is PORTAGE. This is a state-of-the-art phrase-based translation system which has been made available to Canadian universities for research and education iteration and added to the training data. These sepurposes. We provide a basic description here; for a lected sentence pairs are replaced in each iteration, detailed description see (Ueffing et al., 2007). and only the original bilingual training data, L, is The models (or features) which are employed by kept fixed throughout the algorithm. The process the decoder are: (a) one or several phrase table(s), of generating sentence pairs, selecting a subset of which model the translation directionp(s  |t), (b) one good sentence pairs, and updating the model is conor several n-gram language model(s) trained with tinued until a stopping condition is met. Note that the SRILM toolkit (Stolcke, 2002); in the experi- we run this algorithm in a transductive setting which ments reported here, we used 4-gram models on the means that the set of sentences U is drawn either NIST data, and a trigram model on EuroParl, (c) from a development set or the test set that will be a distortion model which assigns a penalty based used eventually to evaluate the SMT system or from on the number of source words which are skipped additional data which is relevant to the development when generating a new target phrase, and (d) a word or test set. In Algorithm 1, changing the definition penalty. These different models are combined log- of Estimate, Score and Select will give us the diflinearly. Their weights are optimized w.r.t. BLEU ferent semi-supervised learning algorithms we will score using the algorithm described in (Och, 2003). discuss in this paper. This is done on a development corpus which we will Given the probability model p(t  |s), consider the call dev1 in this paper. The search algorithm imple- distribution over all possible valid translations t for mented in the decoder is a dynamic-programming a particular input sentence s. We can initialize beam-search algorithm. this probability distribution to the uniform distribuAfter the main decoding step, rescoring with ad- tion for each sentence s in the unlabeled data U. ditional models is performed. The system generates Thus, this distribution over translations of sentences a 5,000-best list of alternative translations for each from U will have the maximum entropy. Under source sentence. These lists are rescored with the certain precise conditions, as described in (Abney, following models: (a) the different models used in 2004), we can analyze Algorithm 1 as minimizing the decoder which are described above, (b) two dif- the entropy of the distribution over translations of U. ferent features based on IBM Model 1 (Brown et al., However, this is true only when the functions Esti1993), (c) posterior probabilities for words, phrases, mate, Score and Select have very prescribed definin-grams, and sentence length (Zens and Ney, 2006; tions. In this paper, rather than analyze the converUeffing and Ney, 2007), all calculated over the N- gence of Algorithm 1 we run it for a fixed number best list and using the sentence probabilities which of iterations and instead focus on finding useful defthe baseline system assigns to the translation hy- initions for Estimate, Score and Select that can be potheses. The weights of these additional models experimentally shown to improve MT performance. and of the decoder models are again optimized to 3.2 The Estimate Function maximize BLEU score. This is performed on a sec- We consider the following different definitions for ond development corpus, dev2. Estimate in Algorithm 1: 3 The Framework Full Re-training (of all translation models): If 3.1 The Algorithm Estimate(L, T) estimates the model parameters Our transductive learning algorithm, Algorithm 1, based on L U T, then we have a semi-supervised alis inspired by the Yarowsky algorithm (Yarowsky, gorithm that re-trains a model on the original train1995; Abney, 2004). The algorithm works as fol- ing data L plus the sentences decoded in the last itlows: First, the translation model is estimated based eration. The size of L can be controlled by filtering on the sentence pairs in the bilingual training data L. the training data (see Section 3.5). Then, a set of source language sentences, U, is trans- Additional Phrase Table: If, on the other hand, a lated based on the current model. A subset of good new phrase translation table is learned on T only translations and their sources, Ti, is selected in each and then added as a new component in the log-linear 26 model, we have an alternative to the full re-training Algorithm 1 Transductive learning algorithm for statistical machine translation of the model on labeled and unlabeled data which can be very expensive if L is very large (as on the Chinese–English data set). This additional phrase table is small and specific to the development or test set it is trained on. It overlaps with the original phrase tables, but also contains many new phrase pairs (Ueffing, 2006). Mixture Model: Another alternative for Estimate is to create a mixture model of the phrase table probabilities with new phrase table probabilities where Lp and Tp are phrase table probabilities estimated on L and T, respectively. In cases where new phrase pairs are learned from T, they get added into the merged phrase table. In Algorithm 1, the Score function assigns a score to each translation hypothesis t. We used the following scoring functions in our experiments: which we implemented follows the approaches suggested in (Blatz et al., 2003; Ueffing and Ney, 2007): The confidence score of a target sentence t is calculated as a log-linear combination of phrase posterior probabilities, Levenshtein-based word posterior probabilities, and a target language model score. The weights of the different scores are optimized w.r.t. classification error rate (CER). The phrase posterior probabilities are determined by summing the sentence probabilities of all translation hypotheses in the N-best list which contain this phrase pair. The segmentation of the sentence into phrases is provided by the decoder. This sum is then normalized by the total probability mass of the N-best list. To obtain a score for the whole target sentence, the posterior probabilities of all target phrases are multiplied. The word posterior probabilities are calculated on basis of the Levenshtein alignment between the hypothesis under consideration and all other translations contained in the Nbest list. For details, see (Ueffing and Ney, 2007). Again, the single values are multiplied to obtain a score for the whole sentence. For NIST, the language model score is determined using a 5-gram model trained on the English Gigaword corpus, and on French–English, we use the trigram model which was provided for the NAACL 2006 shared task. The Select function in Algorithm 1 is used to create the additional training data Ti which will be used in the next iteration i + 1 by Estimate to augment the corpus use sentences original bilingual training data. We use the following selection functions: Importance Sampling: For each sentence s in the set of unlabeled sentences U, the Labeling step in Algorithm 1 generates an N-best list of translations, and the subsequent Scoring step assigns a score for each translation t in this list. The set of generated translations for all sentences in U is the event space and the scores are used to put a probability distribution over this space, simply by renormalizing the scores described in Section 3.3. We use importance sampling to select K translations from this distribution. Sampling is done with replacement which means that the same translation may be chosen several times. These K sampled translations and their associated source sentences make up the additional training data Ti. Selection using a Threshold: This method compares the score of each single-best translation to a threshold. The translation is considered reliable and added to the set Ti if its score exceeds the threshold. Else it is discarded and not used in the additional training data. The threshold is optimized on the development beforehand. Since the scores of the translations change in each iteration, the size of Ti also changes. Keep All: This method does not perform any filtering at all. It is simply assumed that all translations in the set Xi are reliable, and none of them are discarded. Thus, in each iteration, the result of the selection step will be Ti = Xi. This method was implemented mainly for comparison with other selection methods. 3.5 Filtering the Training Data In general, having more training data improves the quality of the trained models. However, when it comes to the translation of a particular test set, the question is whether all of the available training data are relevant to the translation task or not. Moreover, working with large amounts of training data requires more computational power. So if we can identify a subset of training data which are relevant to the current task and use only this to re-train the models, we can reduce computational complexity significantly. We propose to Filter the training data, either bilingual or monolingual text, to identify the parts 2006 NIST evaluation (see Table 2). We used the LDC segmenter for Chinese. The multiple translation corpora multi-p3 and multi-p4 were used as development corpora. Evaluation was performed on the 2004 and 2006 test sets. Note that the training data consists mainly of written text, whereas the test sets comprise three and four different genres: editorials, newswire and political speeches in the 2004 test set, and broadcast conversations, broadcast news, newsgroups and newswire in the 2006 test set. Most of these domains have characteristics which are different from those of the training data, e.g., broadcast conversations have characteristics of spontaneous speech, and the newsgroup data is comparatively unstructured. Given the particular data sets described above, Table 3 shows the various options for the Estimate, Score and Select functions (see Section 3). The table provides a quick guide to the experiments we present in this paper vs. those we did not attempt due to computational infeasibility. We ran experiments corresponding to all entries marked with ∗ (see Section 4.2). For those marked ∗∗ the experiments produced only minimal improvement over the baseline and so we do not discuss them in this paper. The entries marked as † were not attempted because they are not feasible (e.g. full re-training on the NIST data). However, these were run on the smaller EuroParl corpus. We evaluated the generated translations using three different evaluation metrics: BLEU score (Papineni et al., 2002), mWER (multi-reference word error rate), and mPER (multi-reference positionindependent word error rate) (Nießen et al., 2000). Note that BLEU score measures quality, whereas mWER and mPER measure translation errors. We will present 95%-confidence intervals for the baseline system which are calculated using bootstrap resampling. The metrics are calculated w.r.t. one and four English references: the EuroParl data comes with one reference, the NIST 2004 evaluation set and the NIST section of the 2006 evaluation set are provided with four references each, whereas the GALE section of the 2006 evaluation set comes with one reference only. This results in much lower BLEU scores and higher error rates for the translations of the GALE set (see Section 4.2). Note that these values do not indicate lower translation quality, but are simply a result of using only one reference.
4.2 Results EuroParl. We ran our initial experiments on EuroParl to explore the behavior of the transductive learning algorithm. In all experiments reported in this subsection, the test set was used as unlabeled data. The selection and scoring was carried out using importance sampling with normalized scores. In one set of experiments, we used the 100K and 150K training sentences filtered according to n-gram coverage over the test set. We fully re-trained the phrase tables on these data and 8,000 test sentence pairs sampled from 20-best lists in each iteration. The results on the test set can be seen in Figure 1. The BLEU score increases, although with slight variation, over the iterations. In total, it increases from 24.1 to 24.4 for the 100K filtered corpus, and from 24.5 to 24.8 for 150K, respectively. Moreover, we see that the BLEU score of the system using 100K training sentence pairs and transductive learning is the same as that of the one trained on 150K sentence pairs. So the information extracted from untranslated test sentences is equivalent to having an additional 50K sentence pairs. In a second set of experiments, we used the whole EuroParl corpus and the sampled sentences for fully re-training the phrase tables in each iteration. We ran the algorithm for three iterations and the BLEU score increased from 25.3 to 25.6. Even though this is a small increase, it shows that the unlabeled data contains some information which can be explored in transductive learning. In a third experiment, we applied the mixture model idea as explained in Section 3.2. The initially learned phrase table was merged with the learned phrase table in each iteration with a weight of A = 0.1. This value for A was found based on cross validation on a development set. We ran the algorithm for 20 iterations and BLEU score increased from 25.3 to 25.7. Since this is very similar to the result obtained with the previous method, but with an additional parameter A to optimize, we did not use mixture models on NIST. Note that the single improvements achieved here are slightly below the 95%-significance level. However, we observe them consistently in all settings.
NIST. Table 4 presents translation results on NIST with different versions of the scoring and selection methods introduced in Section 3. In these experiments, the unlabeled data U for Algorithm 1 is the development or test corpus. For this corpus U, 5,000-best lists were generated using the baseline SMT system. Since re-training the full phrase tables is not feasible here, a (small) additional phrase table, specific to U, was trained and plugged into the SMT system as an additional model. The decoder weights thus had to be optimized again to determine the appropriate weight for this new phrase table. This was done on the dev1 corpus, using the phrase table specific to dev1. Every time a new corpus is to be translated, an adapted phrase table is created using transductive learning and used with the weight which has been learned on dev1. In the first experiment presented in Table 4, all of the generated 1-best translations were kept and used for training the adapted phrase tables. This method yields slightly higher translation quality than the baseline system. The second approach we studied is the use of importance sampling (IS) over 20-best lists, based either on lengthnormalized sentence scores (norm.) or confidence scores (conf.). As the results in Table 4 show, both variants outperform the first method, with a consistent improvement over the baseline across all test corpora and evaluation metrics. The third method uses a threshold-based selection method. Combined with confidence estimation as scoring method, this yields the best results. All improvements over the baseline are significant at the 95%-level. Table 5 shows the translation quality achieved on the NIST test sets when additional source language data from the Chinese Gigaword corpus comprising newswire text is used for transductive learning. These Chinese sentences were sorted according to their n-gram overlap (see Section 3.5) with the development corpus, and the top 5,000 Chinese sentences were used. The selection and scoring in Algorithm 1 were performed using confidence estimation with a threshold. Again, a new phrase table was trained on these data. As can be seen in Table 5, this system outperforms the baseline system on all test corpora. The error rates are significantly reduced in all three settings, and BLEU score increases in all cases. A comparison with Table 4 shows that transductive learning on the development set and test corpora, adapting the system to their domain and style, is more effective in improving the SMT system than the use of additional source language data. In all experiments on NIST, Algorithm 1 was run for one iteration. We also investigated the use of an iterative procedure here, but this did not yield any improvement in translation quality.
5 Previous Work. Semi-supervised learning has been previously applied to improve word alignments. In (CallisonBurch et al., 2004), a generative model for word alignment is trained using unsupervised learning on parallel text. In addition, another model is trained on a small amount of hand-annotated word alignment data. A mixture model provides a probability for phrase table trained on monolingual Chinese news data. Selection step using threshold on confidence scores. NIST Chinese–English. word alignment. Experiments showed that putting a large weight on the model trained on labeled data performs best. Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. The word alignments are used to train a standard phrasebased SMT system, resulting in increased translation quality . In (Callison-Burch, 2002) co-training is applied to MT. This approach requires several source languages which are sentence-aligned with each other and all translate into the same target language. One language pair creates data for another language pair and can be naturally used in a (Blum and Mitchell, 1998)-style co-training algorithm. Experiments on the EuroParl corpus show a decrease in WER. However, the selection algorithm applied there is actually supervised because it takes the reference translation into account. Moreover, when the algorithm is run long enough, large amounts of co-trained data injected too much noise and performance degraded. Self-training for SMT was proposed in (Ueffing, 2006). An existing SMT system is used to translate the development or test corpus. Among the generated machine translations, the reliable ones are automatically identified using thresholding on confidence scores. The work which we presented here differs from (Ueffing, 2006) as follows: fidence estimation used there, we applied importance sampling and combined it with confidence estimation for transductive learning.
6 Discussion. It is not intuitively clear why the SMT system can learn something from its own output and is improved through semi-supervised learning. There are two main reasons for this improvement: Firstly, the selection step provides important feedback for the system. The confidence estimation, for example, discards translations with low language model scores or posterior probabilities. The selection step discards bad machine translations and reinforces phrases of high quality. As a result, the probabilities of lowquality phrase pairs, such as noise in the table or overly confident singletons, degrade. Our experiments comparing the various settings for transductive learning shows that selection clearly outperforms the method which keeps all generated translations as additional training data. The selection methods investigated here have been shown to be wellsuited to boost the performance of semi-supervised learning for SMT. Secondly, our algorithm constitutes a way of adapting the SMT system to a new domain or style without requiring bilingual training or development data. Those phrases in the existing phrase tables which are relevant for translating the new data are reinforced. The probability distribution over the phrase pairs thus gets more focused on the (reliable) parts which are relevant for the test data. For an analysis of the self-trained phrase tables, examples of translated sentences, and the phrases used in translation, see (Ueffing, 2006).