1 Introduction. In this paper we investigate a new problem of automatically identifying the perspective from which a document is written. By perspective we mean a “subjective evaluation of relative significance, a point-of-view.”1 For example, documents about the Palestinian-Israeli conflict may appear to be about the same topic but reveal different perspectives: (1) The inadvertent killing by Israeli forces of Palestinian civilians – usually in the course of shooting at Palestinian terrorists – is considered no different at the moral and ethical level than the deliberate targeting of Israeli civilians by Palestinian suicide bombers. (2) In the first weeks of the Intifada, for example, Palestinian public protests and civilian demonstrations were answered brutally by Israel, which killed tens of unarmed protesters. Example 1 is written from an Israeli perspective; Example 2 is written from a Palestinian perspective. Anyone knowledgeable about the issues of the Israeli-Palestinian conflict can easily identify the perspectives from which the above examples were written. However, can computers learn to identify the perspective of a document given a training corpus? When an issue is discussed from different perspectives, not every sentence strongly reflects the perspective of the author. For example, the following sentences were written by a Palestinian and an Israeli. Examples 3 and 4 both factually introduce the background of the issue of the “green line” without expressing explicit perspectives. Can we develop a system to automatically discriminate between sentences that strongly indicate a perspective and sentences that only reflect shared background information? A system that can automatically identify the perspective from which a document is written will be a valuable tool for people analyzing huge collections of documents from different perspectives. Political analysts regularly monitor the positions that countries take on international and domestic issues. Media analysts frequently survey broadcast news, newspapers, and weblogs for differing viewpoints. Without the assistance of computers, analysts have no choice but to read each document in order to identify those from a perspective of interest, which is extremely time-consuming. What these analysts need is to find strong statements from different perspectives and to ignore statements that reflect little or no perspective. In this paper we approach the problem of learning individual perspectives in a statistical framework. We develop statistical models to learn how perspectives are reflected in word usage, and we treat the problem of identifying perspectives as a classification task. Although our corpus contains documentlevel perspective annotations, it lacks sentence-level annotations, creating a challenge for learning the perspective of sentences. We propose a novel statistical model to overcome this problem. The experimental results show that the proposed statistical models can successfully identify the perspective from which a document is written with high accuracy.
2 Related Work. Identifying the perspective from which a document is written is a subtask in the growing area of automatic opinion recognition and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et al., 2002; Turney and Littman, 2003; Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al., 2004; Hu and Liu, 2004)) is perhaps the most similar to our work. As with review classification, we treat perspective identification as a document-level classification task, discriminating, in a sense, between different types of opinions. However, there is a key difference. A positive or negative opinion toward a particular movie or product is fundamentally different from an overall perspective. One’s opinion will change from movie to movie, whereas one’s perspective can be seen as more static, often underpinned by one’s ideology or beliefs about the world. There has been research in discourse analysis that examines how different perspectives are expressed in political discourse (van Dijk, 1988; Pan et al., 1999; Geis, 1987). Although their research may have some similar goals, they do not take a computational approach to analyzing large collections of documents. To the best of our knowledge, our approach to automatically identifying perspectives in discourse is unique.
3 Corpus. Our corpus consists of articles published on the bitterlemons website2. The website is set up to “contribute to mutual understanding [between Palestinians and Israelis] through the open exchange of ideas.”3 Every week an issue about the IsraeliPalestinian conflict is selected for discussion (e.g., “Disengagement: unilateral or coordinated?”), and a Palestinian editor and an Israeli editor each contribute one article addressing the issue. In addition, the Israeli and Palestinian editors invite one Israeli and one Palestinian to express their views on the issue (sometimes in the form of an interview), resulting in a total of four articles in a weekly edition. We choose the bitterlemons website for two reasons. First, each article is already labeled as either Palestinian or Israeli by the editors, allowing us to exploit existing annotations. Second, the bitterlemons corpus enables us to test the generalizability of the proposed models in a very realistic setting: training on articles written by a small number of writers (two editors) and testing on articles from a much larger group of writers (more than 200 different guests). We collected a total of 594 articles published on the website from late 2001 to early 2005. The distribution of documents and sentences are listed in cluding edition numbers, publication dates, topics, titles, author names and biographic information. We used OpenNLP Tools4 to automatically extract sentence boundaries, and reduced word variants using the Porter stemming algorithm. We evaluated the subjectivity of each sentence using the automatic subjective sentence classifier from (Riloff and Wiebe, 2003), and find that 65.6% of Palestinian sentences and 66.2% of Israeli sentences are classified as subjective. The high but almost equivalent percentages of subjective sentences in the two perspectives support our observation in Section 2 that a perspective is largely expressed using subjective language, but that the amount of subjectivity in a document is not necessarily indicative of its perspective.
4 Statistical Modeling of Perspectives. We develop algorithms for learning perspectives using a statistical framework. Denote a training corpus as a set of documents Wn and their perspectives labels Dn, n = 1, ... , N, where N is the total number of documents in the corpus. Given a new document W with a unknown document perspective, the perspective D� is calculated based on the following conditional probability. We are also interested in how strongly each sentence in a document conveys perspective information. Denote the intensity of the m-th sentence of the n-th document as a binary random variable Sm,n. To evaluate Sm,n, how strongly a sentence reflects a particular perspective, we calculate the following conditional probability. We model the process of generating documents from a particular perspective as follows: First, the parameters π and θ are sampled once from prior distributions for the whole corpus. Beta and Dirichlet are chosen because they are conjugate priors for binomial and multinomial distributions, respectively. We set the hyperparameters απ, βπ, and αθ to one, resulting in non-informative priors. A document perspective Dn is then sampled from a binomial distribution with the parameter π. The value of Dn is either d0 (Israeli) or d1 (Palestinian). Words in the document are then sampled from a multinomial distribution, where Ln is the length of the document. A graphical representation of the model is shown in Figure 1. The model described above is commonly known as a naive Bayes (NB) model. NB models have been widely used for various classification tasks, including text categorization (Lewis, 1998). The NB model is also a building block for the model described later that incorporates sentence-level perspective information. To predict the perspective of an unseen document using naive Bayes , we calculate the posterior distribution of D in (5) by integrating out the parameters, However, the above integral is difficult to compute. As an alternative, we use Markov Chain Monte Carlo (MCMC) methods to obtain samples from the posterior distribution. Details about MCMC methods can be found in Appendix A. We introduce a new binary random variable, S, to model how strongly a perspective is reflected at the sentence level. The value of S is either s1 or s0, where s1 indicates a sentence is written strongly from a perspective while s0 indicates it is not. The whole generative process is modeled as follows: The parameters π and θ have the same semantics as in the naive Bayes model. S is naturally modeled as a binomial variable, where τ is the parameter of S. S represents how likely it is that a sentence strongly conveys a perspective. We call this model the Latent Sentence Perspective Model (LSPM) because S is not directly observed. The graphical model representation of LSPM is shown in Figure 2. To use LSPM to identify the perspective of a new document D with unknown sentence perspectives S, we calculate posterior probabilities by summing out possible combinations of sentence perspective in the document and parameters. As before, we resort to MCMC methods to sample from the posterior distributions, given in Equations (5) and (6). As is often encountered in mixture models, there is an identifiability issue in LSPM. Because the values of S can be permuted without changing the likelihood function, the meanings of s0 and s1 are ambiguous. In Figure 3a, four θ values are used to represent the four possible combinations of document perspective d and sentence perspective intensity s. If we do not impose any constraints, s1 and s0 are exchangeable, and we can no longer strictly interpret s1 as indicating a strong sentence-level perspective and s0 as indicating that a sentence carries little or no perspective information. The other problem of this parameterization is that any improvement from LSPM over the naive Bayes model is not necessarily due to the explicit modeling of sentence-level perspective. 5 may capture aspects of the document collection that we never intended to model. For example, s0 may capture the editors’ writing styles and s1 the guests’ writing styles in the bitterlemons corpus. We solve the identifiability problem by forcing Bd1,s0 and Bd0,s0 to be identical and reducing the number of B parameters to three. As shown in Figure 3b, there are separate B parameters conditioned on the document perspective (left branch of the tree, d0 is Israeli and d1 is Palestinian), but there is single B parameter when 5 = s0 shared by both documentlevel perspectives (right branch of the tree). We assume that the sentences with little or no perspective information, i.e., 5 = s0, are generated independently of the perspective of a document. In other words, sentences that are presenting common background information or introducing an issue and that do not strongly convey any perspective should look similar whether they are in Palestinian or Israeli documents. By forcing this constraint, we become more confident that s0 represents sentences of little perspectives and s1 represents sentences of strong perspectives from d1 and d0 documents.
5 Experiments. We evaluate three different models for the task of identifying perspective at the document level: two naive Bayes models (NB) with different inference methods and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000). NB-B uses full Bayesian inference and NB-M uses Maximum a posteriori (MAP). We compare NB with SVM not only because SVM has been very effective for classifying topical documents (Joachims, 1998), but also to contrast generative models like NB with discriminative models like SVM. For training SVM, we represent each document as a V-dimensional feature vector, where V is the vocabulary size and each coordinate is the normalized term frequency within the document. We use a linear kernel for SVM and search for the best parameters using grid methods. To evaluate the statistical models, we train them on the documents in the bitterlemons corpus and calculate how accurately each model predicts document perspective in ten-fold cross-validation experiments. Table 2 reports the average classification accuracy across the the 10 folds for each model. The accuracy of a baseline classifier, which randomly assigns the perspective of a document as Palestinian or Israeli, is 0.5, because there are equivalent numbers of documents from the two perspectives. The last column of Table 2 is error reduction relative to SVM. The results show that the naive Bayes models and SVM perform surprisingly well on both the Editors and Guests subsets of the bitterlemons corpus. The naive Bayes models perform slightly better than SVM, possibly because generative models (i.e., naive Bayes models) achieve optimal performance with a smaller number of training examples than discriminative models (i.e., SVM) (Ng and Jordan, 2002), and the size of the bitterlemons corpus is indeed small. NB-B, which performs full Bayesian inference, improves on NB-M, which only performs point estimation. The results suggest that the choice of words made by the authors, either consciously or subconsciously, reflects much of their political perspectives. Statistical models can capture word usage well and can identify the perspective of documents with high accuracy. Given the performance gap between Editors and Guests, one may argue that there exist distinct editing artifacts or writing styles of the editors and guests, and that the statistical models are capturing these things rather than “perspectives.” To test if the statistical models truly are learning perspectives, we conduct experiments in which the training and testing data are mismatched, i.e., from different subsets of the corpus. If what the SVM and naive Bayes models learn are writing styles or editing artifacts, the classification performance under the mismatched conditions will be considerably degraded. The results on the mismatched training and testing experiments are shown in Table 3. Both SVM and the two variants of naive Bayes perform well on the different combinations of training and testing data. As in Table 2, the naive Bayes models perform better than SVM with larger error reductions, and NB-B slightly outperforms NB-M. The high accuracy on the mismatched experiments suggests that statistical models are not learning writing styles or editing artifacts. This reaffirms that document perspective is reflected in the words that are chosen by the writers. We list the most frequent words (excluding stopwords) learned by the the NB-M model in Table 4. The frequent words overlap greatly between the Palestinian and Israeli perspectives, including “state,” “peace,” “process,” “secure” (“security”), and “govern” (“government”). This is in contrast to what we expect from topical text classification (e.g., “Sports” vs. “Politics”), in which frequent words seldom overlap. Authors from different perspectives often choose words from a similar vocabulary but emphasize them differently. For example, in documents that are written from the Palestinian perspective, the word “palestinian” is mentioned more frequently than the word “israel.” It is, however, the reverse for documents that are written from the Israeli perspective. Perspectives are also expressed in how frequently certain people (“sharon” v.s. “arafat”), countries (“international” v.s. “america”), and actions (“occupation” v.s. “settle”) are mentioned. While one might solicit these contrasting word pairs from domain experts, our results show that statistical models such as SVM and naive Bayes can automatically acquire them. In addition to identifying the perspective of a document, we are interested in knowing which sentences of the document strongly conveys perspective information. Sentence-level perspective annotations do not exist in the bitterlemons corpus, which makes estimating parameters for the proposed Latent Sentence Perspective Model (LSPM) difficult. The posterior probability that a sentence strongly covey a perspective (Example (6)) is of the most interest, but we can not directly evaluate this model without gold standard annotations. As an alternative, we evaluate how accurately LSPM predicts the perspective of a document, again using 10-fold cross validation. Although LSPM predicts the perspective of both documents and sentences, we will doubt the quality of the sentence-level predictions if the document-level predictions are incorrect. The experimental results are shown in Table 5. We include the results for the naive Bayes models from Table 3 for easy comparison. The accuracy of LSPM is comparable or even slightly better than that of the naive Bayes models. This is very encouraging and suggests that the proposed LSPM closely captures how perspectives are reflected at both the document and sentence levels. Examples 1 and 2 from the introduction were predicted by LSPM as likely to contain strong perspectives, i.e., large Pr(S� = s1). Examples 3 and 4 from the introduction were predicted by LSPM as likely to contain little or no perspective information, i.e., high Pr( S� = s0). The comparable performance between the naive Bayes models and LSPM is in fact surprising. We can train a naive Bayes model directly on the sentences and attempt to classify a sentence as reflecting either a Palestinian or Israeli perspective. A sentence is correctly classified if the predicted perspective for the sentence is the same as the perspective of the document from which it was extracted. Using this model, we obtain a classification accuracy of only 0.7529, which is much lower than the accuracy previously achieved at the document level. Identifying perspectives at the sentence level is thus more difficult than identifying perspectives at the document level. The high accuracy at the document level shows that LSPM is very effective in pooling evidence from sentences that individually contain little perspective information.
6 Conclusions. In this paper we study a new problem of learning to identify the perspective from which a text is written at the document and sentence levels. We show that much of a document’s perspective is expressed in word usage, and statistical learning algorithms such as SVM and naive Bayes models can successfully uncover the word patterns that reflect author perspective with high accuracy. In addition, we develop a novel statistical model to estimate how strongly a sentence conveys perspective, in the absence of sentence-level annotations. By introducing latent variables and sharing parameters, the Latent Sentence Perspective Model is shown to capture well how perspectives are reflected at the document and sentence levels. The small but positive improvement due to sentence-level modeling in LSPM is encouraging. In the future, we plan to investigate how consistently LSPM sentence-level predictions are with human annotations.
Acknowledgment. This material is based on work supported by the Advanced Research and Development Activity (ARDA) under contract number NBCHC040037. A Gibbs Samplers Based the model specification described in Section 4.2 we derive the Gibbs samplers (Chen et al., 2000) for the Latent Sentence Perspective Model as follows, where dbinom and dmultinom are the density functions of binomial and multinomial distributions, respectively. The superscript t indicates that a sample is from the t-th iteration. We run three chains and collect 5000 samples. The first half of burn-in samples are discarded.