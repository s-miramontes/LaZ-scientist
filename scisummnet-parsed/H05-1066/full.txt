Introduction. Dependency parsing has seen a surge of interest lately for applications such as relation extraction (Culotta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym genera tion (Shinyama et al, 2002), and lexical resource augmentation (Snow et al, 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse whilestill encoding much of the predicate-argument infor mation needed in applications. root John hit the ball with the bat Figure 1: An example dependency tree. Dependency representations, which link words to their arguments, have a long history (Hudson, 1984). Figure 1 shows a dependency tree for the sentence John hit the ball with the bat. We restrict ourselvesto dependency tree analyses, in which each word de pends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, theedges can be drawn above the words without cross ings, or, equivalently, a word and its descendants form a contiguous substring of the sentence.In English, projective trees are sufficient to ana lyze most sentence types. In fact, the largest sourceof English dependency trees is automatically gener ated from the Penn Treebank (Marcus et al, 1993)and is by convention exclusively projective. However, there are certain examples in which a non projective tree is preferable. Consider the sentenceJohn saw a dog yesterday which was a Yorkshire Ter rier. Here the relative clause which was a YorkshireTerrier and the object it modifies (the dog) are sep arated by an adverb. There is no way to draw the dependency tree for this sentence in the plane withno crossing edges, as illustrated in Figure 2. In lan guages with more flexible word order than English, such as German, Dutch and Czech, non-projective dependencies are more frequent. Rich inflection systems reduce reliance on word order to express 523 root John saw a dog yesterday which was a Yorkshire Terrier root O to nove? ve?ts?inou nema? ani za?jem a taky na to ve?ts?inou nema? pen??ze He is mostly not even interested in the new things and in most cases, he has no money for it either. Figure 2: Non-projective dependency trees in English and Czech.grammatical relations, allowing non-projective dependencies that we need to represent and parse ef ficiently. A non-projective example from the Czech Prague Dependency Treebank (Hajic? et al, 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projectiveparser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduc tion of non-projective edges into dependency trees through learned edge transformations within their memory-based parser. They test this system onCzech and show improved accuracy relative to a projective parser. Our approach differs from those ear lier efforts in searching optimally and efficiently the full space of non-projective trees. The main idea of our method is that dependencyparsing can be formalized as the search for a maximum spanning tree in a directed graph. This formalization generalizes standard projective parsing mod els based on the Eisner algorithm (Eisner, 1996) toyield efficient O(n2) exact parsing methods for nonprojective languages like Czech. Using this spanning tree representation, we extend the work of McDonald et al (2005) on online large-margin discriminative training methods to non-projective depen dencies. The present work is related to that of Hirakawa(2001) who, like us, reduces the problem of depen dency parsing to spanning tree search. However, his parsing method uses a branch and bound algorithm that is exponential in the worst case, even thoughit appears to perform reasonably in limited experi ments. Furthermore, his work does not adequately address learning or measure parsing accuracy on held-out data. Section 2 describes an edge-based factorizationof dependency trees and uses it to equate depen dency parsing to the problem of finding maximumspanning trees in directed graphs. Section 3 out lines the online large-margin learning framework used to train our dependency parsers. Finally, in Section 4 we present parsing results for Czech. The trees in Figure 1 and Figure 2 are untyped, that is, edges are not partitioned into types representingadditional syntactic information such as grammati cal function. We study untyped dependency treesmainly, but edge types can be added with simple ex tensions to the methods discussed here.
Dependency Parsing and Spanning Trees. . 2.1 Edge Based Factorization. In what follows, x = x1 ? ? xn represents a genericinput sentence, and y represents a generic depen dency tree for sentence x. Seeing y as the set of tree edges, we write (i, j) ? y if there is a dependency in y from word xi to word xj .In this paper we follow a common method of fac toring the score of a dependency tree as the sum of the scores of all edges in the tree. In particular, wedefine the score of an edge to be the dot product be 524 tween a high dimensional feature representation of the edge and a weight vector, s(i, j) = w ? f(i, j) Thus the score of a dependency tree y for sentence x is, s(x,y) = ? (i,j)?y s(i, j) = ? (i,j)?y w ? f(i, j) Assuming an appropriate feature representation as well as a weight vector w, dependency parsing is the task of finding the dependency tree y with highest score for a given sentence x. For the rest of this section we assume that the weight vector w is known and thus we know the score s(i, j) of each possible edge. In Section 3 we present a method for learning the weight vector. 2.2 Maximum Spanning Trees. We represent the generic directed graph G = (V,E) by its vertex set V = {v1, . . . , vn} and set E ? [1 : n]? [1 : n] of pairs (i, j) of directed edges vi ? vj .Each such edge has a score s(i, j). Since G is di rected, s(i, j) does not necessarily equal s(j, i). A maximum spanning tree (MST) of G is a tree y ? E that maximizes the value ? (i,j)?y s(i, j) such thatevery vertex in V appears in y. The maximum pro jective spanning tree of G is constructed similarlyexcept that it can only contain projective edges rel ative to some total order on the vertices of G. The MST problem for directed graphs is also known as the maximum arborescence problem. For each sentence x we define the directed graph Gx = (Vx, Ex) given by Vx = {x0 = root, x1, . . . , xn} Ex = {(i, j) : i 6= j, (i, j) ? [0 : n] ? [1 : n]} That is, Gx is a graph with the sentence words and the dummy root symbol as vertices and a directed edge between every pair of distinct words and fromthe root symbol to every word. It is clear that dependency trees for x and spanning trees for Gx co incide, since both kinds of trees are required to be rooted at the dummy root and reach all the wordsin the sentence. Hence, finding a (projective) depen dency tree with highest score is equivalent to finding a maximum (projective) spanning tree in Gx. Chu-Liu-Edmonds(G, s) Graph G = (V, E) Edge weight function s : E ? R 1. Let M = {(x?, x) : x ? V, x? = arg maxx? s(x?, x)}. 2. Let GM = (V, M). 4. Otherwise, find a cycle C in GM. 5. Let GC = contract(G, C, s). 6. Let y = Chu-Liu-Edmonds(GC , s). 7. Find a vertex x ? C s. t. (x?, x) ? y, (x??, x) ? C. 8. return y ? C ? {(x??, x)} contract(G = (V, E), C, s) 1. Let GC be the subgraph of G excluding nodes in C. 2. Add a node c to GC representing cycle C. Add edge (c, x) to GC with s(c, x) = maxx??C s(x?, x) 4. For x ? V ? C : ?x??C(x, x?) E. Add edge (x, c) to GC with s(x, c) = maxx??C [s(x, x?) s(a(x?), x?) + s(C)] where a(v) is the predecessor of v in C and s(C) = Pv?C s(a(v), v) 5. return GC Figure 3: Chu-Liu-Edmonds algorithm for finding maximum spanning trees in directed graphs. 2.2.1 Non-projective Trees To find the highest scoring non-projective tree we simply search the entire space of spanning trees with no restrictions. Well-known algorithms exist for theless general case of finding spanning trees in undi rected graphs (Cormen et al, 1990). Efficient algorithms for the directed case are less well known, but they exist. We will use here the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;Edmonds, 1967), sketched in Figure 3 follow ing Leonidas (2003). Informally, the algorithm has each vertex in the graph greedily select the incoming edge with highest weight. If a tree results, it must be the maximum spanning tree. If not, there must be a cycle. The procedure identifies a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle. It can be shown that a maximum spanning tree on the contracted graph isequivalent to a maximum spanning tree in the orig inal graph (Leonidas, 2003). Hence the algorithm can recursively call itself on the new graph. Naively,this algorithm runs in O(n3) time since each recur sive call takes O(n2) to find the highest incoming edge for each word and to contract the graph. There are at most O(n) recursive calls since we cannot contract the graph more then n times. However, 525 Tarjan (1977) gives an efficient implementation of the algorithm with O(n2) time complexity for dense graphs, which is what we need here. To find the highest scoring non-projective tree for a sentence, x, we simply construct the graph Gx and run it through the Chu-Liu-Edmonds algorithm. The resulting spanning tree is the best non-projective dependency tree. We illustrate here the application of the Chu-Liu-Edmonds algorithm to dependency parsing on the simple example x = John saw Mary, with directed graph representation Gx, root saw John Mary 10 9 9 30 3020 3 0 11 The first step of the algorithm is to find, for each word, the highest scoring incoming edge root saw John Mary30 3020 If the result were a tree, it would have to be the maximum spanning tree. However, in this case we have a cycle, so we will contract it into a single node and recalculate edge weights according to Figure 3. root saw John Mary 40 9 30 31 wjs The new vertex wjs represents the contraction of vertices John and saw. The edge from wjs to Mary is 30 since that is the highest scoring edge from any vertex in wjs. The edge from root into wjs is set to40 since this represents the score of the best span ning tree originating from root and including only the vertices in wjs. The same leads to the edge from Mary to wjs. The fundamental property of the Chu-Liu-Edmonds algorithm is that an MST in thisgraph can be transformed into an MST in the orig inal graph (Leonidas, 2003). Thus, we recursively call the algorithm on this graph. Note that we need to keep track of the real endpoints of the edges into and out of wjs for reconstruction later. Running the algorithm, we must find the best incoming edge to all words root saw John Mary 40 30 wjs This is a tree and thus the MST of this graph. We now need to go up a level and reconstruct the graph. The edge from wjs to Mary originally was from the word saw, so we include that edge. Furthermore, the edge from root to wjs represented a tree from root to saw to John, so we include all those edges to get the final (and correct) MST, root saw John Mary 10 3030 A possible concern with searching the entire spaceof spanning trees is that we have not used any syntactic constraints to guide the search. Many lan guages that allow non-projectivity are still primarily projective. By searching all possible non-projective trees, we run the risk of finding extremely bad trees. We address this concern in Section 4. 2.2.2 Projective TreesIt is well known that projective dependency pars ing using edge based factorization can be handledwith the Eisner algorithm (Eisner, 1996). This algorithm has a runtime of O(n3) and has been employed successfully in both generative and discrimi native parsing models (Eisner, 1996; McDonald et al., 2005). Furthermore, it is trivial to show that the Eisner algorithm solves the maximum projective spanning tree problem. The Eisner algorithm differs significantly from the Chu-Liu-Edmonds algorithm. First of all, it is abottom-up dynamic programming algorithm as opposed to a greedy recursive one. A bottom-up al gorithm is necessary for the projective case since it must maintain the nested structural constraint, which is unnecessary for the non-projective case. 2.3 Dependency Trees as MSTs: Summary. In the preceding discussion, we have shown that nat ural language dependency parsing can be reduced to finding maximum spanning trees in directed graphs.This reduction results from edge-based factoriza tion and can be applied to projective languages with 526the Eisner parsing algorithm and non-projective languages with the Chu-Liu-Edmonds maximum span ning tree algorithm. The only remaining problem is how to learn the weight vector w. A major advantage of our approach over other dependency parsing models is its uniformity and simplicity. By viewing dependency structures asspanning trees, we have provided a general framework for parsing trees for both projective and non projective languages. Furthermore, the resultingparsing algorithms are more efficient than lexi calized phrase structure approaches to dependencyparsing, allowing us to search the entire space with out any pruning. In particular the non-projective parsing algorithm based on the Chu-Liu-EdmondsMST algorithm provides true non-projective parsing. This is in contrast to other non-projective meth ods, such as that of Nivre and Nilsson (2005), who implement non-projectivity in a pseudo-projective parser with edge transformations. This formulation also dispels the notion that non-projective parsing is?harder? than projective parsing. In fact, it is easier since non-projective parsing does not need to en force the non-crossing constraint of projective trees. As a result, non-projective parsing complexity is justO(n2), against the O(n3) complexity of the Eisner dynamic programming algorithm, which by con struction enforces the non-crossing constraint.
Online Large Margin Learning. . In this section, we review the work of McDonald etal. (2005) for online large-margin dependency pars ing. As usual for supervised learning, we assume a training set T = {(xt,yt)}Tt=1, consisting of pairs of a sentence xt and its correct dependency tree yt. In what follows, dt(x) denotes the set of possible dependency trees for sentence x. The basic idea is to extend the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer,2003; Crammer et al, 2003) to learning with struc tured outputs, in the present case dependency trees. Figure 4 gives pseudo-code for the MIRA algorithmas presented by McDonald et al (2005). An on line learning algorithm considers a single training instance at each update to w. The auxiliary vector v accumulates the successive values of w, so that thefinal weight vector is the average of the weight vec Training data: T = {(xt, yt)}Tt=1 1. w0 = 0; v = 0; i = 0 2. for n : 1..N 3. for t : 1..T 4. min ? ? w(i+1) ? w(i) ? ? s.t. s(xt, yt) ? s(xt, y?) L(yt, y?), ?y? ? dt(xt) 5. v = v + w(i+1) 6. i = i + 1 7. w = v/(N ? T ) Figure 4: MIRA learning algorithm. tors after each iteration. This averaging effect has been shown to help overfitting (Collins, 2002). On each update, MIRA attempts to keep the new weight vector as close as possible to the old weight vector, subject to correctly classifying the instance under consideration with a margin given by the loss of the incorrect classifications. For dependency trees, the loss of a tree is defined to be the number of words with incorrect parents relative to the correct tree. This is closely related to the Hamming loss that is often used for sequences (Taskar et al, 2003).For arbitrary inputs, there are typically exponen tially many possible parses and thus exponentially many margin constraints in line 4 of Figure 4. 3.1 Single-best MIRA. One solution for the exponential blow-up in number of trees is to relax the optimization by using only the single margin constraint for the tree with the highest score, s(x,y). The resulting online update (to be inserted in Figure 4, line 4) would then be: min ? ?w(i+1) ? w(i) ? ? s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?) where y? = arg maxy? s(xt,y?) McDonald et al (2005) used a similar update with k constraints for the k highest-scoring trees, and showed that small values of k are sufficient toachieve the best accuracy for these methods. However, here we stay with a single best tree because k best extensions to the Chu-Liu-Edmonds algorithm are too inefficient (Hou, 1996). This model is related to the averaged perceptron algorithm of Collins (2002). In that algorithm, the single highest scoring tree (or structure) is used toupdate the weight vector. However, MIRA aggres sively updates w to maximize the margin between 527 the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy. 3.2 Factored MIRA. It is also possible to exploit the structure of the output space and factor the exponential number of mar gin constraints into a polynomial number of local constraints (Taskar et al, 2003; Taskar et al, 2004). For the directed maximum spanning tree problem,we can factor the output by edges to obtain the fol lowing constraints: min ? ?w(i+1) ? w(i) ? ? s.t. s(l, j) ? s(k, j) ? 1 ?(l, j) ? yt, (k, j) /? yt This states that the weight of the correct incomingedge to the word xj and the weight of all other in coming edges must be separated by a margin of 1. It is easy to show that when all these constraintsare satisfied, the correct spanning tree and all incor rect spanning trees are separated by a score at least as large as the number of incorrect incoming edges.This is because the scores for all the correct arcs can cel out, leaving only the scores for the errors causingthe difference in overall score. Since each single er ror results in a score increase of at least 1, the entirescore difference must be at least the number of er rors. For sequences, this form of factorization has been called local lattice preference (Crammer et al, 2004). Let n be the number of nodes in graph Gx. Then the number of constraints is O(n2), since for each node we must maintain n ? 1 constraints.The factored constraints are in general more re strictive than the original constraints, so they mayrule out the optimal solution to the original problem. McDonald et al (2005) examines briefly factored MIRA for projective English dependency pars ing, but for that application, k-best MIRA performs as well or better, and is much faster to train.
Experiments. . We performed experiments on the Czech Prague De pendency Treebank (PDT) (Hajic?, 1998; Hajic? et al,2001). We used the predefined training, develop ment and testing split of this data set. Furthermore, we used the automatically generated POS tags that are provided with the data. Czech POS tags are very complex, consisting of a series of slots that may ormay not be filled with some value. These slots rep resent lexical and grammatical properties such as standard POS, case, gender, and tense. The result is that Czech POS tags are rich in information, but quite sparse when viewed as a whole. To reduce sparseness, our features rely only on the reducedPOS tag set from Collins et al (1999). The num ber of features extracted from the PDT training set was 13, 450, 672, using the feature set outlined by McDonald et al (2005). Czech has more flexible word order than Englishand as a result the PDT contains non-projective de pendencies. On average, 23% of the sentences in the training, development and test sets have at least one non-projective dependency. However, less than2% of total edges are actually non-projective. There fore, handling non-projective edges correctly has a relatively small effect on overall accuracy. To show the effect more clearly, we created two Czech data sets. The first, Czech-A, consists of the entire PDT.The second, Czech-B, includes only the 23% of sen tences with at least one non-projective dependency.This second set will allow us to analyze the effectiveness of the algorithms on non-projective mate rial. We compared the following systems: 1. COLL1999: The projective lexicalized phrase-structure. parser of Collins et al (1999). 2. N&N2005: The pseudo-projective parser of Nivre and Nilsson (2005). 3. McD2005: The projective parser of McDonald et al. (2005) that uses the Eisner algorithm for both training and testing. This system uses k-best MIRA with k=5. 4. Single-best MIRA: In this system we use the Chu-Liu-. Edmonds algorithm to find the best dependency tree for Single-best MIRA training and testing. based on edge factorization as described in Section 3.2. We use the Chu-Liu-Edmonds algorithm to find the best tree for the test data. 4.1 Results. Results are shown in Table 1. There are two mainmetrics. The first and most widely recognized is Ac curacy, which measures the number of words that correctly identified their parent in the tree. Completemeasures the number of sentences in which the re sulting tree was completely correct.Clearly, there is an advantage in using the ChuLiu-Edmonds algorithm for Czech dependency pars 528 Czech-A Czech-B Accuracy Complete Accuracy CompleteCOLL1999 82.8 - - N&N2005 80.0 31.8 - McD2005 83.3 31.3 74.8 0.0 Single-best MIRA 84.1 32.2 81.0 14.9 Factored MIRA 84.4 32.3 81.5 14.3 Table 1: Dependency parsing results for Czech. Czech-B is the subset of Czech-A containing only sentences with at least one non-projective dependency. ing. Even though less than 2% of all dependenciesare non-projective, we still see an absolute improve ment of up to 1.1% in overall accuracy over the projective model. Furthermore, when we focus on the subset of data that only contains sentences with at least one non-projective dependency, the effect is amplified. Another major improvement here isthat the Chu-Liu-Edmonds non-projective MST al gorithm has a parsing complexity of O(n2), versusthe O(n3) complexity of the projective Eisner algo rithm, which in practice leads to improvements in parsing time. The results also show that in terms of Accuracy, factored MIRA performs better than single-best MIRA. However, for the factored model,we do have O(n2) margin constraints, which re sults in a significant increase in training time over single-best MIRA. Furthermore, we can also see that the MST parsers perform favorably compared to the more powerful lexicalized phrase-structure parsers, such as those presented by Collins et al (1999) andZeman (2004) that use expensive O(n5) parsing al gorithms. We should note that the results in Collins et al (1999) are different then reported here due to different training and testing data sets.One concern raised in Section 2.2.1 is that search ing the entire space of non-projective trees couldcause problems for languages that are primarily projective. However, as we can see, this is not a prob lem. This is because the model sets its weights with respect to the parsing algorithm and will disfavor features over unlikely non-projective edges. Since the space of projective trees is a subset ofthe space of non-projective trees, it is natural to won der how the Chu-Liu-Edmonds parsing algorithm performs on projective data since it is asymptotically better than the Eisner algorithm. Table 2 shows theresults for English projective dependency trees ex tracted from the Penn Treebank (Marcus et al, 1993) using the rules of Yamada and Matsumoto (2003). English Accuracy Complete McD2005 90.9 37.5 Single-best MIRA 90.2 33.2 Factored MIRA 90.2 32.3Table 2: Dependency parsing results for English us ing spanning tree algorithms. This shows that for projective data sets, training and testing with the Chu-Liu-Edmonds algorithm is worse than using the Eisner algorithm. This is notsurprising since the Eisner algorithm uses the a pri ori knowledge that all trees are projective.
Discussion. . We presented a general framework for parsing dependency trees based on an equivalence to maximum spanning trees in directed graphs. This frame work provides natural and efficient mechanismsfor parsing both projective and non-projective languages through the use of the Eisner and Chu-Liu Edmonds algorithms. To learn these structures we used online large-margin learning (McDonald et al,2005) that empirically provides state-of-the-art per formance for Czech.A major advantage of our models is the ability to naturally model non-projective parses. Non projective parsing is commonly considered more difficult than projective parsing. However, under our framework, we show that the opposite is actuallytrue that non-projective parsing has a lower asymptotic complexity. Using this framework, we pre sented results showing that the non-projective modeloutperforms the projective model on the Prague De pendency Treebank, which contains a small number of non-projective edges. Our method requires a tree score that decomposes according to the edges of the dependency tree. One might hope that the method would generalize to 529include features of larger substructures. Unfortu nately, that would make the search for the best tree intractable (Ho?ffgen, 1993). Acknowledgments We thank Lillian Lee for bringing an importantmissed connection to our attention, and Koby Cram mer for his help with learning algorithms. This work has been supported by NSF ITR grants 0205448 and 0428193.