We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment. 1 Desiderata Many words are ambiguous in their part of speech. For example, &quot;tag&quot; can be a noun or a verb. However, when a word appears in the context of other words, the ambiguity is often reduced: in &quot;a tag is a part-of-speech label,&quot; the &quot;tag&quot; can only be a noun. A tagger is a system that uses context to assign parts of speech to words. Automatic text tagging is an important first step in discovering the linguistic structure of large text corpora. Part-of-speech information facilitates higher-level analysis, such as recognizing noun phrases and other patterns in text. For a tagger to function as a practical component in a language processing system, we believe that a tagger must be: corpora contain ungrammatical constructions, isolated phrases (such as titles), and nonlinguistic data (such as tables). Corpora are also likely to contain words that are unknown to the tagger. It is desirable that a tagger deal gracefully with these situations. a tagger is to be used to analyze arbitrarily large corpora, it must be efficient—performing in time linear in the number of words tagged. Any training required should also be fast, enabling rapid turnaround with new corpora and new text genres. A should attempt to assign the correct part-of-speech tag to every word encountered. A should be able to take advantage of linguistic insights. One should be able to correct errors by supplying appropriate priori &quot;hints.&quot; It should be possible to give different hints for different corpora. effort required to retarget a tagger to new corpora, new tagsets, and new languages should be minimal. 2 Methodology 2.1 Background Several different approaches have been used for building text taggers. Greene and Rubin used a rule-based approach in the TAGGIT program [Greene and Rubin, 1971], which was an aid in tagging the Brown corpus [Francis and KuEera, 1982]. TAGGIT disambiguated 77% of the corpus; the rest was done manually over a period of several years. More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines [Koskenniemi, 1990]. Statistical methods have also been used (e.g., [DeRose, [Garside al., These provide the capability of resolving ambiguity on the basis of most likely interpretation. A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part-of-speech category, which in turn depends solely on the categories of the preceding two words. Two types of training (i.e., parameter estimation) have been used with this model. The first makes use of a tagged training corpus. Derouault and Merialdo use a bootstrap method for training [Derouault and Merialdo, 1986]. At first, a relatively small amount of text is manually tagged and used to train a partially accurate model. The model is then used to tag more text, and the tags are manually corrected and then used to retrain the model. Church uses the tagged Brown corpus for training [Church, 1988]. These models involve probabilities for each word in the lexicon, so large tagged corpora are required for reliable estimation. The second method of training does not require a tagged training corpus. In this situation the Baum-Welch algorithm (also known as the forward-backward algorithm) can be used [Baum, 1972]. Under this regime the model is a Markov model as state transitions (i.e., part-of-speech categories) are assumed to be unobservable. Jelinek has used this method for training a text tagger [Jelinek, 1985]. Parameter smoothing can be conachieved using the method of interpolawhich weighted estimates are taken from secondand first-order models and a uniform probability distribution [Jelinek and Mercer, 1980]. Kupiec used word equivclasses (referred to here as classes) on parts of speech, to pool data from individual words [Kupiec, 1989b]. The most common words are still represented individually, as sufficient data exist for robust estimation. 133 However all other words are represented according to the set of possible categories they can assume. In this manner, the vocabulary of 50,000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes [Kupiec, 1992]. To further reduce the number of parameters, a first-order model can be employed (this assumes that a word's category depends only on the immediately preceding word's category). In [Kupiec, 1989a], networks are used to selectively augment the context in a basic firstorder model, rather than using uniformly second-order dependencies. 2.2 Our approach We next describe how our choice of techniques satisfies the listed in section 1. The use of an complete flexibility in the choice of training corpora. Text from any desired domain can be used, and a tagger can be tailored for use with a particular text database by training on a portion of that database. Lexicons containing alternative tag sets can be easily accommodated without any need for re-labeling the training corpus, affording further flexibility in the use of specialized tags. As the resources required are simply a lexicon and a suitably large sample of ordinary text, taggers can be built with minimal effort, even for other languages, such as French (e.g., [Kupiec, 1992]). The use of ambiguity classes and a first-order model reduces the number of parameters to be estimated without significant reduction in accuracy (discussed in section 5). This also enables a tagger to be reliably trained using only moderate amounts of text. We have produced reasonable results training on as few as 3,000 sentences. Fewer parameters also reduce the time required for training. Relatively few ambiguity classes are sufficient for wide coverage, so it is unlikely that adding new words to the lexicon requires retraining, as their ambiguity classes are already accommodated. Vocabulary independence is achieved by predicting categories for words not in the lexicon, using both context and suffix information. Probabilities corresponding to category sequences that never occurred in the training data are assigned small, non-zero values, ensuring that the model will accept any sequence of tokens, while still providing the most likely tagging. By using the fact that words are typically associated with only a few part-ofspeech categories, and carefully ordering the computation, the algorithms have linear complexity (section 3.3). 3 Hidden Markov Modeling The hidden Markov modeling component of our tagger is implemented as an independent module following the specgiven in [Levinson et with special attention to space and time efficiency issues. Only first-order modeling is addressed and will be presumed for the remainder of this discussion. 3.1 Formalism brief, an a doubly stochastic process that generates sequence of symbols ={Sl,S2, , 1 <i< where W is some finite set of possible symbols, by composing an underlying Markov process with a state-dependent symbol generator (i.e., a Markov process with noise).' Th Markov process captures the notion of sequence depen and is described by a set of a matrix c probabilities A -= 1 < a, the probability of moving from state i to state of initial probabilities H = {70 1 < i < is the probability of starting in state i. The symbol ger erator is a state-dependent measure on V described by of symbol probabilities 1 < j < < < M = IW I is the probability ( symbol given that the Markov process is i In part-of-speech tagging, we will model word order di pendency through an underlying Markov process that 01 erates in terms of lexical tags,' yet we will only be ab to observe the sets of tags, or ambiguity classes, that ai possible for individual words. The ambiguity class of eac word is the set of its permitted parts of speech, only or of which is correct in context. Given the parameters .4, Markov modeling allows us to compute ti most probable sequence of state transitions, and hence a mostly likely sequence of lexical tags, corresponding to of ambiguity classes. In the following, identified with the number of possible .tags, and W wil the set of all ambiguity classes. Applying an HMM consists of two tasks: estimating ti parameters A, a training set; ar computing the most likely sequence of underlying sta transitions given new observations. Maximum likeliho( estimates (that is, estimates that maximize the probabili of the training set) can be found through application of z ternating expectation in a procedure known as the Baur Welch, or forward-backward, algorithm [Baum, 1972]. proceeds by recursively defining two sets of probabiliti( the forward probabilities eft+1. (i) = bi(St+i) < t < T — 1, ( = for all the backward prob bilities, i3(i) = — 1 < t < 1, ( 1.1 1 for all forward probabili the joint probability of the sequence up to tir t, S2, , the event that the Markov pr is in state i at time the backwa is the probability of seeing the sequen , ST} that the Markov process is state i at time t. It follows that the probability of t entire sequence is N N =