1 Introduction. Recently, there has been a surge in research in machine translation that is based on empirical methods. The seminal work by Brown et al. [1990] at IBM on the Candide system laid the foundation for much of the current work in Statistical Machine Translation (SMT). Some of this work has been re-implemented and is freely available for research purposes [AlOnaizan et al., 1999]. Roughly speaking, SMT divides the task of translation into two steps: a word-level translation model and a model for word reordering during the translation process. The statistical models are trained on parallel corpora: large amounts of text in one language along with their translation in another. Various parallel texts have recently become available, mostly from government sources such as parliament proceedings (the Canadian Hansard, the minutes of the European parliament1) or law texts (from Hong Kong). Still, for most language pairs, parallel texts are hard to come by. This is clearly the case for low-density languages such as Tamil, Swahili, or Tetun. Furthermore, texts derived from parliament speeches may not be appropriate for a particular targeted domain. Specific parallel texts can be constructed by hand for the purpose of training an SMT system, but this is a very costly endeavor. On the other hand, the digital revolution and the wide-spread use of the World Wide Web have proliferated vast amounts of monolingual corpora. Publishing text in one language is a much more natural human activity than producing parallel texts. To illustrate this point: The world wide web alone contains currently over two billion pages, a number that is still growing exponentially. According to Google,2 the word directory occurs 61 million times, empathy 383,000 times, and reflex 787,000 times. In the Hansard, each of these words occurs only once. The objective of this research to build a translation lexicon solely from monolingual corpora. Specifically, we want to automatically generate a one-to-one mapping of German and English nouns. We are testing our mappings against a bilingual lexicon of 9,206 German and 10,645 English nouns. The two monolingual corpora should be in a fairly comparable domain. For our experiments we use the 1990-1992 Wall Street Journal corpus on the English side and the 1995-1996 German news wire (DPA) corpus on the German side. Both corpora are news sources in the general sense. However, they span different time periods and have a different orientation: the World Street Journal covers mostly business news, the German news wire mostly German politics. For experiments on training probabilistic translation lexicons from parallel corpora and similar tasks on the same test corpus, refer to our earlier work [Koehn and Knight, 2000, 2001].
2 Clues. This section will describe clues that enable us to find translations of words of the two monolingual corpora. We will examine each clue separately. The following clues are considered: more frequent than flower, as its translation Regierung is more frequent than Blume. We will now look in detail how these clues may contribute to building a German-English translation lexicon. Due to cultural exchange, a large number of words that originate in one language are adopted by others. Recently, this phenomenon can be seen with words such as Internet, or Aids. These terms may be adopted verbatim, or changed by well-established rules. For instance, immigration (German and English) has the Portuguese translation immigra¸c˜ao, as many words ending in -tion have translations with the same spelling except for the ending changed to -¸c˜ao. We examined the German words in our lexicon and tried to find English words that have the exact same spelling. Surprisingly, we could count a total of 976 such words. When checking them against a benchmark lexicon, we found these mappings to be 88% correct. The correctness of word mappings acquired in this fashion depends highly on word length. This is illustrated in Table 1: While identical 3letter words are only translations of each other 60% of the time, this is true for 98% of 10-letter words. Clearly, for shorter words, the accidental existence of an identically spelled word in the other language word is much higher. This includes words such as fee, ton, art, and tag. spelled words are in fact translations of each other: The accuracy of this assumption depends highly on the length of the words (see Section 2.1) Knowing this allows us to restrict the word length to be able to increase the accuracy of the collected word pairs. For instance, by relying only on words at least of length 6, we could collect 622 word pairs with 96% accuracy. In our experiments, however, we included all the words pairs. As already mentioned, there are some wellestablished transformation rules for the adoption of words from a foreign language. For German to English, this includes replacing the letters k and z by c and changing the ending -t¨at by -ty. Both these rules can be observed in the word pair Elektrizit¨at and electricity. By using these two rules, we can gather 363 additional word pairs of which 330, or 91%, are in fact translations of each other. The combined total of 1339 (976+363) word pairs are separated and form the seed for some of the following steps. When words are adopted into another language, their spelling might change slightly in a manner that can not be simply generalized in a rule. Observe, for instance website and Webseite. This is even more the case for words that can be traced back to common language roots, such as friend and Freund, or president and Pr¨asident. Still, these words – often called cognates – maintain a very similar spelling. This can be defined as differing in very few letters. This measurement can be formalized as the number of letters common in sequence between the two words, divided by the length of the longer word. The example word pair friend and freund shares 5 letters (fr-e-nd), and both words have length 6, hence there spelling similarity is 5/6, or 0.83. This measurement is called longest common subsequence ratio [Melamed, 1995]. In related work, string edit distance (or, Levenshtein distance) has been used [Mann and Yarowski, 2001]. With this computational means at hand, we can now measure the spelling similarity between every German and English word, and sort possible word pairs accordingly. By going through this list starting at the top we can collect new word pairs. We do this is in a greedy fashion – once a word is assigned to a word pair, we do not look for another match. Table 2 gives the top 24 generated word pairs by this algorithm. ing words with most similar spelling in a greedy fashion. The applied measurement of spelling similarity does not take into account that certain letter changes (such as z to s, or dropping of the final e) are less harmful than others. Tiedemann [1999] explores the automatic construction of a string similarity measure that learns which letter changes occur more likely between cognates of two languages. This measure is trained, however, on parallel sentence-aligned text, which is not available here. Obviously, the vast majority of word pairs can not be collected this way, since their spelling shows no resemblance at all. For instance, Spiegel and mirror share only one vowel, which is rather accidental. If our monolingual corpora are comparable, we can assume a word that occurs in a certain context should have a translation that occurs in a similar context. Context, as we understand it here, is defined by the frequencies of context words in surrounding positions. This local context has to be translated into the other language, and we can search the word with the most similar context. This idea has already been investigated in earlier work. Rapp [1995, 1999] proposes to collect counts over words occurring in a four word window around the target word. For each occurrence of a target word, counts are collected over how often certain context words occur in the two positions directly ahead of the target word and the two following positions. The counts are collected separately for each position and then entered into in a context vector with an dimension for each context word in each position. Finally, the raw counts are normalized, so that for each of the four word positions the vector values add up to one. Vector comparison is done by adding all absolute differences of all components. Fung and Yee [1998] propose a similar approach: They count how often another word occurs in the same sentence as the target word. The counts are then normalized by a using the tf/idf method which is often used in information retrieval [Jones, 1979]. The need for translating the context poses a chicken-and-egg problem: If we already have a translation lexicon we can translate the context vectors. But we can only construct a translation lexicon with this approach if we are already able to translate the context vectors. Theoretically, it is possible to use these methods to build a translation lexicon from scratch [Rapp, 1995]. The number of possible mappings has complexity O(n! ), and the computing cost of each mapping has quadratic complexity O(n2). For a large number of words n – at least more than 10,000, maybe more than 100,000 – the combined complexity becomes prohibitively expensive. Because of this, both Rapp and Fung focus on expanding an existing large lexicon to add a few novel terms. Clearly, a seed lexicon to bootstrap these methods is needed. Fortunately, we have outlined in Section 2.1 how such a seed lexicon can be obtained: by finding words spelled identically in both languages. We can then construct context vectors that contain information about how a new unmapped word co-occurs with the seed words. This vector can be translated into the other language, since we already know the translations of the seed words. Finally, we can look for the best matching context vector in the target language, and decide upon the corresponding word to construct a word mapping. Again, as in Section 2.2, we have to compute all possible word – or context vector – matches. We collect then the best word matches in a greedy fashion. Table 3 displays the top 15 generated word pairs by this algorithm. The context vectors are constructed in the way proposed by Rapp [1999], with the difference that we collect counts over a four noun window, not a four word window, by dropping all intermediate words. Intuitively it is obvious that pairs of words that are similar in one language should have translations that are similar in the other language. For instance, Wednesday is similar to Thursday as Mittwoch is similar to Donnerstag. Or: dog is similar to cat in English, as Hund is similar to Katze in German. The challenge is now to come up with a quantifiable measurement of word similarity. One strategy is to define two words as similar if they occur in a similar context. Clearly, this is the case for Wednesday and Thursday, as well as for dog and cat. Exactly this similarity measurement is used in the work by Diab and Finch [2000]. Their approach to constructing and comparing context vectors differs significantly from methods discussed in the previous section. For each word in the lexicon, the context vector consists of co-occurrence counts in respect to 150 so-called peripheral tokens, basically the most frequent words. These counts are collected for each position in a 4-word window around the word in focus. This results in a 600-dimensional vector. Instead of comparing these co-occurrence counts directly, the Spearman rank order correlation is applied: For each position the tokens are compared in frequency and the frequency count is replaced by the frequency rank – the most frequent token count is replaced by 1, the least frequent by n = 150. The similarity of two context vectors a = (ai) and b = (bi) is then defined by:3 The result of all this is a matrix with similarity scores between all German words, and second one with similarity scores between all English words. Such matrices could also be constructed using the definitions of context we reviewed in the previous section. The important point here is that we have generated a similarity matrix, which we will use now to find new translation word pairs. Again, as in the previous Section 2.3, we as3In the given formula we fixed two mistakes of the original presentation [Diab and Finch, 2000]: The square of the differences is used, and the denominator contains the additional factor 4, since essentially 4 150-word vectors are compared. sume that we will already have a seed lexicon. For a new word we can look up its similarity scores to the seed words, thus creating a similarity vector. Such a vector can be translated into the other language – recall that dimensions of the vector are the similarity scores to seed words, for which we already have translations. The translated vector can be compared to other vectors in the second language. As before, we search greedily for the best matching similarity vectors and add the corresponding words to the lexicon. Finally, another simple clue is the observation that in comparable corpora, the same concepts should be used with similar frequencies. Even if the most frequent word in the German corpus is not necessarily the translation of the most frequent English word, it should also be very frequent. Table 4 illustrates the situation with our corpora. It contains the top 10 German and English words, together with the frequency ranks of their best translations. For both languages, 4 of the 10 words have translations that also rank in the top 10. Clearly, simply aligning the nth frequent German word with the nth frequent English word is not a viable strategy. In our case, this is additionally hampered by the different orientation of the news sources. The frequent financial terms in the English WSJ corpus (stock, bank, sales, etc.) are rather rare in the German corpus. For most words, especially for more comparable corpora, there is a considerable correlation between the frequency of a word and its translation. Our frequency measurement is defined as ratio of the word frequencies, normalized by the corpus sizes.
3 Experiments. This section provides more detail on the experiments we have carried out to test the methods just outlined. quent German and English words and their translations. We are trying to build a one-to-one GermanEnglish translation lexicon for the use in a machine translation system. To evaluate this performance we use two different measurements: Firstly, we record how many correct word-pairs we have constructed. This is done by checking the generated wordpairs against an existing bilingual lexicon.4 In essence, we try to recreate this lexicon, which contains 9,206 distinct German and 10,645 distinct English nouns and 19,782 lexicon entries. For a machine translation system, it is often more important to get more frequently used words right than obscure ones. Thus, our second evaluation measurement tests the word translations proposed by the acquired lexicon against the actual word-level translations in a 5,000 sentence aligned parallel corpus.5 The starting point to extending the lexicon is the seed lexicon of identically spelled words, as described in Section 2.1. It consists of 1339 entries, of which are (88.9%) correct according to the existing bilingual lexicon. Due to computational constraints,6 we focus on the additional mapping of only 1,000 German and English words. These 1,000 words are chosen from the 1,000 most frequent lexicon entries in the dictionary, without duplications of words. This frequency is defined by the sum of two word frequencies of the words in the entry, as found in the monolingual corpora. We did not collect statistics of the actual use of lexical entries in, say, a parallel corpus. In a different experimental set-up we also simply tried to match the 1,000 most frequent German words with the 1,000 most frequent English words. The results do not differ significantly. Each of the four clues described in the Sections 2.2 to 2.5 provide a matching score between a German and an English word. The likelihood of these two words being actual translations of each other should correlate to these scores. There are many ways one could search for the best set of lexicon entries based on these scores. We could perform an exhaustive search: construct all possible mappings and find the highest combined score of all entries. Since there are O(n!) possible mappings, a brute force approach to this is practically impossible. We therefore employed a greedy search: First we search for the highest score for any word pair. We add this word pair to the lexicon, and drop word pairs that include either the German and English word from further search. Again, we search for the highest score and add the corresponding word pair, drop these words from further search, and so on. This is done iteratively, until all words are used up. Tables 2 and 3 illustrate this process for the spelling and context similarity clues, when applied separately. The results are summarized in Table 5. Recall that for each word that we are trying to map to the other language, a thousand possible target words exist, but only one is correct. The baseline for this task, choosing words at random, results on average in only 1 correct mapping in the entire lexicon. A perfect lexicon, of course, contains 1000 correct entries. The starting point for the corpus score is the 15.8% that are already achieved with the seed lexicon from Section 2.1. In an experiment where we identified the best lexical entries using a very large parallel corpus, we could achieve 89% accuracy on this test corpus. many correct lexicon entries where added (Entries), and how well the resulting translation lexicon performs compared to the actual wordlevel translations in a parallel corpus (Corpus). For all experiments the starting point was the seed lexicon of 1339 identical spelled words described in Section 2.1. which achieve 15.8% Corpus score. Taken alone, both the context and spelling clues learn over a hundred lexicon entries correctly. The similarity and frequency clues, however, seem to be too imprecise to pinpoint the search to the correct translations. A closer look of the spelling and context scores reveals that while the spelling clue allows to learn more correct lexicon entries (140 opposed to 107), the context clue does better with the more frequently used lexicon entries, as found in the test corpus (accuracy of 31.9% opposed to 25.4%). Combining different clues is quite simple: We can simply add up the matching scores. The scores can be weighted. Initially we simply weighted all clues equally. We then changed the weights to see, if we can obtain better results. We found that there is generally a broad range of weights that result in similar performance. When using the spelling clue in combination with others, we found it useful to define a cutoff. If two words agree in 30% of their letters this is generally as bad as if they do not agree in any – the agreements are purely coincidental. Therefore we counted all spelling scores below 0.3 as 0.3. Combining the context and the spelling clues yields a significantly better result than using each clue by itself. A total of 185 correct lexical entries are learned with a corpus score of 38.6%. Adding in the other scores, however, does not seem to be beneficial: only adding the frequency clue to the spelling clue provides some improvement. In all other cases, these scores are not helpful. Besides this linear combination of scores from the different clues, more sophisticated methods may be possible [Koehn, 2002].
4 Conclusions. We have attempted to learn a one-to-one translation lexicon purely from unrelated monolingual corpora. Using identically spelled words proved to be a good starting point. Beyond this, we examined four different clues. Two of them, matching similar spelled words and words with the same context, helped us to learn a significant number of additional correct lexical entries. Our experiments have been restricted to nouns. Verbs, adjectives, adverbs and other part of speech may be tackled in a similar way. They might also provide useful context information that is beneficial to building a noun lexicon. These methods may be also useful given a different starting point: For efforts in building machine translation systems, some small parallel text should be available. From these, some high-quality lexical entries can be learned, but there will always be many words that are missing. These may be learned using the described methods.