Introduction. Named Entity (NE) recognition is a task in whichproper nouns and numerical information in a docu ment are detected and classified into categories suchas person, organization, and date. It is a key technol ogy of Information Extraction and Open-Domain Question Answering (Voorhees and Harman, 2000). We are building a trainable Open-Domain Question Answering System called SAIQA-II. In this paper, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores thanconventional systems. SVMs have given high per formance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). However, it turned out that off-the-shelf SVM classifiers are too inefficient for NE recognition. The recognizer runs at a rate of only 85 bytes/sec on an Athlon 1.3 GHz Linux PC, while rule-based systems (e.g., Isozaki, (2001)) can process several kilobytes in a second. The major reason is the inefficiency of SVM classifiers. There are otherreports on the slowness of SVM classifiers. Another SVM-based NE recognizer (Yamada and Mat sumoto, 2001) is 0.8 sentences/sec on a Pentium III 933 MHz PC. An SVM-based part-of-speech (POS). tagger (Nakagawa et al, 2001) is 20 tokens/sec on an Alpha 21164A 500 MHz processor. It is difficult to use such slow systems in practical applications. In this paper, we present a method that makes the NE system substantially faster. This method can also be applied to other tasks in natural languageprocessing such as chunking and POS tagging. Another problem with SVMs is its incomprehensibil ity. It is not clear which features are important or how they work. The above method is also useful for finding useless features. We also mention a method to reduce training time. 1.1 Support Vector Machines. Suppose we have a set of training data for a two class problem:     , where   ffflfi is a feature vector of the ffi -th sample in the training data and   !$#%# is the label forthe sample. The goal is to find a decision func tion that accurately predicts for unseen  . A non-linear SVM classifier gives a decision function ( ) * sign ,+-) for an input vector  where +-) .* / 0 21)3 546879: !6; Here, () *=!$# means  is a member of a cer tain class and () $* # means  is not a mem ber. 7 s are called support vectors and are repre sentatives of training examples. is the numberof support vectors. Therefore, computational com plexity of +?) is proportional to  . Support vectorsand other constants are determined by solving a cer tain quadratic programming problem. 4687@ is akernel that implicitly maps vectors into a higher di mensional space. Typical kernels use dot products: 4687@ A*CBED7@ . A polynomial kernel of degree Fis given by BG? *HI#J!KG L . We can use vari MM M M N M M M M M M M M M N M O O O O O N O O O O O O O O O O O O M : positive example, O : negative example N M , N O : support vectors Figure 1: Support Vector Machine ous kernels, and the design of an appropriate kernel for a particular application is an important research issue.Figure 1 shows a linearly separable case. The de cision hyperplane defined by +-) P*RQ separatespositive and negative examples by the largest mar gin. The solid line indicates the decision hyperplaneand two parallel dotted lines indicate the margin be tween positive and negative examples. Since such aseparating hyperplane may not exist, a positive pa rameter S is introduced to allow misclassifications. See Vapnik (1995). 1.2 SVM-based NE recognition. As far as we know, the first SVM-based NE system was proposed by Yamada et al (2001) for Japanese.His system is an extension of Kudo?s chunking sys tem (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. In theirsystem, every word in a sentence is classified sequentially from the beginning or the end of a sen tence. However, since Yamada has not compared it with other methods under the same conditions, it is not clear whether his NE system is better or not. Here, we show that our SVM-based NE system ismore accurate than conventional systems. Our sys tem uses the Viterbi search (Allen, 1995) instead of sequential determination.For training, we use ?CRL data?, which was prepared for IREX (Information Retrieval and Extrac tion Exercise1, Sekine and Eriguchi (2000)). It has about 19,000 NEs in 1,174 articles. We also use additional data by Isozaki (2001). Both datasets are based on Mainichi Newspaper?s 1994 and 1995 CD-ROMs. We use IREX?s formal test data calledGENERAL that has 1,510 named entities in 71 ar ticles from Mainichi Newspaper of 1999. Systems are compared in terms of GENERAL?s F-measure 1http://cs.nyu.edu/cs/projects/proteus/irexwhich is the harmonic mean of ?recall? and ?preci sion? and is defined as follows. Recall = M/(the number of correct NEs), Precision = M/(the number of NEs extracted by a system), where M is the number of NEs correctly extracted and classified by the system.We developed an SVM-based NE system by following our NE system based on maximum entropy (ME) modeling (Isozaki, 2001). We sim ply replaced the ME model with SVM classifiers.The above datasets are processed by a morphological analyzer ChaSen 2.2.12. It tokenizes a sen tence into words and adds POS tags. ChaSen uses about 90 POS tags such as common-noun and location-name. Since most unknown words are proper nouns, ChaSen?s parameters for unknownwords are modified for better results. Then, a char acter type tag is added to each word. It uses 17character types such as all-kanji and small integer. See Isozaki (2001) for details. Now, Japanese NE recognition is solved by theclassification of words (Sekine et al, 1998; Borth wick, 1999; Uchimoto et al, 2000). For instance, the words in ?President George Herbert Bush saidClinton is . . . are classified as follows: ?President? = OTHER, ?George? = PERSON-BEGIN, ?Her bert? = PERSON-MIDDLE, ?Bush? = PERSON-END, ?said? = OTHER, ?Clinton? = PERSON-SINGLE, ?is? = OTHER. In this way, the first word of a person?s name is labeled as PERSON-BEGIN. The last word is labeled as PERSON-END. Other words in the nameare PERSON-MIDDLE. If a person?s name is expressed by a single word, it is labeled as PERSON SINGLE. If a word does not belong to any namedentities, it is labeled as OTHER. Since IREX de fines eight NE classes, words are classified into 33 ( *UTWVEX!K# ) categories.Each sample is represented by 15 features be cause each word has three features (part-of-speech tag, character type, and the word itself), and two preceding words and two succeeding words are also used for context dependence. Although infrequent features are usually removed to prevent overfitting, we use all features because SVMs are robust. Each sample is represented by a long binary vector, i.e., a sequence of 0 (false) and 1 (true). For instance, ?Bush? in the above example is represented by a 2http://chasen.aist-nara.ac.jp/ vector P*YG[Z\#^]_ G[Z `a] described below. Only 15 elements are 1. bdcfe8ghji // Current word is not ?Alice? bdc klghme // Current word is ?Bush? bdc nghji // Current word is not ?Charlie? : bdcfe^opikpqpghme // Current POS is a proper noun bdcfe^opinipghji // Current POS is not a verb : bdc nqre^sre ghji // Previous word is not ?Henry? bdc nqre^skghme // Previous word is ?Herbert? :Here, we have to consider the following problems. First, SVMs can solve only a two-class problem. Therefore, we have to reduce the above multi class problem to a group of two-class problems. Second, we have to consider consistency among word classes in a sentence. For instance, a word classified as PERSON-BEGIN should be followed by PERSON-MIDDLE or PERSON-END. It impliesthat the system has to determine the best combina tions of word classes from numerous possibilities.Here, we solve these problems by combining exist ing methods. There are a few approaches to extend SVMs to cover t -class problems. Here, we employ the ?oneclass versus all others? approach. That is, each clas sifier (%u ) is trained to distinguish members of a class v from non-members. In this method, two or more classifiers may give !$# to an unseen vector or no classifier may give !$# . One common way to avoid such situations is to compare + u ) values and to choose the class index v of the largest + u ) . The consistency problem is solved by the Viterbi search. Since SVMs do not output probabilities, we use the SVM+sigmoid method (Platt, 2000). That is, we use a sigmoid function wxG? J*y#zI#{! |l}~ {G to map + u ) to a probability-like value. The output of the Viterbi search is adjusted by a postprocessor for wrong word boundaries. The adjustment rules are also statistically determined (Isozaki, 2001). 1.3 Comparison of NE recognizers. We use a fixed value ?* #Q9Q . F-measures are not very sensitive to  unless  is too small. Whenwe used 1,038,986 training vectors, GENERAL?s F measure was 89.64% for ?*?Q?# and 90.03% for 6*?#Q9Q . We employ the quadratic kernel ( F *Y? ) because it gives the best results. Polynomial kernels of degree 1, 2, and 3 resulted in 83.03%, 88.31%, F-measure (%) ? ? RG+DT ? ? ME ? ? SVM 0 20 40 60 80 100 120 CRL data ???E? ?^??:??? 76 78 80 82 84 86 88 90 Number of NEs in training data ( ?? ) Figure 2: F-measures of NE systems and 87.04% respectively when we used 569,994 training vectors. Figure 2 compares NE recognizers in terms ofGENERAL?s F-measures. ?SVM? in the figure in dicates F-measures of our system trained by Kudo?s TinySVM-0.073 with S?*?Q?# . It attained 85.04% when we used only CRL data. ?ME? indicates our ME system and ?RG+DT? indicates a rule-basedmachine learning system (Isozaki, 2001). According to this graph, ?SVM? is better than the other sys tems.However, SVM classifiers are too slow. Fa mous SVM-Light 3.50 (Joachims, 1999) took 1.2 days to classify 569,994 vectors derived from 2 MB documents. That is, it runs at only 19 bytes/sec. TinySVM?s classifier seems best optimized among publicly available SVM toolkits, but it still works at only 92 bytes/sec.
Efficient Classifiers. . In this section, we investigate the cause of this in efficiency and propose a solution. All experiments are conducted for training data of 569,994 vectors. The total size of the original news articles was 2 MB and the number of NEs was 39,022. According to the definition of +-) , a classifier has to process support vectors for each  . Table 1 shows  s for different word classes. According to this table, classi fication of one word requires  ?s dot products with 228,306 support vectors in 33 classifiers. Therefore, the classifiers are very slow. We have never seensuch large  s in SVM literature on pattern recogni tion. The reason for the large  s is word features. Inother domains such as character recognition, dimen 3http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM sion ` is usually fixed. However, in the NE task, ` increases monotonically with respect to the size of the training data. Since SVMs learn combinations of features,  tends to be very large. This tendencywill hold for other tasks of natural language pro cessing, too. Here, we focus on the quadratic kernel BG * I#!?G ? that yielded the best score in the above experiments. Suppose ?* G[Z\#^]_ G[Z `a] hasonly ? (=15) non-zero elements. The dot prod uct of  and 7  * 5? Z\#^]_ ?  Z `] is given by ? fi ? 1) G[Z?? Z?? ] . Hence, I#!??D?7  ? *?#!W? fi 0 ? 1) G?Z?? Z???]!? fi 0 ? 1) G?Z?? Z???] ?  We can rewrite +-) as follows. fi 0 ? 1) _?  Z?? ]?G[Z???]?!m? ? Z???]?G[Z???] fi.? 0 ? 1) fi 0 ? 1 ?? ???rZ??? B@]?G[Z?? ]?G?Z?B@]_ where ? ? / ?1) 3 ? ??Z?? / ?1) 3 5? Z?? ]_ ? ? Z?? ]?* ? / ?1) 3 ??p8Z?? ]??% ?P?rZ?? B@]?* ? ? / ?1) 3  ?  Z?? Z?B@]_ For binary vectors, it can be simplified as +-) .*?? 0 ??,?9? ?l? 1) _?C?  Z???] 0 ?-?,????%? ?9? 1) ? ? Z?? B@]  where ? ? Z???]?* ?  Z???] !m? ? Z???]Y* ? 0  ???5? ?l? 1) 3   ???9Z??? B@]?* ? 0  ?,???_? ?l? 1 ????? 1) 3   Now, +?) can be given by summing up ? ? Z???] for every non-zero element G?Z?? ] and ? ? Z?? B@] for every non-zero pair G?Z?? ]?G[Z?B@] . Accordingly, we only need to add #W!???!??j?R?# z%? (=121) con stants to get +-) . Therefore, we can expect thismethod to be much faster than a na??ve implementa tion that computes tens of thousands of dot products at run time. We call this method ?XQK? (eXpand the Quadratic Kernel). Table 1 compares TinySVM and XQK in terms of CPU time taken to apply 33 classifiers to process the training data. Classes are sorted by  . Small numbers in parentheses indicate the initializationtime for reading support vectors 7   and allocat ing memory. XQK requires a longer initialization time in order to prepare ? ? and ??? For instance,TinySVM took 11,490.26 seconds (3.2 hours) in to tal for applying OTHER?s classifier to all vectors in the training data. Its initialization phase took 2.13 seconds and all vectors in the training data were classified in 11,488.13 ( *=#9#%X?%Q??9????x?#p? ) sec onds. On the other hand, XQK took 225.28 secondsin total and its initialization phase took 174.17 sec onds. Therefore, 569,994 vectors were classified in51.11 seconds. The initialization time can be disre garded because we can reuse the above coefficents. Consequently, XQK is 224.8 (=11,488.13/51.11) times faster than TinySVM for OTHER. TinySVM took 6 hours to process all the word classes, whereas XQK took only 17 minutes. XQK is 102 times faster than SVM-Light 3.50 which took 1.2 days.
Removal of useless features. . XQK makes the classifiers faster, but mem ory requirement increases from ? ? / ?1) ?  to ? ? / ?1) ?  ? !fl# z%?r where ? (=15) is the num ber of non-zero elements in 7  . Therefore, removal. of useless features would be beneficial. Conven tional SVMs do not tell us how an individual feature works because weights are given not to features but to 4687  . However, the above weights ( ? ? and ??? ) clarify how a feature or a feature pair works. We can use this fact for feature selection after the training. We simplify ( ) by removing all features ? that satisfy ?? } 8??? Z?? ]?f??? } ? ?????rZ??? B@]?f ?? } ? ???P?rZ?B- ?]?? K??? The largest ? that does not change the number of misclassifications for the training data is found by using the binary searchfor each word class. We call this method ?XQKFS? (XQK with Feature Selection). This approx imation slightly degraded GENERAL?s F-measure from 88.31% to 88.03%.Table 2 shows the reduction of features that ap pear in support vectors. Classes are sorted by the numbers of original features. For instance, OTHERhas 56,220 features in its support vectors. Accord ing to the binary search, its performance did notchange even when the number of features was re duced to 21,852 at ?*KQ?Qr?9?r?%? Table 1: Reduction of CPU time (in seconds) by XQK word class  TinySVM (init) XQK (init) speed up SVM-Light OTHER 64,970 11,488.13 (2.13) 51.11 (174.17) 224.8 29,986.52 ARTIFACT-MIDDLE 14,171 1,372.85 (0.51) 41.32 (14.98) 33.2 6,666.26 LOCATION-SINGLE 13,019 1,209.29 (0.47) 38.24 (11.41) 31.6 6,100.54 ORGANIZ..-MIDDLE 12,050 987.39 (0.44) 37.93 (11.70) 26.0 5,570.82 : : : : : : TOTAL 228,306 21,754.23 (9.83) 1,019.20 (281.28) 21.3 104,466.31 Table 2: Reduction of features by XQK-FS word class number of features number of non-zero weights seconds OTHER 56,220 ? 21,852 (38.9%) 1,512,827 ? 892,228 (59.0%) 42.31 ARTIFIFACT-MIDDLE 22,090 ? 4,410 (20.0%) 473,923 ? 164,632 (34.7%) 30.47 LOCATION-SINGLE 17,169 ? 3,382 (19.7%) 366,961 ? 123,808 (33.7%) 27.72 ORGANIZ..-MIDDLE 17,123 ? 9,959 (58.2%) 372,784 ? 263,695 (70.7%) 31.02 ORGANIZ..-END 15,214 ? 3,073 (20.2%) 324,514 ? 112,307 (34.6%) 26.87 : : : : TOTAL 307,721 ? 75,455 (24.5%) 6,669,664 ? 2,650,681 (39.7%) 763.10 The total number of features was reduced by 75%and that of weights was reduced by 60%. The ta ble also shows CPU time for classification by the selected features. XQK-FS is 28.5 (=21754.23/ 763.10) times faster than TinySVM. Although the reduction of features is significant, the reduction of CPU time is moderate, because most of the reducedfeatures are infrequent ones. However, simple re duction of infrequent features without consideringweights damages the system?s performance. For instance, when we removed 5,066 features that ap peared four times or less in the training data, themodified classifier for ORGANIZATION-END misclassified 103 training examples, whereas the origi nal classifier misclassified only 19 examples. On theother hand, XQK-FS removed 12,141 features with out an increase in misclassifications for the training data. XQK can be easily extended to a more generalquadratic kernel BG? ?*??vl??!?v  G ? and to nonbinary sparse vectors. XQK-FS can be used to se lect useful features before training by other kernels. As mentioned above, we conducted an experiment for the cubic kernel ( F *?? ) by using all features.When we trained the cubic kernel classifiers by us ing only features selected by XQK-FS, TinySVM?s classification time was reduced by 40% because  was reduced by 38%. GENERAL?s F-measure was slightly improved from 87.04% to 87.10%. Onthe other hand, when we trained the cubic ker nel classifiers by using only features that appeared three times or more (without considering weights), TinySVM?s classification time was reduced by only 14% and the F-measure was slightly degraded to86.85%. Therefore, we expect XQK-FS to be use ful as a feature selection method for other kernels when such kernels give much better results than the quadratic kernel.
Reduction of training time. . Since training of 33 classifiers also takes a longtime, it is difficult to try various combinations of pa rameters and features. Here, we present a solution for this problem. In the training time, calculation of B???Dr   B??$Dr ?  B??D@  for various  ? s is dominant. Conventional systems save time by caching the results. By analyzing TinySVM?s classifier, we found that they can be calculated more efficiently. For sparse vectors, most SVM classifiers (e.g., SVM-Light) use a sparse dot product algorithm (Platt, 1999) that compares non-zero elements of  and those of 7  to get BED7  in +-) . However,  is common to all dot products in B?D7   BD 7/ . Therefore, we can implement a faster classifierthat calculates them concurrently. TinySVM?s clas sifier prepares a list fi2si Z?? ] that contains all 7  s whose ? -th coordinates are not zero. In addition, counters for ?D%7  p ?D%7 / are prepared because dot products of binary vectors are integers. Then, for each non-zero G[Z?? ] , the counters are incremented for all 7   fi2si Z???] By checking only members of fi2si Z?? ] for non-zero G[Z?? ] , the classifier is not bothered by fruitless cases: G?Z?? ]?*?Q ?8Z???]??*YQ orG[Z???]W?*?Q ? ?Z???]?*yQ . Therefore, TinySVM?s clas sifier is faster than other classifiers. This method is applicable to any kernels based on dot products. For the training phase, we can build fi2si ? Z???] that contains all   s whose ? -th coordinates are notzero. Then, B??D   B???D  can be efficiently calculated because ?? is common. This im provement is effective especially when the cache is small and/or the training data is large. When we used a 200 MB cache, the improved system took only 13 hours for training by the CRL data, while TinySVM and SVM-Light took 30 hours and 46hours respectively for the same cache size. Al though we have examined other SVM toolkits, we could not find any system that uses this approach in the training phase.
Discussion. . The above methods can also be applied to othertasks in natural language processing such as chunk ing and POS tagging because the quadratic kernels give good results. Utsuro et al (2001) report that a combination of two NE recognizers attained F = 84.07%, butwrong word boundary cases are excluded. Our system attained 85.04% and word boundaries are auto matically adjusted. Yamada (Yamada et al, 2001) also reports that F*?? is best. Although his sys tem attained F = 83.7% for 5-fold cross-validation of the CRL data (Yamada and Matsumoto, 2001), our system attained 86.8%. Since we followedIsozaki?s implementation (Isozaki, 2001), our system is different from Yamada?s system in the fol lowing points: 1) adjustment of word boundaries, 2)ChaSen?s parameters for unknown words, 3) char acter types, 4) use of the Viterbi search. For efficient classification, Burges and Scho?lkopf (1997) propose an approximation method that uses ?reduced set vectors? instead of support vectors. Since the size of the reduced set vectors is smaller than  , classifiers become more efficient, but the computational cost to determine the vectors is verylarge. Osuna and Girosi (1999) propose two meth ods. The first method approximates +-) by support vector regression, but this method is applicable onlywhen S is large enough. The second method reformulates the training phase. Our approach is sim pler than these methods. Downs et al (Downs et al, 2001) try to reduce the number of support vectors by using linear dependence. We can also reduce the run-time complexity of a multi-class problem by cascading SVMs in the form of a binary tree (Schwenker, 2001) or a directacyclic graph (Platt et al, 2000). Yamada and Mat sumoto (2001) applied such a method to their NEsystem and reduced its CPU time by 39%. This ap proach can be combined with our SVM classifers.NE recognition can be regarded as a variablelength multi-class problem. For this kind of prob lem, probability-based kernels are studied for more theoretically well-founded methods (Jaakkola and Haussler, 1998; Tsuda et al, 2001; Shimodaira et al., 2001).
Conclusions. . Our SVM-based NE recognizer attained F = 90.03%. This is the best score, as far as we know. Since it was too slow, we made SVMs faster. The improved classifier is 21 times faster than TinySVMand 102 times faster than SVM-Light. The im proved training program is 2.3 times faster than TinySVM and 3.5 times faster than SVM-Light. We also presented an SVM-based feature selectionmethod that removed 75% of features. These methods can also be applied to other tasks such as chunk ing and POS tagging. AcknowledgmentWe would like to thank Yutaka Sasaki for the training data. We thank members of Knowledge Pro cessing Research Group for valuable comments and discussion. We also thank Shigeru Katagiri and Ken-ichiro Ishii for their support.