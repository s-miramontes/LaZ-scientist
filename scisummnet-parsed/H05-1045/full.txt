Introduction. In recent years, there has been a great deal of interest in methods for automatically identifying opin ions, emotions, and sentiments in text. Much of this research explores sentiment classification, a text categorization task in which the goal is to classifya document as having positive or negative polar ity (e.g., Das and Chen (2001), Pang et al (2002), Turney (2002), Dave et al (2003), Pang and Lee(2004)). Other research efforts analyze opinion expressions at the sentence level or below to recog nize opinions, their polarity, and their strength (e.g., Dave et al (2003), Pang and Lee (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Wiebeand Riloff (2005)). Many applications could benefit from these opinion analyzers, including prod uct reputation tracking (e.g., Morinaga et al (2002), Yi et al (2003)), opinion-oriented summarization (e.g., Cardie et al (2004)), and question answering (e.g., Bethard et al (2004), Yu and Hatzivassiloglou (2003)). We focus here on another aspect of opinion analysis: automatically identifying the sources of the opinions. Identifying opinion sources willbe especially critical for opinion-oriented questionanswering systems (e.g., systems that answer ques tions of the form ?How does [X] feel about [Y]??) and opinion-oriented summarization systems, both of which need to distinguish the opinions of one source from those of another.1 The goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text. To illustrate the nature of this problem, consider the examples below: S1: Taiwan-born voters favoring independence... 1In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and Riloff (2005), Wilson et al (2005)) and the nesting structure of sources (e.g., Breck and Cardie (2004)). The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus. 355 S2: According to the report, the human rights record in China is horrendous. S3: International officers believe that the EU will prevail. S4: International officers said US officials want the EU to prevail.In S1, the phrase ?Taiwan-born voters? is the direct (i.e., first-hand) source of the ?favoring? sen timent. In S2, ?the report? is the direct source of the opinion about China?s human rights record. In S3, ?International officers? are the direct source of an opinion regarding the EU. The same phrase in S4, however, denotes an indirect (i.e., second-hand, third-hand, etc.) source of an opinion whose direct source is ?US officials?. In this paper, we view source identification as an information extraction task and tackle the problemusing sequence tagging and pattern matching tech niques simultaneously. Using syntactic, semantic, and orthographic lexical features, dependency parse features, and opinion recognition features, we train alinear-chain Conditional Random Field (CRF) (Lafferty et al, 2001) to identify opinion sources. In ad dition, we employ features based on automaticallylearned extraction patterns and perform feature in duction on the CRF model. We evaluate our hybrid approach using the NRRC corpus (Wiebe et al, 2005), which is manually annotated with direct and indirect opinion source information. Experimental results show that theCRF model performs well, and that both the extraction patterns and feature induction produce perfor mance gains. The resulting system identifies opinionsources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% pre cision and 60.6% recall using an overlap measure.
The Big Picture. . The goal of information extraction (IE) systems is to extract information about events, including the participants of the events. This task goes beyond Named Entity recognition (e.g., Bikel et al (1997))because it requires the recognition of role relationships. For example, an IE system that extracts information about corporate acquisitions must distinguish between the company that is doing the acquiring and the company that is being acquired. Sim ilarly, an IE system that extracts information about terrorism must distinguish between the person who is the perpetrator and the person who is the victim.We hypothesized that IE techniques would be well suited for source identification because an opinion statement can be viewed as a kind of speech event with the source as the agent. We investigate two very different learning-basedmethods from information extraction for the problem of opinion source identification: graphical mod els and extraction pattern learning. In particular, we consider Conditional Random Fields (Lafferty et al, 2001) and a variation of AutoSlog (Riloff, 1996a).CRFs have been used successfully for Named En tity recognition (e.g., McCallum and Li (2003),Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in sev eral domains (Riloff, 1996a). While CRFs treatsource identification as a sequence tagging task, Au toSlog views the problem as a pattern-matching task, acquiring symbolic patterns that rely on both thesyntax and lexical semantics of a sentence. We hy pothesized that a combination of the two techniques would perform better than either one alone.Section 3 describes the CRF approach to identify ing opinion sources and the features that the systemuses. Section 4 then presents a new variation of Au toSlog, AutoSlog-SE, which generates IE patterns toextract sources. Section 5 describes the hybrid sys tem: we encode the IE patterns as additional features in the CRF model. Finally, Section 6 presents our experimental results and error analysis.
Semantic Tagging via Conditional. . Random FieldsWe defined the problem of opinion source identification as a sequence tagging task via CRFs as fol lows. Given a sequence of tokens, x = x1x2...xn, we need to generate a sequence of tags, or labels, y = y1y2...yn. We define the set of possible labelvalues as ?S?, ?T?, ?-?, where ?S? is the first to ken (or Start) of a source, ?T? is a non-initial token (i.e., a conTinuation) of a source, and ?-? is a token that is not part of any source.2 A detailed description of CRFs can be found in2This is equivalent to the IOB tagging scheme used in syn tactic chunkers (Ramshaw and Marcus, 1995). 356 Lafferty et al (2001). For our sequence tagging problem, we create a linear-chain CRF based on an undirected graph G = (V,E), where V is the set of random variables Y = {Yi|1 ? i ? n}, one for each of n tokens in an input sentence; and E = {(Yi?1, Yi)|1 < i ? n} is the set of n ? 1 edges forming a linear chain. For eachsentence x, we define a non-negative clique poten tial exp( ?K k=1 ?kfk(yi?1, yi, x)) for each edge, and exp(?K?k=1 ??kf ?k(yi, x)) for each node, where fk(...) is a binary feature indicator function, ?k is a weight assigned for each feature function, and K and K ? are the number of features defined for edges and nodes respectively. Following Lafferty et al (2001), the conditional probability of a sequence of labels y given a sequence of tokens x is: P (y|x) = 1Zx exp ? X i,k ?k fk(yi?1, yi, x)+ X i,k ??k f ?k(yi, x) ? (1) Zx = X y exp ? X i,k ?k fk(yi?1, yi, x) + X i,k ??k f ?k(yi, x) ? (2) where Zx is a normalization constant for eachx. Given the training data D, a set of sentences paired with their correct ?ST-? source la bel sequences, the parameters of the model are trained to maximize the conditional log-likelihood ? (x,y)?D P (y|x). For inference, given a sentence x in the test data, the tagging sequence y is given by argmaxy?P (y?|x). 3.1 Features. To develop features, we considered three properties of opinion sources. First, the sources of opinions are mostly noun phrases. Second, the source phrases should be semantic entities that can bear or expressopinions. Third, the source phrases should be directly related to an opinion expression. When con sidering only the first and second criteria, this task reduces to named entity recognition. Because of thethird condition, however, the task requires the recognition of opinion expressions and a more sophisticated encoding of sentence structure to capture relationships between source phrases and opinion ex pressions.With these properties in mind, we define the fol lowing features for each token/word xi in an input sentence. For pedagogical reasons, we will describesome of the features as being multi-valued or cate gorical features. In practice, however, all features are binarized for the CRF model.Capitalization features We use two boolean fea tures to represent the capitalization of a word: all-capital, initial-capital.Part-of-speech features Based on the lexical cat egories produced by GATE (Cunningham et al, 2002), each token xi is classified into one of a set of coarse part-of-speech tags: noun, verb, adverb, wh-word, determiner, punctuation, etc. We do the same for neighboring words in a [?2,+2] window in order to assist noun phrase segmentation.Opinion lexicon features For each token xi, we in clude a binary feature that indicates whether or not the word is in our opinion lexicon ? a set of words that indicate the presence of an opinion. We do the same for neighboring words in a [?1,+1] window.Additionally, we include for xi a feature that in dicates the opinion subclass associated with xi, ifavailable from the lexicon. (e.g., ?bless? is clas sified as ?moderately subjective? according to the lexicon, while ?accuse? and ?berate? are classified more specifically as ?judgments?.) The lexicon is initially populated with approximately 500 opinionwords 3 from (Wiebe et al, 2002), and then aug mented with opinion words identified in the training data. The training data contains manually producedphrase-level annotations for all expressions of opinions, emotions, etc. (Wiebe et al, 2005). We col lected all content words that occurred in the training set such that at least 50% of their occurrences were in opinion annotations. Dependency tree features For each token xi, we create features based on the parse tree produced by the Collins (1999) dependency parser. The purposeof the features is to (1) encode structural informa tion, and (2) indicate whether xi is involved in any grammatical relations with an opinion word. Two pre-processing steps are required before features can be constructed: 3Some words are drawn from Levin (1993); others are fromFramenet lemmas (Baker et al 1998) associated with commu nication verbs. 357 1. Syntactic chunking. We traverse the depen-. dency tree using breadth-first search to identifyand group syntactically related nodes, producing a flatter, more concise tree. Each syntac tic ?chunk? is also assigned a grammatical role (e.g., subject, object, verb modifier, time,location, of-pp, by-pp) based on its con stituents. Possessives (e.g., ?Clinton?s idea?) and the phrase ?according to X? are handled as special cases in the chunking process. 2. Opinion word propagation. Although the. opinion lexicon contains only content wordsand no multi-word phrases, actual opinions of ten comprise an entire phrase, e.g., ?is really willing? or ?in my opinion?. As a result, wemark as an opinion the entire chunk that con tains an opinion word. This allows each tokenin the chunk to act as an opinion word for fea ture encoding.After syntactic chunking and opinion word propagation, we create the following dependency tree fea tures for each token xi: ? the grammatical role of its chunk ? the grammatical role of xi?1?s chunk ? whether the parent chunk includes an opinion word ? whether xi?s chunk is in an argument position with respect to the parent chunk ? whether xi represents a constituent boundarySemantic class features We use 7 binary fea tures to encode the semantic class of each word xi: authority, government, human, media, organization or company, proper name,and other. The other class captures 13 seman tic classes that cannot be sources, such as vehicle and time. Semantic class information is derived from named entity and semantic class labels assigned to xi by the Sundance shallow parser (Riloff, 2004). Sundance uses named entity recognition rules to label noun phrases as belonging to named entity classes, and assigns semantic tags to individual words based on a semantic dictionary. Table 1 shows the hierarchy that Sundance uses for semantic classes associatedwith opinion sources. Sundance is also used to rec ognize and instantiate the source extraction patterns PROPER NAMEAUTHORITY LOCATION CITY COUNTRY PLANET PROVINCE PERSON NAME PERSON DESC NATIONALITY TITLE COMPANY GOVERNMENT MEDIA ORGANIZATION HUMAN SOURCE Figure 1: The semantic hierarchy for opinion sources that are learned by AutoSlog-SE, which is described in the next section.
Semantic Tagging via Extraction. . PatternsWe also learn patterns to extract opinion sources using a statistical adaptation of the AutoSlog IE learning algorithm. AutoSlog (Riloff, 1996a) is a supervised extraction pattern learner that takes a train ing corpus of texts and their associated answer keys as input. A set of heuristics looks at the contextsurrounding each answer and proposes a lexico syntactic pattern to extract that answer from the text.The heuristics are not perfect, however, so the result ing set of patterns needs to be manually reviewed by a person. In order to build a fully automatic system that does not depend on manual review, we combinedAutoSlog?s heuristics with statistics from the an notated training data to create a fully automatic supervised learner. We will refer to this learner as AutoSlog-SE (Statistically Enhanced variation of AutoSlog). AutoSlog-SE?s learning process has three steps: Step 1: AutoSlog?s heuristics are applied to every noun phrase (NP) in the training corpus. Thisgenerates a set of extraction patterns that, col lectively, can extract every NP in the training corpus. Step 2: The learned patterns are augmented withselectional restrictions that semantically constrain the types of noun phrases that are legiti mate extractions for opinion sources. We used 358the semantic classes shown in Figure 1 as se lectional restrictions.Step 3: The patterns are applied to the training corpus and statistics are gathered about their extractions. We count the number of extractions that match annotations in the corpus (cor rect extractions) and the number of extractionsthat do not match annotations (incorrect extrac tions). These counts are then used to estimate the probability that the pattern will extract an opinion source in new texts: P (source | patterni) = correct sources correct sources + incorrect sources This learning process generates a set of extractionpatterns coupled with probabilities. In the next sec tion, we explain how these extraction patterns are represented as features in the CRF model.
Extraction Pattern Features for the CRF. . The extraction patterns provide two kinds of infor mation. SourcePatt indicates whether a wordactivates any source extraction pattern. For exam ple, the word ?complained? activates the pattern?<subj> complained? because it anchors the ex pression. SourceExtr indicates whether a word is extracted by any source pattern. For example, in thesentence ?President Jacques Chirac frequently complained about France?s economy?, the words ?President?, ?Jacques?, and ?Chirac? would all be ex tracted by the ?<subj> complained? pattern.Each extraction pattern has frequency and prob ability values produced by AutoSlog-SE, hence we create four IE pattern-based features for each token xi: SourcePatt-Freq, SourceExtr-Freq, SourcePatt-Prob, and SourceExtr-Prob, where the frequency values are divided into threeranges: {0, 1, 2+} and the probability values are di vided into five ranges of equal size.
Experiments. . We used the Multi-Perspective Question Answering (MPQA) corpus4 for our experiments. This corpus 4The MPQA corpus can be freely obtained at http://nrrc.mitre.org/NRRC/publications.htm.consists of 535 documents that have been manually annotated with opinion-related information in cluding direct and indirect sources. We used 135 documents as a tuning set for model development and feature engineering, and used the remaining 400 documents for evaluation, performing 10-fold crossvalidation. These texts are English language ver sions of articles that come from many countries and cover many topics.5We evaluate performance using 3 measures: over lap match (OL), head match (HM), and exact match(EM). OL is a lenient measure that considers an extraction to be correct if it overlaps with any of the an notated words. HM is a more conservative measure that considers an extraction to be correct if its head matches the head of the annotated source. We reportthese somewhat loose measures because the annota tors vary in where they place the exact boundaries of a source. EM is the strictest measure that requires an exact match between the extracted words and the annotated words. We use three evaluation metrics:recall, precision, and F-measure with recall and pre cision equally weighted. 6.1 Baselines. We developed three baseline systems to assess the difficulty of our task. Baseline-1 labels as sources all phrases that belong to the semantic categories authority, government, human, media, organization or company, proper name.Table 1 shows that the precision is poor, suggest ing that the third condition described in Section 3.1 (opinion recognition) does play an important role in source identification. The recall is much higher butstill limited due to sources that fall outside of the semantic categories or are not recognized as belong ing to these categories. Baseline-2 labels a noun phrase as a source if any of the following are true: (1) the NP is the subject of a verb phrase containing an opinion word, (2) the NP follows ?according to?, (3) the NP contains a possessive and is preceded byan opinion word, or (4) the NP follows ?by? and at taches to an opinion word. Baseline-2?s heuristicsare designed to address the first and the third condi tions in Section 3.1. Table 1 shows that Baseline-2 is substantially better than Baseline-1. Baseline-35This data was obtained from the Foreign Broadcast Infor mation Service (FBIS), a U.S. government agency. 359 Recall Prec F1 OL 77.3 28.8 42.0 Baseline-1 HM 71.4 28.6 40.8 EM 65.4 20.9 31.7 OL 62.4 60.5 61.4 Baseline-2 HM 59.7 58.2 58.9 EM 50.8 48.9 49.8 OL 49.9 72.6 59.2 Baseline-3 HM 47.4 72.5 57.3 EM 44.3 58.2 50.3 OL 48.5 81.3 60.8 Extraction Patterns HM 46.9 78.5 58.7 EM 41.9 70.2 52.5 CRF: OL 56.1 81.0 66.3 basic features HM 55.1 79.2 65.0 EM 50.0 72.4 59.2 CRF: OL 59.1 82.4 68.9 basic + IE pattern HM 58.1 80.5 67.5 features EM 52.5 73.3 61.2 CRF-FI: OL 57.7 80.7 67.3 basic features HM 56.8 78.8 66.0 EM 51.7 72.4 60.3 CRF-FI: OL 60.6 81.2 69.4 basic + IE pattern HM 59.5 79.3 68.0 features EM 54.1 72.7 62.0 Table 1: Source identification performance table labels a noun phrase as a source if it satisfies both Baseline-1 and Baseline-2?s conditions (this should satisfy all three conditions described in Section 3.1). As shown in Table 1, the precision of this approach is the best of the three baselines, but the recall is the lowest. 6.2 Extraction Pattern Experiment. We evaluated the performance of the learned extrac tion patterns on the source identification task. The learned patterns were applied to the test data and the extracted sources were scored against the manualannotations.6 Table 1 shows that the extraction pat terns produced lower recall than the baselines, but with considerably higher precision. These results show that the extraction patterns alone can identify 6These results were obtained using the patterns that had a probability > .50 and frequency > 1.nearly half of the opinion sources with good accu racy. 6.3 CRF Experiments. We developed our CRF model using the MALLET code from McCallum (2002). For training, we useda Gaussian prior of 0.25, selected based on the tuning data. We evaluate the CRF using the basic fea tures from Section 3, both with and without the IE pattern features from Section 5. Table 1 shows that the CRF with basic features outperforms all of thebaselines as well as the extraction patterns, achiev ing an F-measure of 66.3 using the OL measure, 65.0 using the HM measure, and 59.2 using theEM measure. Adding the IE pattern features fur ther increases performance, boosting recall by about3 points for all of the measures and slightly increas ing precision as well. CRF with feature induction. One limitation of log-linear function models like CRFs is that they cannot form a decision boundary from conjunctionsof existing features, unless conjunctions are explic itly given as part of the feature vector. For the task of identifying opinion sources, we observedthat the model could benefit from conjunctive fea tures. For instance, instead of using two separatefeatures, HUMAN and PARENT-CHUNK-INCLUDES OPINION-EXPRESSION, the conjunction of the two is more informative.For this reason, we applied the CRF feature in duction approach introduced by McCallum (2003). As shown in Table 1, where CRF-FI stands for theCRF model with feature induction, we see consistent improvements by automatically generating conjunctive features. The final system, which com bines the basic features, the IE pattern features, and feature induction achieves an F-measure of 69.4(recall=60.6%, precision=81.2%) for the OL measure, an F-measure of 68.0 (recall=59.5%, preci sion=79.3%) for the HM measure, and an F-measure of 62.0 (recall=54.1%, precision=72.7%) for the EM measure. 6.4 Error Analysis. An analysis of the errors indicated some common mistakes: ? Some errors resulted from error propagation in 360our subsystems. Errors from the sentence bound ary detector in GATE (Cunningham et al, 2002) were especially problematic because they causedthe Collins parser to fail, resulting in no depen dency tree information. Some errors were due to complex and unusualsentence structure, which our rather simple fea ture encoding for CRF could not capture well. Some errors were due to the limited coverage of the opinion lexicon. We failed to recognize some cases when idiomatic or vague expressions were used to express opinions. Below are some examples of errors that we foundinteresting. Doubly underlined phrases indicate in correctly extracted sources (either false positives or false negatives). Opinion words are singly underlined. False positives: (1) Actually, these three countries do have one common denominator, i.e., that their values and policies do not agree with those of the United States and none of them are on good terms with the United States. (2) Perhaps this is why Fidel Castro has not spoken out against what might go on in Guantanamo.In (1), ?their values and policies? seems like a rea sonable phrase to extract, but the annotation does notmark this as a source, perhaps because it is some what abstract. In (2), ?spoken out? is negated, which means that the verb phrase does not bear an opinion, but our system failed to recognize the negation. False negatives: (3) And for this reason, too, they have a moral duty to speak out, as Swedish Foreign Minister Anna Lindh, among others, did yesterday. (4) In particular, Iran and Iraq are at loggerheads with each other to this day. Example (3) involves a complex sentence structure that our system could not deal with. (4) involves an uncommon opinion expression that our system did not recognize.
Related Work. . To our knowledge, our research is the first to auto matically identify opinion sources using the MPQAopinion annotation scheme. The most closely re lated work on opinion analysis is Bethard et al (2004), who use machine learning techniques to identify propositional opinions and their holders (sources). However, their work is more limited in scope than ours in several ways. Their work only addresses propositional opinions, which are?localized in the propositional argument? of certain verbs such as ?believe? or ?realize?. In con trast, our work aims to find sources for all opinions, emotions, and sentiments, including those that are not related to a verb at all. Furthermore, Berthardet al?s task definition only requires the identifica tion of direct sources, while our task requires the identification of both direct and indirect sources. Bethard et al evaluate their system on manuallyannotated FrameNet (Baker et al, 1998) and Prop Bank (Palmer et al, 2005) sentences and achieve 48% recall with 57% precision. Our IE pattern learner can be viewed as a crossbetween AutoSlog (Riloff, 1996a) and AutoSlog TS (Riloff, 1996b). AutoSlog is a supervised learner that requires annotated training data but does notcompute statistics. AutoSlog-TS is a weakly super vised learner that does not require annotated databut generates coarse statistics that measure each pattern?s correlation with relevant and irrelevant docu ments. Consequently, the patterns learned by bothAutoSlog and AutoSlog-TS need to be manually re viewed by a person to achieve good accuracy. In contrast, our IE learner, AutoSlog-SE, computes statistics directly from the annotated training data, creating a fully automatic variation of AutoSlog.
Conclusion. . We have described a hybrid approach to the problem of extracting sources of opinions in text. We cast this problem as an information extraction task, using both CRFs and extraction patterns. Our research is the first to identify both direct and indirect sources for all types of opinions, emotions, and sentiments.Directions for future work include trying to in crease recall by identifying relationships between opinions and sources that cross sentence boundaries,and relationships between multiple opinion expres sions by the same source. For example, the fact that a coreferring noun phrase was marked as a source in one sentence could be a useful clue for extracting the source from another sentence. The probability or the strength of an opinion expression may also play a useful role in encouraging or suppressing source extraction. 361
Acknowledgments. . We thank the reviewers for their many helpful com ments, and the Cornell NLP group for their advice and suggestions for improvement. This work wassupported by the Advanced Research and Develop ment Activity (ARDA), by NSF Grants IIS-0208028 and IIS-0208985, and by the Xerox Foundation.