1 Introduction. Many natural language processing (NLP) applications require accurate noun phrase coreference resolution: They require a means for determining which noun phrases in a text or dialogue refer to the same real-world entity. The vast majority of algorithms for noun phrase coreference combine syntactic and, less often, semantic cues via a set of hand-crafted heuristics and filters. All but one system in the MUC-6 coreference performance evaluation (MUC, 1995), for example, handled coreference resolution in this manner. This same reliance on complicated hand-crafted algorithms is true even for the narrower task of pronoun resolution. Some exceptions exist, however. Ge et al. (1998) present a probabilistic model for pronoun resolution trained on a small subset of the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). Dagan and Itai (1991) develop a statistical filter for resolution of the pronoun &quot;it&quot; that selects among syntactically viable antecedents based on relevant subject-verb-object cooccurrences. Aone and Bennett (1995) and McCarthy and Lehnert (1995) employ decision tree algorithms to handle a broader subset of general noun phrase coreference problems. This paper presents a new corpus-based approach to noun phrase coreference. We believe that it is the first such unsupervised technique developed for the general noun phrase coreference task. In short, we view the task of noun phrase coreference resolution as a clustering task. First, each noun phrase in a document is represented as a vector of attribute-value pairs. Given the feature vector for each noun phrase, the clustering algorithm coordinates the application of context-independent and context-dependent coreference constraints and preferences to partition the noun phrases into equivalence classes, one class for each real-world entity mentioned in the text. Context-independent coreference constraints and preferences are those that apply to two noun phrases in isolation. Context-dependent coreference decisions, on the other hand, consider the relationship of each noun phrase to surrounding noun phrases. In an evaluation on the MUC-6 coreference resolution corpus, our clustering approach achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC6 evaluation. More importantly, the clustering approach outperforms the only MUC-6 system to view coreference resolution as a learning problem: The RESOLVE system (McCarthy and Lehnert, 1995) employs decision tree induction and achieves an Fmeasure of 47% on the MUC-6 data set. Furthermore, our approach has a number of important advantages over existing learning and non-learning methods for coreference resolution: As a result, we believe that viewing noun phrase coreference as clustering provides a promising framework for corpus-based coreference resolution. The remainder of the paper describes the details of our approach. The next section provides a concrete specification of the noun phrase coreference resolution task. Section 3 presents the clustering algorithm. Evaluation of the approach appears in Section 4. Qualitative and quantitative comparisons to related work are included in Section 5.
2 Noun Phrase Coreference. It is commonly observed that a human speaker or author avoids repetition by using a variety of noun phrases to refer to the same entity. While human audiences have little trouble mapping a collection of noun phrases onto the same entity, this task of noun phrase (NP) coreference resolution can present a formidable challenge to an NLP system. Figure 1 depicts a typical coreference resolution system, which takes as input an arbitrary document and produces as output the appropriate coreference equivalence classes. The subscripted noun phrases in the sample output constitute two noun phrase coreference equivalence classes: Class JS contains the five noun phrases that refer to John Simon, and class PC contains the two noun phrases that represent Prime Corp. The figure also visually links neighboring coreferent noun phrases. The remaining (unbracketed) noun phrases have no coreferent NPs and are considered singleton equivalence classes. Handling the JS class alone requires recognizing coreferent NPs in appositive and genitive constructions as well as those that occur as proper names, possessive pronouns, and definite NPs.
3 Coreference as Clustering. Our approach to the coreference task stems from the observation that each group of coreferent noun phrases defines an equivalence classl. Therefore, it is natural to view the problem as one of partitioning, or clustering, the noun phrases. Intuitively, all of the noun phrases used to describe a specific concept will be &quot;near&quot; or related in some way, i.e. their conceptual &quot;distance&quot; will be small. Given a description of each noun phrase and a method for measuring the distance between two noun phrases, a clustering algorithm can then group noun phrases together: Noun phrases with distance greater than a clustering radius r are not placed into the same partition and so are not considered coreferent. The subsections below describe the noun phrase representation, the distance metric, and the clustering algorithm in turn. Given an input text, we first use the Empire noun phrase finder (Cardie and Pierce, 1998) to locate all noun phrases in the text. Note that Empire identifies only base noun phrases, i.e. simple noun phrases that contain no other smaller noun phrases within them. For example, Chief Financial Officer of Prime Corp. is too complex to be a base noun phrase. It contains two base noun phrases: Chief Financial Officer and Prime Corp. Each noun phrase in the input text is then represented as a set of 11 features as shown in Table 1. This noun phrase representation is a first approximation to the feature vector that would be required for accurate coreference resolution. All feature values are automatically generated and, therefore, are not always perfect. In particular, we use very simple heuristics to approximate the behavior of more complex feature value computations: Individual Words. The words contained in the noun phrase are stored as a feature. Head noun. The last word in the noun phrase is considered the head noun. Position. Noun phrases are numbered sequentially, starting at the beginning of the document. Pronoun Type. Pronouns are marked as one of Nominative, Accusative, POSSessive, or AMBiguous (you and it). All other noun phrases obtain the value NONE for this feature. Article. Each noun phrase is marked INDEFinite (contains a or an), DEFinite (contains the), or NONE. Appositive. Here we use a simple, overly restrictive heuristic to determine whether or not the noun phrase is in a (post-posed) appositive construction: If the noun phrase is surrounded by commas, contains an article, and is immediately preceded by another noun phrase, then it is marked as an appositive. Number. If the head noun ends in an 's', then the noun phrase is marked PLURAL; otherwise, it is considered siNGular. Expressions denoting money, numbers, or percentages are also marked as PLURAL. Proper Name. Proper names are identified by looking for two adjacent capitalized words, optionally containing a middle initial. Semantic Class. Here we use WordNet (Fellbaum, 1998) to obtain coarse semantic information for the head noun. The head noun is characterized as one of TIME, CITY, ANIMAL, HUMAN, or OBJECT. If none of these classes pertains to the head noun, its immediate parent in the class hierarchy is returned as the semantic class, e.g. PAYMENT for the head noun pay in NP6 of Table 1. A separate algorithm identifies NUMBERS, MONEY, and COMPANYS. Gender. Gender (mAsculine, FEMinine, EITHER, or NEUTER) is determined using WordNet and (for proper names) a list of common first names. Animacy. Noun phrases classified as HUMAN or ANIMAL are marked ANIM; all other NPs are considered INANIM. Next, we define the following distance metric between two noun phrases: where F corresponds to the NP feature set described above; incompatibility f is a function that returns a value between 0 and 1 inclusive and indicates the degree of incompatibility of f for N Pi and NP; and w1 denotes the relative importance of compatibility w.r.t. feature f. The incompatibility functions and corresponding weights are listed in Table 2.2 In general, weights are chosen to represent linguistic knowledge about coreference. Terms with a weight of oo represent filters that rule out impossible antecedents: Two noun phrases can never corefer when they have incompatible values for that term's feature. In the current version of our system, the NUMBER, PROPER NAME, SEMANTIC CLASS, GENDER, and ANIMACY features operate as coreference filters. Conversely, terms with a weight of —oo force coreference between two noun phrases with compatible values for that term's feature. The APPOSITIVE and WORDS-SUBSTRING terms operate in this fashion in the current distance metric. Terms with a weight of r — the clustering radius threshold — implement a preference that two NPs not be coreferent if they are incompatible w.r.t. that term's feature. As will be explained below, however, two such NPs can be merged into the same equivalence class by the clustering algorithm if there is enough other evidence that they are similar (i.e. there are other, coreferent noun phrase(s) sufficiently close to both). All other terms obtain weights selected using the development corpus. Although additional testing is required, our current results indicate that these weights are sensitive to the distance metric, but probably not to the corpus. When computing a sum that involves both oo and —oo, we choose, the more conservative route, and the oo distance takes precedence (i.e. the two noun phrases are not Considered coreferent). An example of where this might occur is in the following sentence: [i Reardon Steel Co.] manufactures several thousand tons of [2 steel] each week. Here, NPi subsumes NP2, giving them a distance of —oo via the word substring term; however, NPi's semantic class is COMPANY, and NP2's class is OBJECT, generating a distance of oo via the semantic class feature. Therefore, dist(NP1,NP2) = oo and the two noun phrases are not considered coreferent. The coreference distance metric is largely contextindependent in that it determines the distance between two noun phrases using very little, if any, of their intervening or surrounding context. The clustering algorithm described below is responsible for coordinating these local coreference decisions across arbitrarily long contexts and, thus, implements a series of context-dependent coreference decisions. The clustering algorithm is given in Figure 2. Because noun phrases generally refer to noun phrases that precede them, we start at the end of the document and work backwards. Each noun phrase is compared to all preceding noun phrases. If the distance between two noun phrases is less than the clustering radius r, then their classes are considered for possible merging. Two coreference equivalence classes can be merged unless there exist any incompatible NPs in the classes to be merged. It is useful to consider the application of our algorithm to an excerpt from a document: [i The chairman] spoke with [2 Ms. White] yesterday. [3 He] ... The noun phrase instances for this fragment are shown in Table 3. Initially, NPI, NP2, and NP3 are all singletons and belong to coreference classes c2, and c3, respectively. We begin by considering NP3. Dist(N P2,N P3) = oo due to a mismatch on gender, so they are not considered for possible merging. Next, we calculate the distance from NPi to NP3. Pronouns are not expected to match when the words of two noun phrases are compared, so there is no penalty here for word (or head noun) mismatches. The penalty for their difference in position is dependent on the length of the document. For illustration, assume that this is less than r. Thus, dist(NPi, NP3) < r. Their coreference classes, c1 and c3, are then considered for merging. Because they are singleton classes, there is no additional possibility for conflict, and both noun phrases are merged into cl. We developed and evaluated the clustering approach to coreference resolution using the &quot;dry run&quot; and &quot;formal evaluation&quot; MUC-6 coreference corpora. Each corpus contains 30 documents that have been annotated with NP coreference links. We used the dryrun data for development of the distance measure and selection of the clustering radius r and reserved the formal evaluation materials for testing. All results are reported using the standard measures of recall and precision or F-measure (which combines recall and precision equally). They were calculated automatically using the MUC-6 scoring program (Vilain et al., 1995). Table 4 summarizes our results and compares them to three baselines. For each algorithm, we show the F-measure for the dryrun evaluation (column 2) and the formal evaluation (column 4). (The &quot;adjusted&quot; results are described below.) For the dryrun data set, the clustering algorithm obtains 48.8% recall and 57.4% precision. The formal evaluation produces similar scores: 52.7% recall and 54.6% precision. Both runs use r = 4, which was obtained by testing different values on the dryrun corpus. Table 5 summarizes the results on the dryrun data set for r values from 1.0 to 10.0.3 As expected, increasing r also increases recall, but decreases precision. Subsequent tests with different values for r on the formal evaluation data set also obtained optimal performance with r= 4. This provides partial support for our hypothesis that r need not be recalculated for new corpora. The remaining rows in Table 4 show the performance of the three baseline algorithms. The first baseline marks every pair of noun phrases as coreferent, i.e. all noun phrases in the document form one class. This baseline is useful because it establishes an upper bound for recall on our clustering algorithm (67% for the dryrun and 69% for the formal evaluation). The second baseline marks as coreferent any two noun phrases that have a word in common. The third baseline marks as coreferent any two noun phrases whose head nouns match. Although the baselines perform better one might expect (they outperform one MUC-6 system), the clustering algorithm performs significantly better. In part because we rely on base noun phrases, our Figure 2: Clustering Algorithm The algorithm then considers NP2. Dist(NPi, NP2) = 11.0 plus a small penalty for their difference in position. If this distance is > r, they will not be considered coreferent, and the resulting equivalence classes will be: {{The chairman, he}, {Ms. White}}. Otherwise, the distance is < r, and the algorithm considers c and c2 for merging. However, c1 contains NP3, and, as calculated above, the distance from NP2 to NP3 is oo. This incompatibility prevents the merging of ci and 02, so the resulting equivalence classes would still be {{The chairman, he}, {Ms. White}}. In this way, the equivalence classes grow in a flexible manner. In particular, the clustering algorithm automatically computes the transitive closure of the coreference relation. For instance, if dist(NP,,NP3) < r and dist(NP3,NPk) < r then (assuming no incompatible NPs), NP, NP3, and NPk will be in the same class and considered mutually coreferent. In fact, it is possible that dist(NPz,NPk)> r, according to the distance measure; but as long as that distance is not oo, NP can be in the same class as NPk. The distance measure operates on two noun phrases in isolation, but the clustering algorithm can and does make use of intervening NP information: intervening noun phrases can form a chain that links otherwise distant NPs. By separating context-independent and recall levels are fairly low. The &quot;adjusted&quot; figures of Table 4 reflect this upper bound on recall. Considering only coreference links between base noun phrases, the clustering algorithm obtains a recall of 72.4% on the dryrun, and 75.9% on the formal evaluation. Another source of error is inaccurate and inadequate NP feature vectors. Our procedure for computing semantic class values, for example, is responsible for many errors - it sometimes returns incorrect values and the coarse semantic class distinctions are often inadequate. Without a better named entity finder, computing feature vectors for proper nouns is difficult. Other errors result from a lack of thematic and grammatical role information. The lack of discourse-related topic and focus information also limits system performance. In addition, we currently make no special attempt to handle reflexive pronouns and pleonastic &quot;it&quot;. Lastly, errors arise from the greedy nature of the clustering algorithm. Noun phrase NP., is linked to every preceding noun phrase N13, that is compatible and within the radius r, and that link can never be undone. We are considering three possible ways to make the algorithm less aggressively greedy. First, for each NP3, instead of considering every previous noun phrase, the algorithm could stop on finding the first compatible antecedent. Second, for each NPJ, the algorithm could rank all possible antecedents and then choose the best one and link only to that one. Lastly, the algorithm could rank all possible coreference links (all pairs of noun phrases in the document) and then proceed through them in ranked order, thus progressing from the links it is most confident about to those it is less certain of. Future work will include a more detailed error analysis.
5 Related Work. Existing systems for noun phrase coreference resolution can be broadly characterized as learning and non-learning approaches. All previous attempts to view coreference as a learning problem treat coreference resolution as a classification task: the algorithms classify a pair of noun phrases as coreferent or not. Both MLR (Aone and Bennett, 1995) and RESOLVE (McCarthy and Lehnert, 1995), for example, apply the C4.5 decision tree induction algorithm (Quinlan, 1992) to the task. As supervised learning algorithms, both systems require a fairly large amount of training data that has been annotated with coreference resolution information. Our approach, on the other hand, uses unsupervised learning4 and requires no training data.5 In addition, both MLR and RESOLVE require an additional mechanism to coordinate the collection of pairwise coreference decisions. Without this mechanism, it is possible that the decision tree classifies NP i and NP i as coreferent, and NP i and NPk as coreferent, but NP i and NPk as not coreferent. In an evaluation on the MUC-6 data set (see Table 6), RESOLVE achieves an F-measure of 47%. The MUC-6 evaluation also provided results for a large number of non-learning approaches to coreference resolution. Table 6 provides a comparison of our results to the best and worst of these systems. Most implemented a series of linguistic constraints similar in spirit to those employed in our system. The main advantage of our approach is that all constraints and preferences are represented neatly in the distance metric (and radius r), allowing for simple modification of this measure to incorporate new knowledge sources. In addition, we anticipate being able to automatically learn the weights used in the distance metric. There is also a growing body of work on the narrower task of pronoun resolution. Azzam et al. (1998), for example, describe a focus-based approach that incorporates discourse information when resolving pronouns. Lappin and Leass (1994) make use of a series of filters to rule out impossible antecedents, many of which are similar to our ooincompatibilities. They also make use of more extensive syntactic information (such as the thematic role each noun phrase plays), and thus require a fuller parse of the input text. Ge et al. (1998) present a supervised probabilistic algorithm that assumes a full parse of the input text. Dagan and Itai (1991) present a hybrid full-parse/unsupervised learning approach that focuses on resolving &quot;it&quot;. Despite a large corpus (150 million words), their approach suffers from sparse data problems, but works well when enough relevant data is available. Lastly, Cardie (1992a; 1992b) presents a case-based learning approach for relative pronoun disambiguation. Our clustering approach differs from this previous work in several ways. First, because we only require the noun phrases in any input text, we do not require a full syntactic parse. Although we would expect increases in performance if complex noun phrases were used, our restriction to base NPs does not reflect a limitation of the clustering algorithm (or the distance metric), but rather a self-imposed limitation on the preprocessing requirements of the approach. Second, our approach is unsupervised and requires no annotation of training data, nor a large corpus for computing statistical occurrences. Finally, we handle a wide array of noun phrase coreference, beyond just pronoun resolution.
6 Conclusions and Future Work. We have presented a new approach to noun phrase coreference resolution that treats the problem as a clustering task. In an evaluation on the MUC6 coreference resolution data set, the approach achieves very promising results, outperforming the only other corpus-based learning approach and producing recall and precision scores that place it firmly between the best and worst coreference systems in the evaluation. In contrast to other approaches to coreference resolution, ours is unsupervised and offers several potential advantages over existing methods: no annotated training data is required, the distance metric can be easily extended to account for additional linguistic information as it becomes available to the NLP system, and the clustering approach provides a flexible mechanism for combining a variety of constraints and preferences to impose a partitioning on the noun phrases in a text into coreference equivalence classes. Nevertheless, the approach can be improved in a number of ways. Additional analysis and evaluation on new corpora are required to determine the generality of the approach. Our current distance metric and noun phrase instance representation are only first, and admittedly very coarse, approximations to those ultimately required for handling the wide variety of anaphoric expressions that comprise noun phrase coreference. We would also like to make use of cues from centering theory and plan to explore the possibility of learning the weights associated with each term in the distance metric. Our methods for producing the noun phrase feature vector are also overly simplistic. Nevertheless, the relatively strong performance of the technique indicates that clustering constitutes a powerful and natural approach to noun phrase coreference resolution.
7 Acknowledgments. This work was supported in part by NSF Grant IRI9624639 and a National Science Foundation Graduate fellowship. We would like to thank David Pierce for his formatting and technical advice.