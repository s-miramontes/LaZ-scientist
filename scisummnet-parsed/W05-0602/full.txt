Introduction. Most recent work in learning for semantic parsinghas focused on ?shallow? analysis such as seman tic role labeling (Gildea and Jurafsky, 2002). In thispaper, we address the more ambitious task of learning to map sentences to a complete formal meaning representation language (MRL). We consider two MRL?s that can be directly used to perform useful, complex tasks. The first is a Prolog-based language used in a previously-developed corpus of queries to a database on U.S. geography (Zelle and Mooney, 1996). The second MRL is a coaching language for robotic soccer developed for the RoboCup Coach Competition, in which AI researchers compete to provide effective instructions to a coachable team of agents in a simulated soccer domain (et al, 2003).We present an approach based on a statisti cal parser that generates a semantically augmentedparse tree (SAPT), in which each internal node includes both a syntactic and semantic label. We aug ment Collins? head-driven model 2 (Collins, 1997) to incorporate a semantic label on each internalnode. By integrating syntactic and semantic inter pretation into a single statistical model and finding the globally most likely parse, an accurate combined syntactic/semantic analysis can be obtained. Once a SAPT is generated, an additional step is required totranslate it into a final formal meaning representa tion (MR). Our approach is implemented in a system called SCISSOR (Semantic Composition that IntegratesSyntax and Semantics to get Optimal Representations). Training the system requires sentences an notated with both gold-standard SAPT?s and MR?s. We present experimental results on corpora for bothgeography-database querying and Robocup coaching demonstrating that SCISSOR produces more accurate semantic representations than several previ ous approaches based on symbolic learning (Tang and Mooney, 2001; Kate et al, 2005).
Target MRL?s. . We used two MRLs in our experiments: CLANG and GEOQUERY. They capture the meaning of linguistic utterances in their domain in a formal language. 9 2.1 CLANG: the RoboCup Coach Language. RoboCup (www.robocup.org) is an interna tional AI research initiative using robotic soccer as its primary domain. In the Coach Competition, teams of agents compete on a simulated soccer field and receive advice from a team coach in a formal language called CLANG. In CLANG, tactics and behaviors are expressed in terms of if-then rules. As described in (et al, 2003), its grammar consists of 37 non-terminal symbols and 133 productions. Below is a sample rule with its English gloss: ((bpos (penalty-area our)) (do (player-except our {4}) (pos (half our)))) ?If the ball is in our penalty area, all our players except player 4 should stay in our half.? 2.2 GEOQUERY: a DB Query Language. GEOQUERY is a logical query language for a small database of U.S. geography containing about 800 facts. This domain was originally chosen to testcorpus-based semantic parsing due to the avail ability of a hand-built natural-language interface, GEOBASE, supplied with Turbo Prolog 2.0 (Borland International, 1988). The GEOQUERY language consists of Prolog queries augmented with several meta-predicates (Zelle and Mooney, 1996). Below is a sample query with its English gloss: answer(A,count(B,(city(B),loc(B,C), const(C,countryid(usa))),A)) ?How many cities are there in the US??
Semantic Parsing Framework. . This section describes our basic framework for semantic parsing, which is based on a fairly standard approach to compositional semantics (Juraf sky and Martin, 2000). First, a statistical parseris used to construct a SAPT that captures the se mantic interpretation of individual words and the basic predicate-argument structure of the sentence.Next, a recursive procedure is used to composition ally construct an MR for each node in the SAPT from the semantic label of the node and the MR?s has2 VP?bowner player the ball NN?player CD?unum NP?null NN?null VB?bowner S?bowner NP?player DT?null PRP$?team our Figure 1: An SAPT for a simple CLANG sentence. Function:BUILDMR(N;K) Input: The root node N of a SAPT; predicate-argument knowledge, K, for the MRL. Notation: X MR is the MR of node X . Output: N MR C i := the ith child node of N; 1  i  n C h = GETSEMANTICHEAD(N ) // see Section 3 C h MR = BUILDMR(C h ; K) for each other child C i where i 6= h C i MR = BUILDMR(C i ; K) COMPOSEMR(C h MR , C i MR ; K) // see Section 3 N MR = C h MR Figure 2: Computing an MR from a SAPT.of its children. Syntactic structure provides information of how the parts should be composed. Ambiguities arise in both syntactic structure and the semantic interpretation of words and phrases. By in tegrating syntax and semantics in a single statisticalparser that produces an SAPT, we can use both se mantic information to resolve syntactic ambiguitiesand syntactic information to resolve semantic ambi guities. In a SAPT, each internal node in the parse tree is annotated with a semantic label. Figure 1 showsthe SAPT for a simple sentence in the CLANG do main. The semantic labels which are shown afterdashes are concepts in the domain. Some type con cepts do not take arguments, like team and unum (uniform number). Some concepts, which we refer to as predicates, take an ordered list of arguments,like player and bowner (ball owner). The predicateargument knowledge, K , specifies, for each predicate, the semantic constraints on its arguments. Con straints are specified in terms of the concepts that can fill each argument, such as player(team, unum) and bowner(player). A special semantic label nullis used for nodes that do not correspond to any con cept in the domain.Figure 2 shows the basic algorithm for build ing an MR from an SAPT. Figure 3 illustrates the 10 player the ball N3?bowner(_)N7?player(our,2) N2?null null null N4?player(_,_) N5?team our N6?unum 2 N1?bowner(_) has N8?bowner(player(our,2)) Figure 3: MR?s constructed for each SAPT Node. construction of the MR for the SAPT in Figure 1.Nodes are numbered in the order in which the con struction of their MR?s are completed. The first step, GETSEMANTICHEAD , determines which of anode?s children is its semantic head based on hav ing a matching semantic label. In the example, node N3 is determined to be the semantic head of the sentence, since its semantic label, bowner, matchesN8?s semantic label. Next, the MR of the seman tic head is constructed recursively. The semantichead of N3 is clearly N1. Since N1 is a part-ofspeech (POS) node, its semantic label directly de termines its MR, which becomes bowner( ). Once the MR for the head is constructed, the MR of all other (non-head) children are computed recursively,and COMPOSEMR assigns their MR?s to fill the arguments in the head?s MR to construct the com plete MR for the node. Argument constraints areused to determine the appropriate filler for each ar gument. Since, N2 has a null label, the MR of N3 also becomes bowner( ). When computing the MR for N7, N4 is determined to be the head with the MR: player( , ). COMPOSEMR then assigns N5?s MR to fill the team argument and N6?s MR to fill the unum argument to construct N7?s complete MR: player(our, 2). This MR in turn is composed withthe MR for N3 to yield the final MR for the sen tence: bowner(player(our,2)). For MRL?s, such as CLANG, whose syntax doesnot strictly follow a nested set of predicates and ar guments, some final minor syntactic adjustment of the final MR may be needed. In the example, thefinal MR is (bowner (player our f2g)). In the fol lowing discussion, we ignore the difference between these two.There are a few complications left which re quire special handling when generating MR?s,like coordination, anaphora resolution and noncompositionality exceptions. Due to space limitations, we do not present the straightforward tech niques we used to handle them.
Corpus Annotation. . This section discusses how sentences for training SCISSOR were manually annotated with SAPT?s. Sentences were parsed by Collins? head-driven model 2 (Bikel, 2004) (trained on sections 02-21 of the WSJ Penn Treebank) to generate an initial syntactic parse tree. The trees were then manually corrected and each node augmented with a semantic label. First, semantic labels for individual words, called semantic tags, are added to the POS nodes in thetree. The tag null is used for words that have no cor responding concept. Some concepts are conveyedby phrases, like ?has the ball? for bowner in the pre vious example. Only one word is labeled with the concept; the syntactic head word (Collins, 1997) is preferred. During parsing, the other words in thephrase will provide context for determining the se mantic label of the head word. Labels are added to the remaining nodes in abottom-up manner. For each node, one of its chil dren is chosen as the semantic head, from which it will inherit its label. The semantic head is chosen as the child whose semantic label can take the MR?s of the other children as arguments. This step wasdone mostly automatically, but required some man ual corrections to account for unusual cases. In order for COMPOSEMR to be able to construct the MR for a node, the argument constraints for its semantic head must identify a unique concept to fill each argument. However, some predicates take multiple arguments of the same type, such as point.num(num,num), which is a kind of point that represents a field coordinate in CLANG. In this case, extra nodes are inserted in the treewith new type concepts that are unique for each ar gument. An example is shown in Figure 4 in whichthe additional type concepts num1 and num2 are in troduced. Again, during parsing, context will be used to determine the correct type for a given word. The point label of the root node of Figure 4 is the concept that includes all kinds of points in CLANG. Once a predicate has all of its arguments filled, we 11 ,0.5 , ?RRB? ?RRB??null ?LRB? 0.1 CD?num CD?num ?LRB??point.num PRN?point CD?num1 CD?num2Figure 4: Adding new types to disambiguate argu ments. use the most general CLANG label for its concept (e.g. point instead of point.num). This generality avoids sparse data problems during training.
Integrated Parsing Model. . 5.1 Collins Head-Driven Model 2. Collins? head-driven model 2 is a generative, lexi calized model of statistical parsing. In the following section, we follow the notation in (Collins, 1997). Each non-terminal X in the tree is a syntactic label, which is lexicalized by annotating it with a word, w, and a POS tag, t syn . Thus, we write a non-. terminal as X(x), where X is a syntactic label and x = hw; t syn i. X(x) is then what is generated by the generative model. Each production LHS ) RHS in the PCFG is in the form: P (h)!L n (l n ):::L 1 (l 1 )H(h)R 1 (r 1 ):::R m (r m )where H is the head-child of the phrase, which in herits the head-word h from its parent P . L 1 :::L n and R 1 :::R m are left and right modifiers of H . Sparse data makes the direct estimation ofP(RHSjLHS) infeasible. Therefore, it is decom posed into several steps ? first generating the head, then the right modifiers from the head outward, then the left modifiers in the same way. Syntactic subcategorization frames, LC and RC, for the leftand right modifiers respectively, are generated be fore the generation of the modifiers. Subcat framesrepresent knowledge about subcategorization preferences. The final probability of a production is com posed from the following probabilities: 1. The probability of choosing a head constituent. label H: P h (HjP; h). 2. The probabilities of choosing the left and right. subcat frames LC and RC: P l (LCjP;H; h) and P r (RCjP;H; h). has2our player the PRP$?team NN?player CD?unum NN?nullDT?null NP?player(player) VP?bowner(has) NP?null(ball) ball S?bowner(has) VB?bowner Figure 5: A lexicalized SAPT. 3. The probabilities of generat-. ing the left and right modifiers: Q i=1::m+1 P r (R i (r i )jH;P; h; i