. lit Section 4, we consider the implications of our experimental results and discuss future work.
2 The Direct Correspondence Assumption. To our knowledge, the direct correspondence assumption underlies all statistical models that attempt to capture a relationship between syntactic structures in two languages, be they constituent models or dependency models. As art example of the former, consider Wu's (1995) stochastic inversion transduction grammar (SITG), in which paired sentences are simultaneously generated using context-free rules; word order differences are accounted for by allowing each rule to be read in a left-toright or right-to-left fashion, depending ort the language. For example, SITG can generate verb initial (English) and verb final (Japanese) verb phrases using the same rule VP V NP. For arty derivation using this rule, if VE and NPE are the English verb and noun phrase, and they are respectively aligned with Japanese verb and noun phrase v j and NP j, then VERB-OBJECT(VE, NPE) and VERB-OBJECT(V j, NP j) mug both be true. As art example where the DCA relates dependency structures, consider the hierarchical alignment algorithm proposed by Alshawi et al. (2000). In this framework, wordlevel alignments and paired dependency structures are constructed simultaneously. The English-Basque example (1) illustrates: if the English word buy is aligned to the Basque word erosi and gift is aligned to opari, the creation of the head-modifier relationship between buy and gift is accompanied by the creation of a corresponding head-modifier relationship between erosi and opari. Let us formalize this intuitive idea about corresponding syntactic relationships in the following more general way:
Direct Correspondence Assumption. (DCA): Given a pair of sentences E and F that are (literal) translations of each other with syntactic structures TreeE and TreeF, if nodes E and YE of TreeE are aligned with nodes xF and YF of TreeF, respectively, and if syntactic relationship R(x E, YE) holds in TreeE, then R(x F , YF) holds in TreeF. Here, R(x , y) may specify a head-modifier relationship between words in a dependency tree, or a sisterhood relationship between nonterminals in a constituency tree. As stated, the DCA amounts to art assumption that the crosslanguage alignment resembles a Itomomorphism relating the syntactic graph of E to the syntactic graph of F.2 Wu's SITG makes this assumption, under the interpretation that R is the head-modifier relation expressed in a rewrite rule. The IBM MT models (Brown et al., 1993) do not respect the DCA, but neither do they attempt to model arty higher level syntactic relationship between constituents within or across languages the translation model (alignments) and the language model are statistically independent. In Yamada and Knight's (2001) extension of the IBM models, on the other hand, grammatical information from the source language is propagated into the noisy channel, and the grammatical transformations in their channel model appear to respect direct correspondence.3 The simultaneous parsing and alignment algorithm of Alshawi et al. (2000) is essentially art implementation of the DCA in which relationship R has no linguistic import (i.e. anything can be a head). The DCA seems to be a reasonable principle, especially when expressed in terms of syntactic dependencies that abstract away word order. That is, the thematic (who-did-what-to-whom) relationships are likely to hold true across translations evert for typologically different languages. Consider example (1) again: despite the fact that the Basque sentence has a different word order, with the verb appearing at the far right of the sentence, the syntactic dependency relationships of English (subject, object, noun modifier, etc.) are largely preserved across the alignment, as illustrated in Table 1. Moreover, the DCA makes possible more elegant formalisms (e.g. SITG) and more efficient algorithms It may allow us to use the syntactic analysis for one language to infer annotations for the corresponding sentence in another language, helping to reduce the labor and expense of creating treebanks in new languages (Cabezas et al., 2001; Yarowsky and Ngai, 2001). Unfortunately, the DCA is flawed, even for literal translations. For example, in sentence pair (1), the indirect object of the verb is expressed in English using a prepositional phrase (headed by the word for) that attaches to the verb, but it is expressed with the dative case marking ort anaiari (BROTHER-DAT) it Basque. If we aligned both for and brother to anaiari, then a many-to-one mapping would be formed, and the DCA would be violated: R(f or, brother) holds in the English tree but R(analari,analari) does not hold in the Basque tree. Similarly, a one-to-many mapping (e.g., aligning got with erosi (Buy) and nion (PAST) in this example) can also be problematic for the DCA. The inadequacy of the DCA should come as no surprise. The syntax literature dating back to Cltomsky (1981), together with a rich computational literature on translation divergences (e.g. (Abeille et al., 1990; Dorr, 1994; Han et al., 2000)), is concerned with characterizing in a systematic way the apparent diversity of mechanisms used by languages to express meanings syntactically. For example, current theories claim that languages employ stable headcomplement orders across construction types. In English, the head of a phrase is uniformly to the left of modifying prepositional phrases, sentential complements, etc. In Chinese, verbal and prepositional phrases respect the English ordering but heads in the nominal system uniformly appear to the right. Systematic application of this sort of linguistic knowledge turns out to be the key in getting beyond the DCA's limitations.
3 Evaluating the DCA using Annotation Projection. Thus far, we have argued that the DCA is a useful and widely assumed principle; at the same time we have illustrated that it is incapable of accounting for some well known and fundamental linguistic facts. Yet this is not art unfamiliar situation. For years, stochastic modeling of language has depended on the linguistically implausible assumptions underlying'n-gram models, hidden Markov models, context-free grammars, and the like, with remarkable success. Having made the DCA explicit, we would suggest that the right questions are: to what extent is it true, and how useful is it when it holds? In the remainder of the paper, we focus on answering the first question empirically by considering the syntactic relationships and alignments between translated sentence pairs in two distant languages (English and Chinese). In our experimental framework, a system is given the &quot;ideal&quot; syntactic analyses for the English sentences and English-Chinese word-alignments, and it uses a Direct Projection Algorithm (described below) to project the English syntactic annotations directly across to the Chinese sentences in accordance with the DCA. The resulting Chinese dependency analyses are then compared with an independently derived gold standard, enabling us to determine recall and precision figures for syntactic dependencies (cf. (Lin, 1998)) and to perform a qualitative error analysis. This error analysis led us to revise our projection approach, and the resulting linguistically informed projection improved significantly the ability to obtain accurate Chinese parses. This experimental framework for the first question is designed with art eye toward the second, concerning the usefulness of making the direct correspondence assumption. If the DCA holds true more often than not, then one might speculate that the projected syntactic structures could be useful as a treebank (albeit a noisy one) for training Chinese parsers, and could help more generally in overcoming the syntactic annotation bottleneck for languages other than English. The DCA translates fairly directly into art algorithm for projecting English dependency analyses across to Chinese using word alignments as the bridge. More formally, given sentence pair (F, F), the English syntactic relations are projected for the following situations: tvi,, tvn,, then create a new empty word mF E F such that mF is the parent of tvi,, tvn, and set WE to align to m F instead. • many-to-one if wjE, wnE E E are all uniquely aligned to WF E F, then delete all alignments between wiE (1 < i < n) and WF except for the head (denoted as whE ); moreover, if wiE , a modifier of .tchE, had its own modifiers, R(tviE, wjE) R(tch, wJF). The many-to-many case is decomposed into a two-step process: first perform one-to-many, then perform many-to-one. In the cases of unaligned Chinese words, they are left out of the projected syntactic tree. The asymmetry in the treatment of one-to-many and many-to-one and of the unaligned words for the two languages arises from the asymmetric nature of the projection. The corpus for this experiment was constructed by obtaining manual English translations for 124 Chinese newswire sentences (with 40 words or less) contained in sections 001-015 of the Penn Chinese Treebank (Xia et al., 2000). The Chinese data in our set ranged from simple sentences to some complicated constructions such as complex relative clauses, multiple run-on clauses, embeddings, nominal constructions, etc. Average sentence length was 23.7 words. Parses for the English sentences were constructed by a process of automatic analysis followed by hand correction; output trees from a broad-coverage lexicalized English parser (Collins, 1997) were automatically converted into dependencies to be corrected. The goldstandard dependency analyses for the Chinese sentences were constructed manually by two fluent speakers of Chinese, working independently and using the Chinese Treebank's (manually constructed) constituency parses for reference.4 Inter-annotator agreement ort unlabeled syntactic dependencies is 92.4%. Manual EnglishChinese alignments were constructed by two annotators who are native speakers of Chinese using a software environment similar to that described by Melamed (1998). The direct projection of English dependencies to Chinese yielded poor results as measured by precision and recall over unlabeled syntactic dependencies: precision was 30.1% and recall 39.1%. Inspection of the results revealed that our manually aligned parallel corpus contained many instances of multiply aligned or unaligned tokens, owing either to freeness of translation 40ne author of this paper served as one of the annotators. (a violation of the assumption that translations are literal) or to differences in how the two languages express the same meaning. For example, to quantify a Chinese noun with a determiner, one also needs to supply a measure word in addition to the quantity. Thus, the noun phrase an apple is expressed as yee (AN) ge (-mEAs) ping-guo (APPLE). Chinese also includes separate words to indicate aspectual categories such as continued action, in contrast to verbal suffixes in English such as the -ing in running. Because Chinese classifiers, aspectual particles, and other functional words do not appear in the English sentence, there is no way for a projected English analysis to correctly account for them. As a result, the Chinese dependency trees usually fail to contain an appropriate grammatical relation for these items. Because they are frequent, the failure to properly account for them significantly hurts performance. Our error analysis led to the conclusion that the correspondence of syntactic relationships would be improved by a better handling of the one-tomany mappings and the unaligned cases. We investigated two ways of addressing this issue. First, we adopted a simple strategy informed by the tendency of languages to have a consistent direction for &quot;headedness&quot;. Chinese and English share the property that they are headinitial for most phrase types. Thus, if an English word aligns to multiple Chinese words cj, , cm, the leftmost word c1 is treated as the head and c2, ..., cm are analyzed as its dependents. If a Chinese empty node was introduced to align with an untranslated English word, it is deleted and its left-most child is promoted to replace it. Looking at language in this non-constructiondependent way allows us to make simple changes that have wide ranging effects. This is illustrative of how our approach tries to rein in cases where the DCA breaks down by using linguistically informed constraints that are as general as possible. Second, we used more detailed linguistic knowledge of Chinese to develop a small set of rules, expressed in a tree-based pattern-action formalism, that perform local modifications of a projected analysis on the Chinese side. To avoid the slippery slope of unending language-specific rule tweaking, we strictly constrained the possible rules. Rules were permitted to refer only to closed class items, to parts of speech projected from the English analysis, or to easily enumerated lexical categories (e.g. {dollar, RMB, $, For example, one such rule deals with noun modification: word nk to its place with nj, nk_j as dependents. Another deals with aspectual markers for verbs: • If vi, vk, a sequence of Chinese words aligned with English verbs, is followed by a, an aspect marker, make a into a modifier of the last verb vk. The most involved transformation places a linguistic constraint on the Chinese functional word de, which may be translated as that (the head of a relative clause), as the preposition of, or as 's (a marker for possessives). This common Chinese functional word is almost always either unaligned or multiply aligned to an English word. The latter two changes may seem unrelated, but they both take advantage of the fact that Chinese violates the head-initial rule in its nominal system, where noun phrases are uniformly head-final. More generally, the majority of rule patterns are variations on the same solution to the same problem. Viewing the problem from a higher level of linguistic abstraction made it possible to find all the relevant cases in a short time (a few days) and express the solution compactly (< 20 rules). The complete set of rules can be found in (Hwa et al., 2002). Because our error analysis and subsequent algorithm refinements made use of our original Chinese-English data set, we created a new test set based on 88 new Chinese sentences from the Penn Chinese Treebank, already manually translated into English as part of the NIST MT evaluation preview.5 These sentences averaged 19.0 words in length. As described above, parses ort the English side were created semi-automatically, and word alignments were acquired manually. However, in order to reduce our reliance ort linguistically sophisticated human annotators for Chinese syntax, we adopted art alternative strategy for obtaining the gold standard: we automatically converted the Treebank's constituency parses of the Chinese sentences into syntactic dependency representations, using art algorithm similar to the one described in Section 2 of the paper by Xia and Palmer (2001).6 The recall and precision figures for the new experiment are summarized in Table 2. The first row of the table shows the results comparing the output of the Direct Projection Algorithm with the gold standard. As we have already seen previously, the quality of these trees is not very good. The second row of the table shows that after applying the single transformation based ort the head-initial assumption, precision and recall both improve significantly: using the F-measure to combine precision and recall into a single figure of merit (Van Rijsbergen, 1979), the increase from 38.1% to 59.4% represents a 55.9% relative improvement. The third row of the table shows that by applying the small set of tree modification rules after direct projection (one of which is default assignment of the head-initial analysis to multi-word phrases when no other rule applies), we obtain art evert larger improvement, the 67.3% F-measure representing a 76.6% relative gain over baseline performance.
4 Conclusions and Future Work. To what extent is the DCA a valid assumption? Our experiments confirm the linguistic intuition, indicating that one cannot safely assume a direct mapping between the syntactic dependencies of one language and the syntactic dependencies of another. How useful is the DCA? The experimental results show that evert the simplistic DCA can be useful when operating in conjunction with small quantities of systematic linguistic knowledge. Syntactic analyses projected from English to Chinese can, in principle, yield Chinese analyses that are nearly 70% accurate (in terms of unlabeled dependencies) after application of a set of linguistically principled rules. In the near future we will address the remaining errors, which also seem to be amenable to a uniform linguistic treatment: in large part they involve differences in category expression (nominal expressions translated as verbs or vice versa) and we believe that we can use context to effect the correct category transformations. We will also explore correction of errors via statistical learning t echni goes . The implication of this work for statistical translation modeling is that a little bit of knowledge can be a good thing. The approach described here strikes a balance somewhere between the endless construction-by-construction tuning of rule-based approaches, ort the one hand, and, ort the other, the development of insufficiently constrained stochastic models. We have systematically diagnosed a common assumption that has been dealt with previously ort a case by case basis, but not named. Most of the models we know of from early work at IBM to second-generation models such as that of Knight and Yamada rectify glaring problems caused by the failure of the DCA using a range of pre- or post-processing techniques. We have identified the source for a host of these problems and have suggested diagnostics for future cases where we might expect these problems to arise. More important, we have shown that linguistically informed strategies can be developed efficiently to improve output that is otherwise compromised by situations where the DCA does not hold. In addition to resolving the remaining problematic cases for our projection framework, we are exploring ways to automatically create large quantities of syntactically annotated data. This will break the bottleneck in developing appropriately annotated training corpora. Currently, we are following two research directions. Our first goal is to minimize the degree of degradation in the quality of the projected trees when the input analyses and word alignments are automatically generated by a statistical parser and word alignment model. To improve the quality of the input analyses, we are adapting active learning and co-training techniques (Hwa, 2000; Sarkar, 2001) to exploit the most reliable data. We are also actively developing art alternative alignment model that makes more use of the syntactic structure (Lopez et al., 2002). Our second goal is to detect and reduce the noise in the projected trees so that they might replace the expensive human-annotated corpora as training examples for statistical parsers. We are investigating the use of filtering strategies to localize the potentially problematic parts of the projected syntactic trees.
Acknowledgments. This work has been supported, in part, by ONR MUM Contract FCP0.810548265, NSA RD02-5700, DARPA/ITO Cooperative Agreement N660010028910, and Mitre Contract 0104187712. The authors would like to thank Edward Hung, Gina Levow, and Lingling Zhang for their assistance as annotators; Michael Collins for the use of his parser; Franz Josef Och for his help with GIZA++; and Lillian Lee, the students of CM5C828, and the anonymous reviewers for their comments ort this paper.