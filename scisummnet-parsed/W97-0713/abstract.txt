We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &quot;greatest&quot; and &quot;significant&quot; or indiphrases as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important senuse words as &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries â€” an empirical view