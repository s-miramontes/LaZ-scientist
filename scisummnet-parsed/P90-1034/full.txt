I.. A variety of linguistic relations apply to sets of semantically similar words. For example, modifiers select semantically similar nouns, selectional restrictions are expressed in terms of the semantic class of objects, and semantic type restricts the possibilities for noun compounding. Therefore, it is useful to have a classification of words into semantically similar sets. Standard approaches to classifying nouns, in terms of an &quot;is-a&quot; hierarchy, have proven hard to apply to unrestricted language. Is-a hierarchies are expensive to acquire by hand for anything but highly restricted domains, while attempts to automatically derive these hierarchies from existing dictionaries have been only partially successful (Chodorow, Byrd, and Heidom 1985). This paper describes an approach to classifying English words according to the predicate-argument structures they show in a corpus of text. The general idea is straightforward: in any natural language there are restrictions on what words can appear together in the same construction, and in particular, on what can be arguments of what predicates. For nouns, there is a restricted set of verbs that it appears as subject of or object of. For example, wine may be drunk, produced, and sold but not pruned. Each noun may therefore be characterized according to the verbs that it occurs with. Nouns may then be grouped according to the extent to which they appear in similar environments. This basic idea of the distributional foundation of meaning is not new. Harris (1968) makes this &quot;distributional hypothesis&quot; central to his linguistic theory. His claim is that: &quot;the meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities.&quot; (Harris 1968:12). Sparck Jones (1986) takes a similar view. It is however by no means obvious that the distribution of words will directly provide a useful semantic classification, at least in the absence of considerable human intervention. The work that has been done based on Harris' distributional hypothesis (most notably, the work of the associates of the Linguistic String Project (see for example, Hirschman, Grishrnan, and Sager 1975)) unfortunately does not provide a direct answer, since the corpora used have been small (tens of thousands of words rather than millions) and the analysis has typically involved considerable intervention by the researchers. The stumbling block to any automatic use of distributional patterns has been that no sufficiently robust syntactic analyzer has been available. This paper reports an investigation of automatic distributional classification of words in English, using a parser developed for extracting grammatical structures from unrestricted text (Hindle 1983). We propose a particular measure of similarity that is a function of mutual information estimated from text. On the basis of a six million word sample of Associated Press news stories, a classification of nouns was developed according to the predicates they occur with. This purely syntax-based similarity measure shows remarkably plausible semantic relations.
2.. A 6 million word sample of Associated Press news stories was analyzed, one sentence at a time, by a deterministic parser (Fidditch) of the sort originated by Marcus (1980). Fidditch provides a single syntactic analysis -- a tree or sequence of trees -- for each sentence; Figure 1 shows part of the output for sentence (1). (1) The clothes we wear, the food we eat, the air we breathe, the water we drink, the land that sustains us, and many of the products we use are the result of agricultural research. (March 22 1987) The parser aims to be non-committal when it is unsure of an analysis. For example, it is perfectly willing to parse an embedded clause and then leave it unattached. If the object or subject of a clause is not found, Fidditch leaves it empty, as in the last two clauses in Figure 1. This non-committal approach simply reduces the effective size of the sample. The aim of the parser is to produce an annotated surface structure, building constituents as large as it can, and reconstructing the underlying clause structure when it can. In sentence (1), six clauses are found. Their predicate-argument information may be coded as a table of 5-tupks, consisting of verb, surface subject, surface object, underlying subject, underlying object, as shown in Table 1. In the subject-verb-object table, the root form of the head of phrases is recorded, and the deep subject and object are used when available. (Noun phrases of the form a n1 of n2 are coded as n1 n2; an example is the first entry in Table 2). The parser's analysis of sentence (1) is far fio.n perfect: the object of wear is not found, the object of use is not found, and the single element land rather than the conjunction of clothes, food, air, water, land, products is taken to be the subject of be. Despite these errors, the analysis is succeeds in discovering a number of the correct predicate-argument relations. The parsing errors that do occur seem to result, for the current purposes, in the omission of predicate-argument relations, rather than their misidentification. This makes the sample less effective than it might he, but it is not in general misleading. (It may also skew the sample to the extent that the parsing errors are consistent.) The analysis of the 6 million word 1987 AP sample yields 4789 verbs in 274613 clausal structures, and 26742 head nouns. This table of predicate-argument relations is the basis of our similarity metric.
3.. For any of verb in the sample, we can ask what nouns it has as subjects or objects. Table 2 shows the objects of the verb drink that occur (more than once) in the sample, in effect giving the answer to the question &quot;what can you drink?&quot; This list of drinkable things is intuitively quite good. The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column. The idea is that, in ranking the importance of noun-verb associations, we are interested not in the raw frequency of cooccurrence of a predicate and argument, but in their frequency normalized by what we would expect. More is to be learned from the fact that you can drink wine than from the fact that you can drink it even though there are more clauses in our sample with it as an object of drink than with wine. To capture this intuition, we turn, following Church and Hanks (1989), to &quot;mutual information&quot; (see Fano 1961). The mutual information of two events /(x y) is defined as follows: where P(x y) is the joint probability of events x and y, and P(x) and P(y) are the respective independent probabilities. When the joint probability P(x y) is high relative to the product of the independent probabilities, f is positive; when the joint probability is relatively low, f is negative. We use the observed frequencies to derive a cooccurrence score Cab; (an estimate of mutual information) defined as follows. where fin v) is the frequency of noun n occurring as object of verb v, fin) is the frequency of the noun n occurring as argument of any verb, f(v) is the frequency of the verb v, and N is the count of clauses in the sample. (C„,hi(n v) is defined analogously.) Calculating the cooccurrence weight for drink, shown in the third column of Table 2, gives us a reasonable ranking of terms, with it near the bottom. For any two nouns in the sample, we can ask what verb contexts they share. The distributional hypothesis is that nouns are similar to the extent that they share contexts. For example, Table 3 shows all the verbs which wine and beer can be objects of, highlighting the three verbs they have in common. The verb drink is the key common factor. There are of course many other objects that can be sold, but most of them are less alike than wine or beer because they can't also be drunk. So for example, a car is an object that you can have and sell, like wine and beer, but you do not — in this sample (confirming what we know from the meanings of the words) -typically drink a car.
4.. We propose the following metric of similarity, based on the mutual information of verbs and arguments. Each noun has a set of verbs that it occurs with (either as subject or object), and for each such relationship, there is a mutual information value. For each noun and verb pair, we get two mutual information values, for subject and object, n) and C j(v nj) We define the object similarity of two nouns with respect to a verb in terms of the minimum shared COOCCCUrrenCe weights, as in (2). The subject similarity of two nouns, SfM,1, is defined analogously. Now define the overall similarity of two nouns as the sum across all verbs of the object similarity and the subject similarity, as in (3). The metric of similarity in (2) and (3) is but one of many that might be explored, but it has some useful properties. Unlike an inner product measure, it is guaranteed that a noun will be most similar to itself. And unlike cosine distance, this metric is roughly proportional to the number of different verb contexts that are shared by two nouns. Using the definition of similarity in (3), we can begin to explore nouns that show the greatest similarity. Table 4 shows the ten nouns most similar to boat, according to our similarity metric. The first column lists the noun which is similar to boat. The second column in each table shows the number of instances that the noun appears in a predicate-argument pair (including verb environments not in the list in the fifth column). Tie third column is the number of distinct verb environments (either subject or object) that the noun occurs in which are shared with the target noun of the table. Thus, boat is found in 79 verb environment. Of these, ship shares 25 common environments (ship also occurs in many other unshared environments). The fourth column is the measure of similarity of the noun with the target noun of the table, SIM(n n 2), as defined above. The fifth column shows the common verb environments, ordered by cooccurrence score, C(vi ni), as defined above. An underscore before the verb indicates that it is a subject environment; a following underscore indicates an object environment. In Table 4, we see that boat is a subject of cruise, and object of sink. In the list for boat, in column five, cruise appears earlier in the list than carry because cruise has a higher cooccurrence score. A before a verb means that the cooccurrence score is negative -i.e. the noun is less likely to occur in that argument context than expected. For many nouns, encouragingly appropriate sets of semantically similar nouns are found. Thus, of the ten nouns most similar to boat (Table 4), nine are words for vehicles; the most similar noun is the near-synonym ship. The ten nouns most similar to treaty (agreement, plan, constitution, contract, proposal, accord, amendment, rule, law, legislation) seem to make up a cluster involving the notions of agreement and rule. Table 5 shows the ten nouns most similar to legislator, again a fairly coherent set. Of course, not all nouns fall into such neat clusters: Table 6 shows a quite heterogeneous group of nouns similar to table, though even here the most similar word (floor) is plausible. We need, in further work, to explore both automatic and supervised means of discriminating the semantically relevant associations from the spurious. hide beneath„ convolute, memorize_, sit at, sit across_, redo_, structure_, sit around, litter_, _carry, lie on_, go from_, _hold, wait_, come to_, return to, turn_, approach_, cover, be on_, share_, publish_, claim_, mean_, go to_, raise, leave_, -have_, do_, be litter, lie on, cover_, be on_, come to_, go to_ _carry, be on_, cover, return to_, tum_, go to_, leave_, -have_ approach_, return to_, mean_, go to_, be on_, tum_, come to_, leave, do_, be go from_, come to, return to_, claim_, go to, -have_, do_ structure, share_, claim_, publish_, be sit across_, mean_, be on, leave_ litter_, approach_, go to_, return to_, come to_, leave_ lie on_, be on, go to_, _hold, -have_, cover_, leave_, come to_ go from_, come to_, cover_, return to, go to_, leave_, -have_ return to, claim_, come to_, go to, cover, leave_ Reciprocally most similar nouns We can define &quot;reciprocally most similar&quot; nouns or &quot;reciprocal nearest neighbors&quot; (RNN) as two nouns which are each other's most similar noun. This is a rather stringent definition; under this definition, boat and ship do not qualify because, while ship is the most similar to boat, the word most similar to ship is not boat but plane (boat is second). For a sample of all the 319 nouns of frequency greater than 100 and less than 200, we asked whether each has a reciprocally most similar noun in the sample. For this sample, 36 had a reciprocal nearest neighbor. These are shown in Table 7 (duplicates are shown only once). The list in Table 7 shows quite a good set of substitutable words, many of which are near synonyms. Some are not synonyms but are nevertheless closely related: economist - analyst, 2 - 3. Some we recognize as synonyms in news reporting style: explosion - blast, bomb - device, tie - relation. And some are hard to interpret. Is the close relation between star and editor some reflection of news reporters' world view? Is list most like field because neither one has much meaning by itself?
5.. Using a similarity metric derived from the distribution of subjects, verbs and objects in a corpus of English text, we have shown the plausibility of deriving semantic relatedness from the distribution of syntactic forms. This demonstration has depended on: 1) the availability of relatively large text corpora; 2) the existence of parsing technology that, despite a large error rate, allows us to find the relevant syntactic relations in unrestricted text; and 3) (most important) the fact that the lexical relations involved in the distribution of words in syntactic structures are an extremely strong linguistic constraint. A number of issues will have to be confronted to further exploit these structurallymediated lexical constraints, including: Potysemy. The analysis presented here does not distinguish among related senses of the (orthographically) same word. Thus, in the table of words similar to table, we find at least two distinct senses of table conflated; the table one can hide beneath is not the table that can be commuted or memorized. Means of separating senses need to be developed. Empty words. Not all nouns are equally contentful. For example, section is a general word that can refer to sections of all sorts of things. As a result, the ten words most similar to section (school, building, exchange, book, house, ship, some, headquarter, industry, office) are a semantically diverse list of words. The reason is clear: section is semantically a rather empty word, and the selectional restrictions on its cooccurence depend primarily on its complement. You might wad a section of a book but not., typically, a section of a house. It would be possible to predetermine a set of empty words in advance of analysis, and thus avoid some of the problem presented by empty words. But it is unlikely that the class is well-defined. Rather, we expect that nouns could be ranked, on the basis of their distribution, according to how empty they are; this is a matter for further exploration. Sample size. The current sample is too small; many words occur too infrequently to be adequately sampled, and it is easy to think of usages that are not represented in the sample. For example, it is quite expected to talk about brewing beer, but the pair of brew and beer does not appear in this sample. Part of the reason for missing selectional pairs is surely the restricted nature of the AP news sublangu age. Further analysis. The similarity metric proposed here, based on subject-verb-object relations, represents a considerable reduction in the information available in the subjec-verbobject table. This reduction is useful in that it permits, for example, a clustering analysis of the nouns in the sample, and for some purposes (such as demonstrating the plausibility of the distribution-based metric) such clustering is useful. However, it is worth noting that the particular information about, for example, which nouns may be objects of a given verb, should not be discarded, and is in itself useful for analysis of text. In this study, we have looked only at the lexical relationship between a verb and the head nouns of its subject and object. Obviously, there are many other relationships among words -- for example, adjectival modification or the possibility of particular prepositional adjuncts -that can be extracted from a corpus and that contribute to our lexical knowledge. It will be useful to extend the analysis presented here to other kinds of relationships, including more complex kinds of verb complementation, noun complementation. and modification both preceding and following the head noun. But in expanding the number of different structural relations noted, it may become less useful to compute a single-dimensional similarity score of the sort proposed in Section 4. Rather, the various lexical relations revealed by parsing a corpus, will be available to be combined in many different ways yet to be explored.