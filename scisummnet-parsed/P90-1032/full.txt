1 INTRODUCTION. Language generation research on lexical choice has focused on syntactic and semantic constraints on word choice and word ordering. Collocational constraints, however, also play a role in how words can co-occur in the same sentence. Often, the use of one word in a particular context of meaning will require the use of one or more other words in the same sentence. While phrasal lexicons, in which lexical associations are pre-encoded (e.g., [Kukich 83], [Jacobs 85], [Danlos 87]), allow for the treatment of certain types of collocations, they also have problems. Phrasal entries must be compiled by hand which is both expensive and incomplete. Furthermore, phrasal entries tend to capture rather rigid, idiomatic expressions. In contrast, collocations vary tremendously in the number of words involved, in the syntactic categories of the words, in the syntactic relations between the words, and in how rigidly the individual words are used together. For example, in some cases, the words of a collocation must be adjacent, while in others they can be separated by a varying number of other words. In this paper, we identify a range of collocations that are necessary for language generation, including open compounds of two or more words, predicative relations (e.g., subject-verb), and phrasal templates representing more idiomatic expressions. We then describe how Xtract automatically acquires the full range of collocations using a two stage statistical analysis of large domain specific corpora. Finally, we show how collocations can be efficiently represented in a flexible lexicon using a unification based formalism. This is a word based lexicon that has been macrocoded with collocational knowledge. Unlike a purely phrasal lexicon, we thus retain the flexibility of word based lexicons which allows for collocations to be combined and merged in syntactically acceptable ways with other words or phrases of the sentence. Unlike pure word based lexicons, we gain the ability to deal with a variety of phrasal entries. Furthermore, while there has been work on the automatic retrieval of lexical information from text [Garside 87], [Choueka, 88], [Klavans 88], [Amsler 89], [Boguraev & Briscoe 89], [Church 89], none of these systems retrieves the entire range of collocations that we identify and no real effort has been made to use this information for language generation [Boguraev & Briscoe 89]. In the following sections, we describe the range of collocations that we can handle, the fully implemented acquisition method, results obtained, and the representation of collocations in Functional Unification Grammars (FUGs) [Kay 79]. Our application domain is the domain of stock market reports and the corpus on which our expertise is based consists of more than 10 million words taken from the Associated Press news wire.
2 SINGLE WORDS TO WHOLE PHRASES: WHAT KIND OF LEXICAL UNITS ARE NEEDED?. Collocational knowledge indicates which members of a set of roughly synonymous words co-occur with other words and how they combine syntactically. These affinities can not be predicted on the basis of semantic or syntactic rules, but can be observed with some regularity in text [Cruse 86]. We have found a range of collocations from word pairs to whole phrases, and as we shall show, this range will require a flexible method of representation. Open Compounds . Open compounds involve uninterrupted sequences of words such as &quot;stock market, ' &quot;foreign exchange,&quot; &quot;New York Stock Exchange,&quot; &quot;The Dow Jones average of 30 industrials.&quot; They can include nouns, adjectives, and closed class words and are similar to the type of collocations retrieved by [Choueka 88] or [Amsler 89]. An open compound generally functions as a single constituent of a sentence. More open compound examples are given in figure 1.1 Predicative Relations consist of two (or several) words repeatedly used together in a similar syntactic relation. These lexical relations are harder to identify since they often correspond to interrupted word sequences in the corpus. They are also the most flexible in their use. This class of collocations is related to Mel'Zuk's Lexical Functions [Mel'Euk 81], and Benson's L-type relations [Benson 86]. Within this class, Xtract retrieves subjectverb, verb-object, noun-adjective, verb-adverb, verbverb and verb-particle predicative relations. Church [Church 89] also retrieves verb-particle associations. Such collocations require a representation that allows for a lexical function relating two or more words. Examples of such collocations are given in figure 2.2 Phrasal templates: consist of idiomatic phrases containing one, several or no empty slots. They are extremely rigid and long collocations. These almost complete phrases are quite representative of a given domain. Due to their slightly idiosyncratic structure, we propose representing and generating them by simple template filling. Although some of these could be generated using a word based lexicon, in general, their usage gives an impression of fluency that cannot be equaled with compositional generation alone. Xtract has retrieved several dozens of such templates from our stock market corpus, including:
3 THE ACQUISITION METHOD: Xtract. In order to produce sentences containing collocations, a language generation system must have knowledge about the possible collocations that occur in a given domain. In previous language generation work [Danlos 87], [lordanskaja 88], [Nirenburg 88], collocations are identified and encoded by hand, sometimes using the help of lexicographers (e.g., Danlos' [Danlos 87] use of Gross' [Gross 75] work). This is an expensive and time-consuming process, and often incomplete. In this section, we describe how Xtract can automatically produce the full range of collocations described above. Xtract has two main components, a concordancing component, Xconcord, and a statistical component, Xstat. Given one or several words, Xconcord locates all sentences in the corpus containing them. Xstat is the co-occurrence compiler. Given Xconcord's output, it makes statistical observations about these words and other words with which they appear. Only statistically significant word pairs are retained. In [Smadja 89a], and [Smadja 88], we detail an earlier version of Xtract and its output, and in [Smadja 89b] we compare our results both qualitatively and quantitatively to the lexicon used in [Kukich 83]. Xtract has also been used for information retrieval in [Maarek & Smadja 89]. In the updated version of Xtract we describe here, statistical significance is based on four parameters, instead of just one, and a second stage of processing has been added that looks for combinations of word pairs produced in the first stage, resulting in multiple word collocations. Stage one: In the first phase, Xconcord is called for a single open class word and its output is pipelined to Xstat which then analyzes the distribution of words in this sample. The output of this first stage is a list of tuples (w1, w2, distance, strength, spread, height, type), where (w1, w2) is a lexical relation between two open-class words (w1 and tu2). Some results are given in Table 1. &quot;Type&quot; represents the syntactic categories of wi and w2.3. &quot;Distance&quot; is the relative distance between the two words, wi and w2 (e.g., a distance of 1 means w2 occurs immediately after wi and a distance of -1 means it occurs immediately before it). A different tuple is produced for each statistically significant word pair and distance. Thus, if the same two words occur equally often separated by two different distances, they will appear twice in the list. &quot;Strength&quot; (also computed in the earlier version of Xtract) indicates how strongly the two words are related (see [Smadja 89a]). &quot;Spread&quot; is the distribution of the relative distance between the two words; thus, the larger the &quot;spread&quot; the more rigidly they are used in combination to one another. &quot;Height&quot; combines the factors of &quot;spread&quot; 'In order to get part of speech information we use a stochastic word tagger developed at AT&T Bell Laboratories by Ken Church [Church 88] word1 -worcl2 distance strength spread height Type stock market 1 47.018 28.5 11457.1 NN president vice -1 40.6496 29.7 10757 NN trade deficit 1 30.3384 28.4361 7358.87 NN directors board -2 22.6038 28.7682 5611.84 NN merger agreement 1 20.62 28.7682 5119.32 NN attempt takeover -1 21.1464 28.407 5118.02 NN average industrial -1 13.1674 29.3682 3406.85 NJ index composite -1 12.3874 29.0682 3139.89 NJ chip blue -1 10.078 30 2721.06 NJ shares totaled -4 20.7815 29.3682 5376.87 NV price closing -1 23.0465 25.9415 4615.48 NV stocks listed -2 27.354 23.8696 4583.57 NV volume totaled 1 16.8724 29.7 4464.89 NV takeover bid -1 19.3312 28.1071 4580.39 NN takeovers hostile 1 13.5184 29.3682 3497.67 NJ takeover offer -1 5.43739 25.7917 1084.05 ' NN takeovers thwart 2 2.61206 _ 30 705.256 NV On Tuesday the Dow Jones industrial average The Dow Jones industrial average a selling spurt that sent the Dow Jones industrial average On Wednesday the Dow Jones industrial average The Dow Jones industrial average The Dow Jones industrial average ... Thursday with the Dow Jones industrial average .â€ž swelling the Dow Jones industrial average The rise in the Dow Jones industrial average The NYSE s composite index The NYSE a composite index The NYSE s composite index The NYSE s composite index The NYSE s composite index The NYSE s composite index The NYSE a composite index The NYSE s composite index The NYSE s composite index type of collocation examples open compound 'leading industrialized countries&quot; open compound &quot;the Dow Jones average of SO industrials&quot; open compound &quot;bear/buil market&quot; open compound &quot;the Dow Jones industrial average&quot; open compound &quot;The NYSE a composite index of all its listed common stocks&quot; open compound &quot;Advancing/winning/losing/declining issues&quot; open compound &quot;The NASDAQ composite index for the over the counter market&quot; open compound &quot;stock market&quot; open compound &quot;central bank open compound &quot;leveraged buyout&quot; open compound &quot;the gross national product&quot; open compound &quot;blue chip stocks&quot; open compound &quot;White House spokesman Marlin Fitzwater&quot; open compound &quot;takeover speculation/strategist/target/threat/attempt&quot; open compound &quot;takeover bid/battle/defense/efforts/fight/law/proposal/rumor&quot; and &quot;strength&quot; resulting in a ranking of the two words for their &quot;distances&quot;. Church [Church 89] produces results similar to those presented in the table using a different statistical method. However, Church's method is mainly based on the computation of the &quot;strength&quot; attribute, and it does not take into account &quot;spread&quot; and &quot;height&quot;. As we shall see, these additional parameters are crucial for producing multiple word collocations and distinguishing between open compounds (words are adjacent) and predicative relations (words can be separated by varying distance). Stage two: In the second phase, Xtra.et first uses the same components but in a different way. It starts with the pairwise lexical relations produced in Stage one to produce multiple word collocations, then classifies the collocations as one of three classes identified above, and finally attempts to determine the syntactic relations between the words of the collocation. To do this, Xtraet studies the lexical relations in context, which is exactly what lexicographers do. For each entry of Table 1, Xtract calls Xconcord on the two words wi and w2 to produce the concordances. Tables 2 and 3 show the concordances (output of Xconeord) for the input pairs: &quot;average-industrial&quot; and &quot;index-composite&quot;. Xstat then compiles information on the words surrounding both wi and w2 in the corpus. This stage allows us to filter out incorrect associations such as &quot;blue-stocks&quot; or &quot;advancing-market&quot; and replace them with the appropriate ones, &quot;blue chip stocks,&quot; &quot;the broader market in the NYSE advancing issues.&quot; This stage also produces phrasal templates such as those given in the previous section. In short, stage two filters inapropriate results and combines word pairs to produce multiple word combinations. To make the results directly usable for language generation we are currently investigating the use of a bottom-up parser in combination with stage two in order to classify the collocations according to syntactic criteria. For example if the lexical relation involves a noun and a verb it determines if it is a subject-verb or a verb-object collocation. We plan to do this using a determin'stic bottom up parser developed at Bell Communication Research [Abney 89] to parse the concordances. The parser would analyze each sentence of the concordances and the parse trees would then be passed to Xstat. Sample results of Stage two are shown in Figures 1, 2 and 3. Figure 3 shows phrasal templates and open compounds. Xstat notices that the words &quot;composite and &quot;index&quot; are used very rigidly throughout the corpus. They almost always appear in one of the two sentences. The lexical relation composite-index thus produces two phrasal templates. For the lexical relation average-industrial Xtract produces an open compound collocation as illustrated in figure 3. Stage two also confirms pairwise relations. Some examples are given in figure 2. By examining the parsed concordances and extracting recurring patterns, Xstat produces all three types of collocations.
4 HOW TO REPRESENT THEM FOR LANGUAGE GENERATION?. Such a wide variety of lexical associations would be difficult to use with any of the existing lexicon formalisms. We need a flexible lexicon capable of using single word entries, multiple word entries as well as phrasal templates and a mechanism that would be able to gracefully merge and combine them with other types of constraints. The idea of a flexible lexicon is not novel in itself. The lexical representation used in [Jacobs 85] and later refined in [Desemer & Jabobs 87] could also represent a wide range of expressions. However, in this language, collocational, syntactic and selectional constraints are mixed together into phrasal entries. This makes the lexicon both difficult to use and difficult to compile. In the following we briefly show how FUGs can be successfully used as they offer a flexible declarative language as well as a powerful mechanism for sentence generation. We have implemented a first version of Cook, a surface generator that uses a flexible lexicon for expressing co-occurrence constraints. Cook uses FUF [Elhaclad 90], an extended implementation of FUGs, to uniformly represent the lexicon and the syntax as originally suggested by Halliday [Halliday 66]. Generating a sentence is equivalent to unifying a semantic structure (Logical Form) with the grammar. The grammar we use is divided into three zones, the &quot;sentential,&quot; the &quot;lericai&quot; and &quot;the syntactic zone.&quot; Each zone contains constraints pertaining to a given domain and the input logical form is unified in turn with the three zones. As it is, full backtracking across the three zones is allowed. â€¢ The sentential zone contains the phrasal templates against which the logical form is unified first. A sentential entry is a whole sentence that should be used in a given context. This context is specified by subparts of the logical form given as input. When there is a match at this point, unification succeeds and generation is reduced to simple template filling. â€¢ The lexical zone contains the information used to lexicalize the input. It contains collocational information along with the semantic context in which to use it. This zone contains predicative and open compound collocations. Its role is to trigger phrases or words in the presence of other words or phrases. Figure 5 is a portion of the lexical grammar used in Cook. It illustrates the choice of the verb to be used when &quot;advancers&quot; is the subject. (See below for more detail). â€¢ The syntactic zone contains the syntactic grammar. It is used last as it is the part of the grammar ensuring the correctness of the produced sentences. An example input logical form is given in Figure 4. In this example, the logical form represents the fact that on the New York stock exchange, the advancing issues (semantic representation or semâ€”R: carinners) were ahead (predicate c:lead) of the losing ones (sem-R: c:losers) and that there were 3 times more winning issues than losing ones ratio). In addition, it also says that this ratio is of degree 2. A degree of I is considered as a slim lead whereas a degree of 5 is a commanding margin. When unified with the grammar, this logical form produces the sentences given in Figure 6. As an example of how Cook uses and merges cooccurrence information with other kind of knowledge consider Figure 5. The figure is an edited portion of the lexical zone. It only includes the parts that are relevant to the choice of the verb when &quot;advancers&quot; is the subject. The lex and sen-ft attributes specify the lexeme we are considering (&quot;advancers&quot;) and its semantic representation (cminners). The semantic context (sem-con-text) which points to the logical form and its features will then be used in order to select among the alternatives classes of verbs. La the figure we only included two alternatives. Both are relative to the predicate p: lead but they are used with different values of the degree attribute. When the degree is 2 then the first alternative containing the verbs listed under 5V-collocates (e.g. &quot;outnumber&quot;) will be selected. When the degree is 4 the second alternative containing the verbs listed under SV-coilocates (e.g. &quot;overpower&quot;) will be selected. All the verbal collocates shown in this figure have actually been retrieved by Xtract at a preceding stage. The unification of the logical form of Figure 4 with the lexical grammar and then with the syntactic grammar will ultimately produce the sentences shown in Figure 6 among others. In this example, the sentential zone was not used since no phrasal template expresses its semantics. The verbs selected are all listed under the 5V-collocates of the first alternative in Figure 5. We have been able to use Cook to generate several sentences in the domain of stock market reports using this method. However, this is still on-going research and the scope of the system is currently limited. We are working on extending Cook's lexicon as well as on developing extensions that will allow flexible interaction among collocations.
5 CONCLUSION. In summary, we have shown in this paper that there are many different types of collocations needed for language generation. Collocations are flexible and they can involve two, three or more words in various ways. We have described a fully implemented program, Xtract, that automatically acquires such collocations from large textual corpora and we have shown how they can be represented in a flexible lexicon using FUF. In FUF, cooccurrence constraints are expressed uniformly with syntactic and semantic constraints. The grammar's function is to satisfy these multiple constraints. We are currently working on extending Cook as well as developing a full sized from Xtract's output.
ACKNOWLEDGMENTS. We would like to thank Karen Kukich and the Computer Systems Research Division at Bell Communication Research for their help on the acquisition part of this work.