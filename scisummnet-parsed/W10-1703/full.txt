1 Introduction. This paper presents the results of the shared tasks of the joint Workshop on statistical Machine Translation (WMT) and Metrics for MAchine TRanslation (MetricsMATR), which was held at ACL 2010. This builds on four previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009), and one previous MetricsMATR meeting (Przybocki et al., 2008). There were three shared tasks this year: a translation task between English and four other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance on each of these shared task was determined after a comprehensive human evaluation. There were a number of differences between this year’s workshop and last year’s workshop: crease the statistical significance of our findings. We discuss the feasibility of using nonexperts evaluators, by analyzing the cost, volume and quality of non-expert annotations. • Clearer results for system combination – This year we excluded Google translations from the systems used in system combination. In last year’s evaluation, the large margin between Google and many of the other systems meant that it was hard to improve on when combining systems. This year, the system combinations perform better than their component systems more often than last year. The primary objectives of this workshop are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation. As with past years, all of the data, translations, and human judgments produced for our workshop are publicly available.2 We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality.
2 Overview of the shared translation and system combination tasks. The workshop examined translation between English and four other languages: German, Spanish, French, and Czech. We created a test set for each language pair by translating newspaper articles. We additionally provided training data and two baseline systems. The test data for this year’s task was created by hiring people to translate news articles that were drawn from a variety of sources from midDecember 2009. A total of 119 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German and Spanish news sites:3 Czech: iDNES.cz (5), iHNed.cz (1), Lidovky (16) French: Les Echos (25) Spanish: El Mundo (20), ABC.es (4), Cinco Dias (11) English: BBC (5), Economist (2), Washington Post (12), Times of London (3) German: Frankfurter Rundschau (11), Spiegel (4) The translations were created by the professional translation agency CEET4. All of the translations were done directly, and not via an intermediate language. As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune parameters. Some statistics about the training materials are given in Figure 1. To lower the barrier of entry for newcomers to the field, we provided two open source toolkits for phrase-based and parsing-based statistical machine translation (Koehn et al., 2007; Li et al., 2009). We received submissions from 33 groups from 29 institutions, as listed in Table 1, a 50% increase over last year’s shared task. We also evaluated 2 commercial off the shelf MT systems, and two online statistical machine translation systems. We note that these companies did not submit entries themselves. The entries for the online systems were done by translating the test data via their web interfaces. The data used to train the online systems is unconstrained. It is possible that part of the reference translations that were taken from online news sites could have been included in the online systems’ language models. In total, we received 153 primary system submissions along with 28 secondary submissions. These were made available to participants in the system combination shared task. Based on feedback that we received on last year’s system combination task, we provided two additional resources to participants: to use as a dev set for system combination. These were translated by all participating sites, and distributed to system combination participants along with reference translations. Table 2 lists the 9 participants in the system combination task.
3 Human evaluation. As with past workshops, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores. It is our contention is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no judgment in the judgment task. that automatic measures are an imperfect substitute for human assessment of translation quality. Therefore, we define the manual evaluation to be primary, and use the human judgments to validate automatic metrics. Manual evaluation is time consuming, and it requires a large effort to conduct it on the scale of our workshop. We distributed the workload across a number of people, including shared-task participants, interested volunteers, and a small number of paid annotators. More than 120 people participated in the manual evaluation5, with 89 people putting in more than an hour’s worth of effort, and 29 putting in more than four hours. A collective total of 337 hours of labor was invested.6 We asked people to evaluate the systems’ output in two different ways: The total number of judgments collected for the different modes of annotation is given in Table 3. In all cases, the output of the various translation systems were judged on equal footing; the output of system combinations was judged alongside that of the individual system, and the constrained and unconstrained systems were judged together. Ranking translations relative to each other is a reasonably intuitive task. We therefore kept the instructions simple: Rank translations from Best to Worst relative to the other choices (ties are allowed). 5We excluded data from three errant annotators, identified as follows. We considered annotators completing at least 3 screens, whose P(A) with others (see 3.2) is less than 0.33. Out of seven such annotators, four were affiliated with shared task teams. The other three had no apparent affiliation, and so we discarded their data, less than 5% of the total data. 6Whenever an annotator appears to have spent more than ten minutes on a single screen, we assume they left their station and left the window open, rather than actually needing more than ten minutes. In those cases, we assume the time spent to be ten minutes. Each screen for this task involved judging translations of three consecutive source segments. For each source segment, the annotator was shown the outputs of five submissions. For each of the language pairs, there were more than 5 submissions. We did not attempt to get a complete ordering over the systems, and instead relied on random selection and a reasonably large sample size to make the comparisons fair. Relative ranking is our official evaluation metric. Individual systems and system combinations are ranked based on how frequently they were judged to be better than or equal to any other system. The results of this are reported in Section 4. Appendix A provides detailed tables that contain pairwise comparisons between systems. We were interested in determining the inter- and intra-annotator agreement for the ranking task, since a reasonable degree of agreement must exist to support our process as a valid evaluation setup. To ensure we had enough data to measure agreement, we purposely designed the sampling of source segments shown to annotators so that items were likely to be repeated, both within an annotator’s assigned tasks and across annotators. We did so by assigning an annotator a batch of 20 screens (each with three ranking sets; see 3.1) that were to be completed in full before generating new screens for that annotator. Within each batch, the source segments for nine of the 20 screens (45%) were chosen from a small pool of 60 source segments, instead of being sampled from the larger pool of 1,000 source segments designated for the ranking task.7 The larger pool was used to choose source segments for nine other screens (also 45%). As for the remaining two screens (10%), they were chosen randomly from the set of eighteen screens already chosen. Furthermore, in the two “local repeat” screens, the system choices were also preserved. Heavily sampling from a small pool of source segments ensured we had enough data to measure inter-annotator agreement, while purposely making 10% of each annotator’s screens repeats of previously seen sets in the same batch ensured we
INTER-ANNOTATOR AGREEMENT. the sentence ranking task. In this task, P(E) is 0.333. had enough data to measure intra-annotator agreement. We measured pairwise agreement among annotators using the kappa coefficient (K), which is defined as where P(A) is the proportion of times that the annotators agree, and P(E) is the proportion of time that they would agree by chance. For inter-annotator agreement for the ranking tasks we calculated P(A) by examining all pairs of systems which had been judged by two or more judges, and calculated the proportion of time that they agreed that A > B, A = B, or A < B. Intraannotator agreement was computed similarly, but we gathered items that were annotated on multiple occasions by a single annotator. Table 4 gives K values for inter-annotator and intra-annotator agreement. These give an indication of how often different judges agree, and how often single judges are consistent for repeated judgments, respectively. The exact interpretation of the kappa coefficient is difficult, but according to Landis and Koch (1977), 0 −.2 is slight, .2 −.4 is fair, .4 − .6 is moderate, .6 − .8 is substantial and the rest is almost perfect. Based on these interpretations the agreement for sentence-level ranking is moderate for interannotator agreement and substantial for intraannotator agreement. These levels of agreement are higher than in previous years, partially due to the fact that that year we randomly included the references along the system outputs. In general, judges tend to rank the reference as the best translation, so people have stronger levels of agreement when it is included. That said, even when comparisons involving reference are excluded, we still see an improvement in agreement levels over last year. In addition to simply ranking the output of systems, we also had people edit the output of MT systems. We did not show them the reference translation, which makes our edit-based evaluation different from the Human-targeted Translation Edit Rate (HTER) measure used in the DARPA GALE program (NIST, 2008). Rather than asking people to make the minimum number of changes to the MT output in order capture the same meaning as the reference, we asked them to edit the translation to be as fluent as possible without seeing the reference. Our hope was that this would reflect people’s understanding of the output. The instructions given to our judges were as follows: Correct the translation displayed, making it as fluent as possible. If no corrections are needed, select “No corrections needed.” If you cannot understand the sentence well enough to correct it, select “Unable to correct.” A screenshot is shown in Figure 2. This year, judges were shown the translations of 5 consecutive source sentences, all produced by the same machine translation system. In last year’s WMT evaluation they were shown only one sentence at a time, which made the task more difficult because the surrounding context could not be used as an aid to understanding. Since we wanted to prevent judges from seeing the reference before editing the translations, we split the test set between the sentences used in the ranking task and the editing task (because they were being conducted concurrently). Moreover, annotators edited only a single system’s output for one source sentence to ensure that their understanding of it would not be influenced by another system’s output. Halfway through the manual evaluation period, we stopped collecting edited translations, and instead asked annotators to do the following: Instructions: You are shown several machine translation outputs. Your task is to edit each translation to make it as fluent as possible. It is possible that the translation is already fluent. In that case, select No corrections needed. If you cannot understand the sentence well enough to correct it, select Unable to correct. The sentences are all from the same article. You can use the earlier and later sentences to help understand a confusing sentence. Your edited translations The machine translations The shortage of snow in mountain The shortage of snow in mountain worries the hoteliers worries the hoteliers correct Reset Edited No corrections needed Unable to The deserted tracks are not The deserted tracks are not putting down problem only at the exploitants of skilift. putting down problem only at the exploitants of skilift. correct Reset Edited No corrections needed Unable to The lack of snow deters the people The lack of snow deters the people to reserving their stays at the ski in the hotels and pension. to reserving their stays at the ski in the hotels and pension. correct Reset Edited No corrections needed Unable to Thereby, is always possible to Thereby, is always possible to track free bedrooms for all the dates in winter, including Christmas and Nouvel An. track free bedrooms for all the dates in winter, including Christmas and Nouvel An. Indicate whether the edited translations represent fully fluent and meaningequivalent alternatives to the reference sentence. The reference is shown with context, the actual sentence is bold. In addition to edited translations, unedited items that were either marked as acceptable or as incomprehensible were also shown. Judges gave a simple yes/no indication to each item.
4 Translation task results. We used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the workshop. In our analysis, we aimed to address the following questions: Table 5 shows the best individual systems. We define the best systems as those which had no other system that was statistically significantly better than them under the Sign Test at p < 0.1. Multiple systems are listed as the winners for many language pairs because it was not possible to draw a statistically significant difference between the systems. There is no individual system clearly outperforming all other systems across the different language pairs. With the exception of FrenchEnglish and English-French one can observe that top-performing constrained systems did as well as the unconstrained system ONLINEB. Table 6 shows the best combination systems. For all language directions, except SpanishEnglish, one can see that the system combination runs outperform the individual systems and that in most cases the differences are statistically significant. While this is to be expected, system combination is not guaranteed to improve performance as some of the lower ranked combination runs show, which are outperformed by individual systems. Also note that except for Czech-English translation the online systems ONLINEA and ONLINEB where not included for the system combination runs Understandability Our hope is that judging the acceptability of edited output as discussed in Section 3 gives some indication of how often a system’s output was understandable. Figure 3 gives the percentage of times that each system’s edited output was judged to be acceptable (the percentage also factors in instances when judges were unable to improve the output because it was incomprehensible). This style of manual evaluation is experimental and should not be taken to be authoritative. Some caveats about this measure:
5 Shared evaluation task overview. In addition to allowing the analysis of subjective translation quality measures for different systems, the judgments gathered during the manual evaluation may be used to evaluate how well the automatic evaluation metrics serve as a surrogate to the manual evaluation processes. NIST began running a “Metrics for MAchine TRanslation” challenge (MetricsMATR), and presented their findings at a workshop at AMTA (Przybocki et al., 2008). This year we conducted a joint MetricsMATR and WMT workshop, with NIST running the shared evaluation task and analyzing the results. In this year’s shared evaluation task 14 different research groups submitted a total of 26 different automatic metrics for evaluation: (Dobrinkat et al., 2010) ? indicates a constrained win, no other constrained system is statistically better. For all pairwise comparisons between systems, please check the appendix. System combinations are listed in the order of how often their translations were ranked higher than or equal to any other system. Ties are broken by direct comparison. We show the best individual systems alongside the system combinations, since the goal of combination is to produce better quality translation than the component systems. ? indicates an individual system that none of the system combinations beat by a statistically significant margin at plevel≤0.1. Note: ONLINEA and ONLINEB were not included among the systems being combined in the system combination shared tasks, except in the Czech-English and English-Czech conditions, where ONLINEB was included. Table 6: Official results for the WMT10 system combination task, based on the human evaluation (ranking translations relative to each other) These numbers also include judgments of the system’s output when it was marked either incomprehensible or acceptable and left unedited. Note that the reference translation was edited alongside the system outputs. Error bars show one positive and one negative standard deviation for the systems in that language pair. a shorter description when concatenated before compression than when concatenated after compression. MT-NCD does not require any language specific resources. Due to a processing issue inherent to the metric, the scores reported were generated excluding the first segment of each document. Also, a separate issue was found for the MT-mNCD metric, and according to the developer the scores reported here would like change with a correction of the issue. BabbleQuest International8 der by various means. The former is assessed by matching word forms at linguistic levels, including surface form, stem, sense and semantic similarity, and further by weighting the informativeness of both matched and unmatched words. The latter is quantified in term of the discordance of word position and word sequence between an MT output and its reference. Due to a version discrepancy of the metric, final scores for ATECD-2.1 differ from those reported here, but only minimally. Carnegie Mellon University (Denkowski and Lavie, 2010) capturing the two most important elements of translation quality. This simple combined metric only has one parameter, which makes its scores easy to interpret. It is also fast to run and language-independent. It uses Kendall’s tau permutation. Due to installation issues, the reported submitted scores for these two metrics have not been verified to produce identical scores at NIST. Harbin Institute of Technology, China The model resembles a Conditional Random Field, but performs regression instead of classification. It is trained on Arabic, Chinese, and Urdu data from the MT-Eval 2008 dataset. Due to installation issues, the reported scores for this metric have not been verified to produce identical scores at NIST. Scores were not submitted along with this metric, and due to installation issues were not produced at NIST in time to be included in this report. University Polit`ecnica de Catalunya/University de Barcelona (Comelles et al., 2010) lations. The basic elements (BEs) consist of content words and various combinations of syntactically-related words. A variety of transformations are performed to allow flexible matching so that words and syntactic constructions conveying similar content in different manners may be matched. The transformations cover synonymy, preposition vs. noun compounding, differences in tenses, etc. BEwT-E was originally created for summarization evaluation and is English-specific. Scores were not submitted for BEwT-E; the runtime required for this metric to process the WMT10 data set prohibited the production of scores in time for publication.
6 Evaluation task results. The results reported here are preliminary; a final release of results will be published on the WMT10 website before July 15, 2010. Metric developers submitted metrics for installation at NIST; they were also asked to submit metric scores on the WMT10 test set along with their metrics. Not all developers submitted scores, and not all metrics were verified to produce the same scores as submitted at NIST in time for publication. Any such caveats are reported with the description of the metrics above. The results reported here are limited to a comparison of metric scores on the full WMT10 test set with human assessments on the humanassessed subset. An analysis comparing the human assessments with the automatic metrics run only on the human-assessed subset will follow at a later date. The WMT10 system output used to generate the reported metric scores was found to have improperly escaped characters for a small number of segments. While we plan to regenerate the metric scores with this issue resolved, we do not expect this to significantly alter the results, given the small number of segments affected. The tables in Appendix B list the metric scores for the language pairs processed by each metric. These first four tables present scores for translations out of English into Czech, French, German and Spanish. In addition to the metric scores of the submitted metrics identified above, we also present (1) the ranking of the system as determined by the human assessments; and (2) the metrics scores for two popular baseline metrics, BLEU as calculated by NIST’s mteval software12 and the NIST score. For each method of system measurement the absolute highest score is identified by being outlined in a box. Similarly, the remaining tables in Appendix B list the metric scores for the submitted metrics and the two baseline metrics, and the ranking based on the human assessments for translations into English from Czech, French, German and Spanish. As some metrics employ language-specific resources, not all metrics produced scores for all language pairs. It is noticeable that system combinations are often among those achieving the highest scores. To assess the performance of the automatic metrics, we correlated the metrics’ scores with the human rankings at the system level. We assigned a consolidated human-assessment rank to each system based on the number of times that the given system’s translations were ranked higher than or equal to the translations of any other system in the manual evaluation of the given language pair. We then compared the ranking of systems by the human assessments to that provided by the automatic metric system level scores on the complete WMT10 test set for each language pair, using Spearman’s p rank correlation coefficient. The correlations are shown in Table 7 for translations to English, and Table 8 out of English, with baseline metrics listed at the bottom. The highest correlation for each language pair and the highest overall average are bolded. Overall, correlations are higher for translations to English than compared to translations from English. For all language pairs, there are a number of new metrics that yield noticeably higher correlations with human assessments than either of the two included baseline metrics. In particular, Bleu performed in the bottom half of the into-English and out-of-English directions. The method employed to collect human judgments of rank preferences at the segment level produces a sparse matrix of decision points. It is unclear whether attempts to normalize the segment level rankings to 0.0–1.0 values, representing the relative rank of a system per segment given the number of comparisons it is involved with, is proper. An intuitive display of how well metrics mirror the human judgments may be shown via a confusion matrix. We compare the human ranks to the ranks as determined by a metric. Below, we show an example of the confusion matrix for the SVM-rank metric which had the highest summed diagonal (occurrences when a particular rank by the metric’s score exactly matches the human judgments) for all segments translated into English. The numbers provided are percentages of the total count. The summed diagonal constitutes 39.01% of all counts in this example matrix. The largest cell is the 1/1 ranking cell (top left). We included the reference translation as a system in this analysis, which is likely to lead to a lot of agreement on the highest rank between humans and automatic metrics. No allowances for ties were made in this analysis. That is, if a human ranked two system translations the same, this analysis expects the metrics to provide the same score in order to get them both correct. Future analysis could relax this constraint. As not all human rankings start with the highest possible rank of “1” (due to ties and withholding judgment on a particular system output being allowed), we set the highest automatic metric rank to the highest human rank and shifted the lower metric ranks down accordingly. Table 9 shows the summed diagonal percentages of the total count of all datapoints for all metrics that WMT10 scores were available for, both combined for all languages to English (X-English) and separately for each language into English. The results are ordered by the highest percentage for the summed diagonal on all languages to English combined. There are quite noticeable changes in ranking of the metrics for the separate language pairs; further analysis into the reasons for this will be necessary. We plan to also analyze metric performance for translation into English.
7 Feasibility of Using Non-Expert Annotators in Future WMTs. In this section we analyze the data that we collected data by posting the ranking task on Amazon’s Mechanical Turk (MTurk). Although we did not use this data when creating the official results, our hope was that it may be useful in future workshops in two ways. First, if we find that it is possible to obtain a sufficient amount of data of good quality, then we might be able to reduce the time commitment expected from the system developers in future evaluations. Second, the additional collected labels might enable us to detect significant differences between systems that would otherwise be insignificantly different using only the data from the volunteers (which we will now refer to as the “expert” data). To that end, we prepared 600 ranking sets for each of the eight language pairs, with each set containing five MT outputs to be ranked, using the same interface used by the volunteers. We posted the data to MTurk and requested, for each one, five redundant assignments, from different workers. Had all the 5 x 8 x 600 = 24,000 assignments been completed, we would have obtained 24,000 x 5 = 120,000 additional rank labels, compared to the 37,884 labels we collected from the volunteers (Table 3). In actuality, we collected closer to 55,000 rank labels, as we discuss shortly. To minimize the amount of data that is of poor quality, we placed two requirements that must be satisfied by any worker before completing any of our tasks. First, we required that a worker have an existing approval rating of at least 85%. Second, we required a worker to reside in a country where the target language of the task can be assumed to be the spoken language. Finally, anticipating a large pool of workers located in the United States, we felt it possible for us to add a third restriction for the *-to-English language pairs, which is that a worker must have had at least five tasks previously approved on MTurk.13 We organized the ranking sets in groups of 3 per screen, with a monetary reward of $0.05 per screen. When we created our tasks, we had no expectation that all the assignments would be completed over the tasks’ lifetime of 30 days. This was indeed the case (Table 10), especially for language pairs with a non-English target language, due to workers being in short supply outside the US. Overall, we see that the amount of data collected from non-US workers is relatively small (left half of Table 10), whereas the pool of US-based workers is much larger, leading to much higher completion rates for language pairs with English as the target language (right half of Table 10). This is in spite of the additional restriction we placed on US workers. 13We suspect that newly registered workers on MTurk already start with an “approval rating” of 100%, and so requiring a high approval rating alone might not guard against new workers. It is not entirely clear if our suspicion is true, but our past experiences with MTurk usually involved a noticeably faster completion rate than what we experienced this time around, indicating our suspicion might very well be correct. Table 10: Statistics for data collected on MTurk for the ranking task. In total, 55,082 rank labels were collected across the eight language pairs (145% of expert data). Each language pair had 600 sets, and we requested each set completed by 5 different workers. Since each set provides 5 labels, we could have potentially obtained 600 x 5 x 5 = 15,000 labels for each language pair. The Label count row indicates to what extent that potential was met (over the 30-day lifetime of our tasks), and the “Completed...” rows give a breakdown of redundancy. For instance, the right-most column indicates that, in the cz-en group, 2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5 workers, with 100% of the sets completed at least once. The total cost of this data collection effort was roughly $200.
INTRA-ANNOTATOR AGREEMENT. With references 0.539 0.309 Without references 0.538 0.307 Table 11: Inter- and intra-annotator agreement for the MTurk workers on the sentence ranking task. (As before, P(E) is 0.333.) For comparison, we repeat here the kappa coefficients of the experts (K*), taken from Table 4. It is encouraging to see that we can collect a large amount of rank labels from MTurk. That said, we still need to guard against data from bad workers, who are either not being faithful and clicking randomly, or who might simply not be competent enough. Case in point, if we examine interand intra-annotator agreement on the MTurk data (Table 11), we see that the agreement rates are markedly lower than their expert counterparts. Another indication of the presence of bad workers is a low reference preference rate (RPR), which we define as the proportion of time a reference translation wins (or ties in) a comparison when it appears in one. Intuitively, the RPR should be quite high, since it is quite rare that an MT output ought to be judged better than the reference. This rate is 96.5% over the expert data, but only 83.7% over the MTurk data. Compare this to a randomly-clicking RPR of 66.67% (because the two acceptable answers are that the reference is either better than a system’s output or tied with it). Also telling would be the rate at which MTurk workers agree with experts. To ensure that we obtain enough overlapping data to calculate such a rate, we purposely select one-sixth14 of our ranking sets so that the five-system group is exactly one that has been judged by an expert. This way, at least one-sixth of the comparisons obtained from an MTurk worker’s labels are comparisons for which we already have an expert judgment. When we calculate the rate of agreement on this data, we find that MTurk workers agree with the expert workers 53.2% of the time, or K = 0.297, and when references are excluded, the agreement rate is 50.0%, or K = 0.249. Ideally, we would want those values to be in the 0.4–0.5 range, since that is where the inter-annotator kappa coefficient lies for the expert annotators. We can use the agreement rate with experts to identify MTurk workers who are not performing the task as required. For each worker w of the 669 workers for whom we have such data, we compute the worker’s agreement rate with the experts, and from it a kappa coefficient Kexp(w) for that worker. (Given that P(E) is 0.333, Kexp(w) ranges between −0.5 and +1.0.) We sort the workers based on Kexp(w) in ascending order, and examine properties of the MTurk data as we remove the lowest-ranked workers one by one (Figure 4). We first note that the amount of data we obtained from MTurk is so large, that we could afford to eliminate close to 30% of the labels, and we would still have twice as much data than using the expert data alone. We also note that two workers in particular (the 103rd and 130th to be removed) are likely responsible for the majority of the bad data, since removing their data leads to noticeable jumps in the reference preference rate and the inter-annotator agreement rate (right two curves of Figure 4). Indeed, examining the data for those two workers, we find that their RPR values are 55.7% and 51.9%, which is a clear indication of random clicking.15 Looking again at those two curves shows degrading values as we continue to remove workers in large droves, indicating a form of “overfitting” to agreement with experts (which, naturally, continues to increase until reaching 1.0; bottom left curve). It is therefore important, if one were to filter out the MTurk data by removing workers this way, to choose a cutoff carefully so that no criterion is degraded dramatically. In Appendix A, after reporting head-to-head comparisons using only the expert data, we also report head-to-head comparisons using the expert 15In retrospect, we should have performed this type of analysis as the data was being collected, since such workers could have been identified early on and blocked. data combined with the MTurk data, in order to be able to detect more significant differences between the systems. We choose the 300-worker point as a reasonable cutoff point before combining the MTurk data with the expert data, based on the characteristics of the MTurk data at that point: a high reference preference rate, high interannotator agreement, and, critically, a kappa coefficient vs. expert data of 0.449, which is close to the expert inter-annotator kappa coefficient of 0.439. In the previous subsection, we outlined an approach by which MTurk data can be filtered out using expert data. Since we were to combine the filtered MTurk data with the expert data to obtain more significant differences, it was reasonable to use agreement with experts to quantify the MTurk workers’ competency. However, we also would like to know whether it is feasible to use the MTurk data alone. Our aim here is not to boost the differences we see by examining expert data, but to eliminate our reliance on obtaining expert data in the first place. We briefly examined some simple ways of filtering/combining the MTurk data, and measured the Spearman rank correlations obtained from the MTurk data (alone), as compared to the rankings obtained using the expert data (alone), and report them in Table 12. (These correlations do not include the references.) We first see that even when using the MTurk data untouched, we already obtain relatively high correlation with expert ranking (“Unfiltered”). This is especially true for the *-to-English language pairs, where we collected much more data than English-to-*. In fact, the relationship between the amount of data and the correlation values is very strong, and it is reasonable to expect the correlation numbers for English-to-* to catch up had more data been collected. We also measure rank correlations when applying some simple methods of cleaning/weighting MTurk data. The first method (“Voting”) is performing a simple vote whenever redundant comparisons (i.e. from different workers) are available. The second method (“K��p-filtered”) first removes labels from the 300 worst workers according to agreement with experts. The third method (“RPR-filtered”) first removes labels from the 62 worst workers according to their RPR. The numbers 300 and 62 were chosen since those are the points at which the MTurk data reaches the level of expert data in the inter-annotator agreement and RPR of the experts. The fourth and fifth methods (“Weighted by Ke,,p” and “Weighted by K(RPR)”) do not remove any data, instead assigning weights to workers based on their agreement with experts and their RPR, respectively. Namely, for each worker, the weight assigned by the fourth method is Ke,,p for that worker, and the weight assigned by the fifth method is K(RPR) for that worker. Examining the correlation coefficients obtained from those methods (Table 12), we see mixed results, and there is no clear winner among those methods. It is also difficult to draw any conclusion as to which method performs best when. However, it is encouraging to see that the two RPR-based methods perform well. This is noteworthy, since there is no need to use expert data to weight workers, which means that it is possible to evaluate a worker using inherent, ‘built-in’ properties of that worker’s own data, without resorting to making comparisons with other workers or with experts.
8 Summary. As in previous editions of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from European languages into English, and vice versa. The number of participants grew substantially compared to previous editions of the WMT workshop, with 33 groups from 29 institutions participating in WMT10. Most groups participated in the translation task only, while the system combination task attracted a somewhat smaller number of participants Unfortunately, fewer rule-based systems participated in this year’s edition of WMT, compared to previous editions. We hope to attract more rule-based systems in future editions as they increase the variation of translation output and for some language pairs, such as German-English, tend to outperform statistical machine translation systems. This was the first time that the WMT workshop was held as a joint workshop with NIST’s MetricsMATR evaluation initiative. This joint effort was very productive as it allowed us to focus more on the two evaluation dimensions: manual evaluation of MT performance and the correlation between manual metrics and automated metrics. This year was also the first time we have introduced quality assessments by non-experts. In previous years all assessments were carried out through peer evaluation exclusively consisting of developers of machine translation systems, and thereby people who are used to machine translation output. This year we have facilitated Amazon’s Mechanical Turk to investigate two aspects of manual evaluation: How stable are manual assessments across different assessor profiles (experts vs. non-experts) and how reliable are quality judgments of non-expert users? While the intra- and inter-annotator agreements between non-expert assessors are considerably lower than for their expert counterparts, the overall rankings of translation systems exhibit a high degree of correlation between experts and non-experts. This correlation can be further increased by applying various filtering strategies reducing the impact of unreliable non-expert annotators. As in previous years, all data sets generated by this workshop, including the human judgments, system translations and automatic scores, are publicly available for other researchers to analyze.16
Acknowledgments. This work was supported in parts by the EuroMatrixPlus project funded by the European Commission (7th Framework Programme), the GALE program of the US Defense Advanced Research Projects Agency, Contract No. HR0011-06-C0022, and the US National Science Foundation under grant IIS-0713448.