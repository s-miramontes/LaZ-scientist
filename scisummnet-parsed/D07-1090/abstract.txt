This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastruc ture is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. Itis capable of providing smoothed probabilities for fast, single-pass decoding. We in troduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.