1 Introduction. The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003). In that work, the proposed method was based on bootstrap resampling and was designed to improve the statistical reliability of results by controlling for randomness across test sets. However, there is no consistently used strategy that controls for the effects of unstable estimates of model parameters.1 While the existence of optimizer instability is an acknowledged problem, it is only infrequently discussed in relation to the reliability of experimental results, and, to our knowledge, there has yet to be a systematic study of its effects on hypothesis testing. In this paper, we present a series of experiments demonstrating that optimizer instability can account for substantial amount of variation in translation quality,2 which, if not controlled for, could lead to incorrect conclusions. We then show that it is possible to control for this variable with a high degree of confidence with only a few replications of the experiment and conclude by suggesting new best practices for significance testing for machine translation.
2 Nondeterminism and Other Optimization Pitfalls. Statistical machine translation systems consist of a model whose parameters are estimated to maximize some objective function on a set of development data. Because the standard objectives (e.g., 1-best BLEU, expected BLEU, marginal likelihood) are not convex, only approximate solutions to the optimization problem are available, and the parameters learned are typically only locally optimal and may strongly depend on parameter initialization and search hyperparameters. Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure. This variation in the parameter vector affects the quality of the model measured on both development data and held-out test data, independently of any ex- explored regularization of MERT to improve generperimental manipulation. Thus, when trying to de- alization on test sets. Moore and Quirk (2008) extermine whether the difference between two mea- plored strategies for selecting better random “restart surements is significant, it is necessary to control for points” in optimization. Cer et al. (2010) analyzed variance due to noisy parameter estimates. This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times). In our experiments, we ran the MERT optimizer to Unfortunately, common practice in reporting ma- optimize BLEU on a held-out development set many chine translation results is to run the optimizer once times to obtain a set of optimizer samples on two difper system configuration and to draw conclusions ferent pairs of systems (4 configurations total). Each about the experimental manipulation from this sin- pair consists of a baseline system (System A) and an gle sample. However, it could be that a particu- “experimental” system (System B), which previous lar sample is on the “low” side of the distribution research has suggested will perform better. over optimizer outcomes (i.e., it results in relatively The first system pair contrasts a baseline phrasepoorer scores on the test set) or on the “high” side. based system (Moses) and experimental hierarchiThe danger here is obvious: a high baseline result cal phrase-based system (Hiero), which were conpaired with a low experimental result could lead to a structed from the Chinese-English BTEC corpus useful experimental manipulation being incorrectly (0.7M words), the later of which was decoded with identified as useless. We now turn to the question of the cdec decoder (Koehn et al., 2007; Chiang, 2007; how to reduce the probability falling into this trap. Dyer et al., 2010). The second system pair con3 Related Work trasts two German-English Hiero/cdec systems conThe use of statistical hypothesis testing has grown structed from the WMT11 parallel training data apace with the adoption of empirical methods in (98M words).4 The baseline system was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including for confidence tion of the German text according to the CRF word estimation in speech recognition (Bisani and Ney, segmentation model of Dyer (2009). The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times. Zhang and Vogel, 2010). Approximate randomiza- Our experiments used the default implementation tion (AR) has been proposed as a more reliable tech- of MERT that accompanies each of the two denique for MT significance testing, and evidence sug- coders. The Moses MERT implementation uses 20 gests that it yields fewer type I errors (i.e., claiming random restart points per iteration, drawn uniformly a significant difference where none exists; Riezler from the default ranges for each feature, and, at each and Maxwell, 2005). Other uses in NLP include iteration, 200-best lists were extracted with the curthe MUC-6 evaluation (Chinchor, 1993) and pars- rent weight vector (Bertoldi et al., 2009). The cdec ing (Cahill et al., 2008). However, these previous MERT implementation performs inference over the methods assume model parameters are elements of decoder search space which is structured as a hyperthe system rather than extraneous variables. graph (Kumar et al., 2009). Rather than using restart Prior work on optimizer noise in MT has fo- points, in addition to optimizing each feature indecused primarily on reducing optimizer instability pendently, it optimizes in 5 random directions per it(whereas our concern is how to deal with optimizer eration by constructing a search vector by uniformly noise, when it exists). Foster and Kuhn (2009) mea- sampling each element of the vector from (−1, 1) sured the instability of held-out BLEU scores across and then renormalizing so it has length 1. For all 10 MERT runs to improve tune/test set correlation. systems, the initial weight vector was manually iniHowever, they only briefly mention the implications tialized so as to yield reasonable translations. of the instability on significance. Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find different local maxima. The noise due to this variable can depend on many number of factors, including the number of random restarts used (in MERT), the number of features in a model, the number of references, the language pair, the portion of the search space visible to the optimizer (e.g. 10best, 100-best, a lattice, a hypergraph), and the size of the tuning set. Unfortunately, there is no proxy to estimate this effect as with bootstrap resampling. To control for this variable, we must run the optimizer multiple times to estimate the spread it induces on the development set. Using the n optimizer samples, with mi as the translation quality measurement of 5METEOR version 1.2 with English ranking parameters and all modules. the development set for the ith optimization run, and m is the average of all mis, we report the standard deviation over the tuning set as sdev: (mi − m)2 n − 1 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Overfitting effects stest As with any optimizer, there is a danger that the optimal weights for a tuning set may not generalize well to unseen data (i.e., we overfit). For a randomized optimizer, this means that parameters can generalize to different degrees over multiple optimizer runs. We measure the spread induced by optimizer randomness on the test set metric score stest, as opposed to the overfitting effect in isolation. The computation of stest is identical to sdev except that the mis are the translation metrics calculated on the test set. In Table 1, we observe that stest > sdev, indicating that optimized parameters are likely not generalizing well. Test set selection ssel The final extraneous variable we consider is the selection of the test set itself. A good test set should be representative of the domain or language for which experimental evidence is being considered. However, with only a single test corpus, we may have unreliable results because of idiosyncrasies in the test set. This can be mitigated in two ways. First, replication of experiments by testing on multiple, non-overlapping test sets can eliminate it directly. Since this is not always practical (more test data may not be availabile), the widely-used bootstrap resampling method (§3) also controls for test set effects by resampling multiple “virtual” test sets from a single set, making it possible to infer distributional parameters such as the standard deviation of the translation metric over (very similar) test sets.6 Furthermore, this can be done for each of our optimizer samples. By averaging the bootstrap-estimated standard deviations over optimizer samples, we have a statistic that jointly quantifies the impact of test set effects and optimizer instability on a test set. We call this statistic ssel. Different values of this statistic can suggest methodological improvements. For example, a large ssel indicates that more replications will be necessary to draw reliable inferences from experiments on this test set, so a larger test set may be helpful. To compute ssel, assume we have n independent optimization runs which produced weight vectors that were used to translate a test set n times. The test set has ` segments with references R = (R1, R2, ... , Rt). Let X = (X1, X2, ... , Xn) where each Xi = (Xi1, Xi2, . . . , Xit) is the list of translated segments from the ith optimization run list of the ` translated segments of the test set. For each hypothesis output Xi, we construct k bootstrap replicates by drawing ` segments uniformly, with replacement, from Xi, together with its corresponding reference. This produces k virtual test sets for each optimization run i. We designate the score of the jth virtual test set of the ith optimization run with mij. si In the previous section, we gave statistics about the distribution of evaluation metrics across a large number of experimental samples (Table 1). Because of the large number of trials we carried out, we can be extremely confident in concluding that for both pairs of systems, the experimental manipulation accounts for the observed metric improvements, and furthermore, that we have a good estimate of the magnitude of that improvement. However, it is not generally feasible to perform as many replications as we did, so here we turn to the question of how to compare two systems, accounting for optimizer noise, but without running 300 replications. We begin with a visual illustration how optimizer instability affects test set scores when comparing two systems. Figure 1 plots the histogram of the 300 optimizer samples each from the two BTEC Chinese-English systems. The phrase-based system’s distribution is centered at the sample mean 48.4, and the hierarchical system is centered at 49.9, a difference of 1.5 BLEU, corresponding to the widely replicated result that hierarchical phrase-based systems outperform conventional phrase-based systems in Chinese-English translation. Crucially, although the distributions are distinct, there is a non-trivial region of overlap, and experimental samples from the overlapping region could suggest the opposite conclusion! To further underscore the risks posed by this overlap, Figure 2 plots the relative frequencies with which different BLEU score deltas will occur, as a function of the number of optimizer samples used. When is a difference significant? To determine whether an experimental manipulation results in a statistically reliable difference for an evaluation metric, we use a stratified approximate randomization (AR) test. This is a nonparametric test that approximates a paired permutation test by sampling permutations (Noreen, 1989). AR estimates the probability (p-value) that a measured difference in metric scores arose by chance by randomly exchanging sentences between the two systems. If there is no significant difference between the systems (i.e., the null hypothesis is true), then this shuffling should not change the computed metric score. Crucially, this assumes that the samples being analyzed are representative of all extraneous variables that could affect the outcome of the experiment. Therefore, we must include multiple optimizer replications. Also, since metric scores (such as BLEU) are in general not comparable across test sets, we stratify, exchanging only hypotheses that correspond to the same sentence. Table 2 shows the p-values computed by AR, testing the significance of the differences between the two systems in each pair. The first three rows illustrate “single sample” testing practice. Depending on luck with MERT, the results can vary widely from insignificant (at p > .05) to highly significant. The last two lines summarize the results of the test when a small number of replications are performed, as ought to be reasonable in a research setting. In this simulation, we randomly selected n optimizer outputs from our large pool and ran the AR test to determine the significance; we repeated this procedure 250 times. The p-values reported are the pvalues at the edges of the 95% confidence interval (CI) according to AR seen in the 250 simulated comparison scenarios. These indicate that we are very likely to observe a significant difference for BTEC at n = 5, and a very significant difference by n = 50 (Table 2). Similarly, we see this trend in the WMT system: more replications leads to more significant results, which will be easier to reproduce. Based on the average performance of the systems reported in Table 1, we expect significance over a large enough number of independent trials.
5 Discussion and Recommendations. No experiment can completely control for all possible confounding variables. Nor are metric scores (even if they are statistically reliable) a substitute for thorough human analysis. However, we believe that the impact of optimizer instability has been neglected by standard experimental methodology in MT research, where single-sample measurements are too often used to assess system differences. In this paper, we have provided evidence that optimizer instability can have a substantial impact on results. However, we have also shown that it is possible to control for it with very few replications (Table 2). We therefore suggest: set evaluation be performed at least three times; more replications may be necessary for experimental manipulations with more subtle effects; • Use of the median system according to a trusted metric when manually analyzing system output; preferably, the median should be determined based on one test set and a second test set should be manually analyzed.
Acknowledgments. We thank Michael Denkowski, Kevin Gimpel, Kenneth Heafield, Michael Heilman, and Brendan O’Connor for insightful feedback. This research was supported in part by the National Science Foundation through TeraGrid resources provided by Pittsburgh Supercomputing Center under TG-DBS110003; the National Science Foundation under IIS-0713402, IIS-0844507, IIS-0915187, and IIS0915327; the DARPA GALE program, the U. S. Army Research Laboratory, and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533.