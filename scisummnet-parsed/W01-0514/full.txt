1 Introduction. The aim of linear text segmentation is to partition a document into blocks, such that each segment is coherent and consecutive segments are about different topics. This procedure is useful in information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarisation (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997) and text navigation (Choi, 2000b). This paper presents a new algorithm for segmenting written text. The method builds on previous work by Choi (2000a). The primary distinction is the use of latent semantic analysis (LSA) in formulating the similarity matrix. We discovered that (1) LSA is a more accurate measure of similarity than the cosine metric, (2) stemming does not always improve segmentation accuracy and (3) ranking is crucial to cosine but not LSA.
2 Background. A text segmentation algorithm has three main parts. First, the input text is divided into elementary blocks. Second, a similarity metric identifies blocks that are about the same topic. Finally, topic boundaries are discovered by a clustering algorithm. An elementary block is the smallest text segment that can describe an entire topic, e.g. sentences (Ponte and Croft, 1997), paragraphs (Yaari, 1997) and arbitrary-sized segments (Hearst, 1994). Linguistic theories (Chafe, 1979; Longacre, 1979; Kieras, 1982) and work in information retrieval (Salton et al., 1993; Kaszkiel and Zobel, 1997) suggest a coherent text segment is represented by paragraphs. We argue that a paragraph can address multiple topics and is motivated by content, writing style and presentation. Thus, a topic segment is a collection of sentences. This view is supported by previous work in text segmentation (Ponte and Croft, 1997; Choi, 2000a). A similarity metric estimates the likelihood of two segments describing the same topic. Existing methods fall into one of two categories. Lexical cohesion methods stem from the work of Halliday and Hasan (1976), in which a coherent topic segment is believed to contain parts with similar vocabulary. Implementations of this use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999; Choi, 2000a), entity repetition (Kan et al., 1998), thesaurus relations (Morris and Hirst, 1991), spreading activation over dictionary (Kozima, 1993), a word distance model (Beeferman et al., 1997) or a word frequency model (Reynar, 1999; Utiyama and Isahara, 2001) to detect cohesion. These methods are typically applied in information retrieval (Hearst, 1994; Reynar, 1998) to segment written text. Multi-source methods use cue phrases, prosodic features, ellipsis, anaphora, syntactic features, language models and lexical cohesion metrics to detect topic boundaries. Features are combined using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995), probabilistic models (Hajime et al., 1998) and maximum entropy models (Beeferman et al., 1997; Reynar, 1998). The aim is to improve segmentation accuracy by combining multiple indicators of topic shift. These methods are typically applied in topic detection and tracking (Allan et al., 1998) to segment transcribed text and broadcast news stories. Topic boundaries are discovered by merging consecutive elementary blocks that are about the same topic. Existing algorithms used a sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998; Utiyama and Isahara, 2001), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994; Choi, 2000a) to determine the optimal segmentation. The main difficulty in clustering is automatic termination, i.e. determining the number of topic boundaries in a document.
3 A new method. The input to our algorithm is a list of tokenised sentences S = {81,..,8}. Content words are identified by removing punctuation marks and stopwords from S. A term frequency vector f, is then constructed for each sentence i. fij denotes the number of times content word j occurs in s,. The C99 algorithm (Choi, 2000a) uses the cosine metric (van Rijsbergen, 1979) (eq. 1) to compute a nxn similarity matrix M for S. represents the similarity between s, and si. The assumption is, two sentences with similar word usage are likely to be about the same topic. This idea has two main problems. First, the estimate is inaccurate for short passages. Second, synonyms are considered negative evidence, e.g. car E s, and automobile E si implies s, and si are dissimilar. The first problem was addressed by replacing with its rank Rii (eq. 2, r defines the local context). The idea is, the difference in magnitude is inaccurate, thus one can only use the order as evidence for segmentation. Consider X = {xl,x2,x3} = {1,3,6} as the length of three objects. If X was measured with an ordinary ruler, one can conclude that x2 is three times longer than x1. This is a quantitative analysis of X, i.e. the quantity is significant. However, if the ruler was warped, but the order of the markings is preserved, one can only conclude that x1 < x2 < x3. This is a qualitative analysis of X, i.e. the order is significant but the relative value has no meaning. This is a more robust interpretation of X. The second problem was addressed by applying a stemming algorithm (Porter, 1980) to S, such that syntactically motivated inflections are placed in an equivalent class. For example, cooking, cooked, cooks, cooker are all instances of the class cook. Unlike morphological analysers (Koskenniemi, 1983, for example), a stemming algorithm does not identify the morphemes. Its simply removes common affixes from a word, e.g. combines, combine —> combin, depart, department —> depart. Thus, similar surface forms are considered positive evidence in the similarity estimate. We propose that latent semantic analysis offers a better solution to the term matching problem. LSA (Deerwester et al., 1990) stems from work in information retrieval, where the main difficulty is formulating a similarity metric that associates a user query with the relevant documents in a database. The basic keyword search approach retrieves all documents which contain some or all of the query terms. This is inaccurate since the same concept may be described using different terms. To circumvent this, Jing and Croft (1994) developed an association thesaurus for matching semantically related words. Xu and Croft (1996) offered a trainable method call local context analysis (LCA) which replaces each query term with frequently cooccurring words. Roughly speaking, LCA computes a word co-occurrence matrix C for a training corpus. A threshold is then applied such that large values in C are replaced by 1 and other values become 0. Each row C, can be considered as a feature vector for word i. The meaning of a text is approximated by the sum of the word feature vectors. Similarity between two texts is estimated by the distance between the corresponding feature vectors (Ponte and Croft, 1997, for details). LSA is a classification approach to query expansion. The method is similar to LCA in that the &quot;meaning&quot; of a word w is represented by its relation to other words. The primary distinction is, LSA applies principle components analysis to a word similarity matrix to identify the best features for distinguishing dissimilar words. Like LCA, the meaning of a text is computed as the sum of the word feature vectors. Text similarity is measured by the cosine of the corresponding feature vectors. LSA has been shown to match human similarity judgements on a wide range of tasks (Landauer and Dumais, 1997; Wolfe et al., 1998; Wiemer-Hastings et al., 1999, for example). LSA is trained on a set of texts A = Y1, ..., with vocabulary twi, turd-. Anxm matrix A is calculated, in which, A,i is the number of times to, occurs in Si. The values are scaled according to a general form of inverse document frequency, Singular value decomposition, or SVD (Golub and van Loan, 1989) is then applied to yield B = UEVT, where XT denotes the transposed matrix of X. The columns of U and V are the eigenvectors of BBT and BTB, respectively. The diagonal values of E are the corresponding singular values, i.e. the non-negative square roots of the eigenvalues of BBT. These are sorted in descending order. BBT is a word similarity matrix, where the &quot;meaning&quot; of a word to, is expressed in terms of its dot-product with all other words {w1, 1.07,}. As a classification problem, the eigenvectors in U are the principle axes for distinguishing the word feature vectors, or rows, in BBT. In other words, the first k columns of U, or Ak, is the best approximation of BBT in k—dimensional space. Ak is the k—dimensional LSA space for A. The i—th row in Ak, or Ak(i), is the LSA feature vector for word to,. Applying SVD to W has three main benefits. First, Ak is a concise representation of W. Thus, storage and computational complexity of the similarity metric is reduced. Second, words which occur in similar contexts are represented by similar feature vectors in Ak. Finally, noise in W is removed by simply omitting the less salient dimensions in U. A sentence s, is represented by its term frequency vector A, where Li is the frequency of term j in s,. Given Ak, the &quot;meaning&quot; of s, is computed by eq. 3. Informally, s, is represented by the sum of the LSA feature vectors. Inter-sentence similarity is estimated by the cosine of the corresponding X (eq. 4, Azk is the k—th element in Ai). Since Ak is derived from the co-occurrence matrix A, the size of each training text Si E A is crucial to its performance. Work in information retrieval uses Sz = document since the aim is to distinguish entire texts. Sz = paragraph is popular in psychology experiments. However, we suspect the segmentation task may benefit from Sz = sentence. Thus, two training corpora were derived from the Brown Corpus (Marcus et al., 1993). Annotations were first removed to leave a set of tokenised raw text (1.2 million tokens). This was partitioned into 35,000 paragraphs or 104,000 sentences, as two training corpora. The parameter k adjusts the accuracy of Ak. A large k implies minor differences in the feature space are significant. Thus, they should be taken into account in the formulation of Ak. This is appropriate when the vocabulary is small and there is sufficient training data. A small k is used when A is sparse and the values in A are inaccurate. Once the similarity matrix M is calculated for the input text S, the image ranking procedure in C99 is then applied to obtain a rank matrix R (see eq. 2). Rzi is the proportion of neighbours of Mzi with a lower value than Mzi. The motivation for applying image ranking in the new algorithm is to test whether a quantitative or qualitative interpretation of the similarity values has any impact on segmentation accuracy. The hypothesis is that LSA similarity values are more accurate than cosine similarity values. Thus, image ranking should have a smaller impact on LSA than the cosine metric. The input matrix X can either be the similarity matrix M or the rank matrix R, depending on whether ranking is applied to M. Topic boundaries are identified by the divisive clustering procedure in C99. A topic segment tk is defined by its start and end sentences, sz and si, or its range tk = [i, j]. The number of intersentence similarity values in tk is O(tk) = Itk12, i.e. (j — j ± 1)2. The sum of the values in tk is /3(tk) = EzEt.1 k E.Ebk X Thus, the average inter-sentence similarity value for a segmentation T = {t,, ...,t} is defined as, The divisive clustering algorithm begins by considering the entire input document S as a coherent topic segment. This is partitioned into two segments T = ftl, t21 at a sentence boundary that maximises [IT, i.e. the most prominent topic boundary. The recursive procedure proceeds until S can no longer be subdivided. The optimal segmentation is signalled by a sharp change in [IT. For implementation details and optimisations, see (Choi, 2000a).
4 Evaluation. The following experiments aim to establish the relationship between linguistic processes (stemming, ranking, cosine metric, LSA) and segmentation error rate. The test procedure is based on that presented in (Choi, 2000a) which was derived from work in TDT (Allan et al., 1998) and previous experiments in text segmentation (Reynar, 1998, 71-73). The task is to find the most prominent topic boundaries in a concatenated text. The accuracy of a segmentation algorithm is assessed by the experiment package' described in (Choi, 2000a). A test sample is a concatenation of ten text segments. Each segment is the first n sentences of a randomly selected document from a subset2 of the Brown corpus (Marcus et al., 1993). Table 1 presents the corpus statistics. A sample is characterised by the range of n. Ti,j is a set of samples with i <n <j. T is the union of the other four test sets. Segmentation accuracy is measured by the metric proposed in (Beeferman et al., 1999). Let Tr and Tp be the reference segmentation and that proposed by an automatic procedure. k is the average segment length in T. p(samelTr, k) and p(diffiTr, k) refer to the likelihood of sentence sz and sz±k belonging to the same and different topic segment(s) in Tk p(samelTr, Tp, diff, k) is the probability of a miss, i.e. sz and sz±k are about different topics in Tk but they belong to the same topic segment in T. p(difflTr, Tp, same, k) is the probability of false alarm, i.e. two sentences are about the same topic in Tr but they belong to different segments in T. Equation 5 combines these four measures to calculate p(errorlTr, Tp, k), the probability of segmentation errors. The error rate of an algorithm is computed as the average of p(errorlTr, Tp, k) for a test set. A low error rate implies high segmentation accuracy. p(diffl , Tp , same, k)p(samelTr, k) This test procedure is not perfect. First, assessing the accuracy of an algorithm in an artificial task is inferior to a test that uses human segmented text. However, this approach does allow us to conduct a large-scale comparative study on similarity metrics which focuses on text similarity rather than topic boundary detection. Second, the error metric favours texts with short topic segments. Segmentation errors within a segment which is smaller than k are not always detected correctly. Thus, an algorithm is assessed using texts with different ranges of segment length. Although the metric is not perfect, it is significantly more accurate than the popular precision/recall metric which ignores near misses. Furthermore, the method is sufficiently accurate for this comparative study. Five degenerate algorithms define the baseline for the experiments. Be partitions a document into e = 10 segments of equal length. Br, does not propose any boundaries. B,„ assumes all potential boundaries are topic boundaries. Bb randomly selects b = 10 boundaries. B? randomly selects any number of boundaries as real boundaries. Details about Bb and B? are described in (Choi, 2000a). Table 2 shows Be performed best with an average error rate of 42%. This is the baseline for algorithms that find the e most prominent topic boundaries. B? serves as the baseline for methods that determines the optimal segmentation, i.e. the number of topic segments in a text. The aim is to relate stemming, ranking and the termination procedure in C99 with segmentation accuracy. The algorithm used in this experiment is identical to that presented in (Choi, 2000a) except tokens such as -- and - are recognised as punctuation marks and removed during pre-processing. Test results show this modification reduces error rate by 1%. An analysis of the original algorithm reveals that non-word tokens introduce errors since they are converted into a null string by the stemming algorithm (Porter, 1980). This implementation of C99 has three parameters. +r implies ranking is applied to the similarity matrix prior to divisive clustering. +s shows the stemming algorithm is used in preprocessing. +b means the algorithm finds the 10 most prominent topic boundaries, i.e. the automatic termination procedure is inactive. Test results (table 3) show ranking is crucial to C99. There is a 10% difference between row 3 and 6 for T. This confirms the cosine metric is inaccurate for short text segments but the order between values, or rank, is significant. Future experiments will establish the relationship between segment size and accuracy. Stemming is generally believed to improve segmentation accuracy. This is confirmed by the experiment results. However, we discovered that the process can introduce errors when segmenting short segments. There is a 0.7% difference between row 1 and 3 for T3,5. Finally, the termination strategy in C99 is not effective for short topic segments. There is a 6.3% improvement between row 1 and 2 for T3,5. However, its performance for larger segments is exceptional (0.6% difference between row 1 and 2 for T). The aim is to establish the relationship between LSA dimensionality, training data and accuracy. Our new algorithm, CWM, was used in this experiment. The method is identical to C99 except the stemming algorithm has been disabled and LSA is used in the formulation of the similarity matrix. Ten LSA spaces were examined. Each space is characterised by the training data and its dimensionality. s and p imply the LSA space was trained on sentences and paragraphs, respectively. The values {100, 200, 300, 400, 500} represent the dimensionality of the trained space. For instance, &quot;p, 400&quot; is a 400-dimensional space that was trained on paragraphs. Like C99, +r implies ranking is applied to the similarity matrix. +b means CWM finds the ten most prominent boundaries. Let ji be the column average. Test results (table 4) show ranked LSA (column 4) has the lowest error rate. The raw values (column 1 and 3) performed well. The 1% difference in accuracy implies the termination strategy works well with LSA. However, the same method is not applicable to the ranked LSA values (see column 2). The results in column 3 highlights the relationship between LSA space and error rate. On average, a LSA space that was trained on paragraphs ([2(p) = 11.8%) out-performed one that was trained on sentences (p(s) = 15.6%). This shows similarity is well modelled by word co-occurrence in paragraphs. It also suggests that although sentences are good for identifying words about the same topic, paragraphs are better for finding dissimilar words. Intuitively speaking, large feature vectors are expected to generate more accurate similarity values. Thus, segmentation accuracy should improve with dimensionality. The figures in column 3 show high dimensionality increases error rate. However, the figures in column 4 suggest the contrary. This implies high dimensionality improves the ranking of LSA values but is detrimental to value accuracy. Table 5 presents a summary of experiment results. C99 and C99b are the algorithms described in (Choi, 2000a). C99b,, is the same as C99b except ranking has been disabled. The 11% difference between the two shows ranking is crucial to the cosine metric. U00 and U00b are the word frequency methods proposed in (Utiyama and Isahara, 2001). CWM is the new method described in this paper. All versions of the algorithm use a LSA space that was trained on paragraphs. CWM1 is identical to C99b except the stemming algorithm has been disabled and it uses A500 to measure similarity. The results show it is more accurate than previous methods. CWM2 is the same algorithm but ranking has been disabled. The 5% difference between this and CWM1 implies ranking does improve accuracy. Finally, CWM3 is a variant of CWM2 which uses A100 to measure similarity. The 11% difference between this and C99b,_, shows LSA is more accurate than the cosine metric. The significance of our results has been confirmed by both t-test and KS-test (Press et al., 1992).
5 Conclusions. A series of experiments were conducted to establish the relationship between linguistic processes and segmentation accuracy. C99 (Choi, 2000a) was used as the test bench. In the first set of experiments, its stemming algorithm, ranking procedure and automatic termination method were systematically disabled to determine the contribution of each process to overall performance. We discovered that, first, stemming generally improves accuracy unless the topic segments are short (3 to 5 sentences). Second, ranking plays a vital role in C99. It reduces error rate by half (22% to 10%). Finally, the termination procedure in C99 is effective (0.6% difference). The method works particularly well on long topic segments (> 6 sentences). The second set of experiments focused on LSA as a similarity metric. The cosine metric in C99 was replaced by LSA. Ten different LSA spaces were examined. We discovered that LSA is twice as accurate as the cosine metric. The results also showed vocabulary difference between paragraphs is a good feature for training a similarity metric. Further investigation into the relationship between ranking, LSA dimensionality and error rate revealed that LSA values become less accurate as more dimensions are incorporated into the feature vectors. This implies the training data is noisy. However, with ranking, error rate decreases. This shows the order of LSA values becomes more accurate when more features are used. Future work will focus on document specific LSA and the termination strategy of the new algorithm. Test results have shown the termination procedure in C99 works well on LSA similarity values but not on the ranked values. We suspect the threshold selection method has to be modified. In terms of clustering, dynamic programming approaches (Ponte and Croft, 1997; Utiyama and Isahara, 2001, for example) will be examined. Finally, a LSA procedure for computing document specific similarity values will be evaluated.
Acknowledgements. Thanks are due to the anonymous reviewers for their invaluable comments; Masao Utiyama and Hitoshi Isahara for providing the U00 algorithm and detailed results; Marti Hearst for guidance on the evaluation problem; Mary McGee Wood for support and HCRC for making this work possible.