1 Introduction. In the approach to discourse structure developed in [Sid83] and [GJW86], a discourse exhibits both global and local coherence. On this view, a key element of local coherence is centering, a system of rules and constraints that govern the relationship between what the discourse is about and some of the linguistic choices made by the discourse participants, e.g. choice of grammatical function, syntactic structure, and type of referring expression (proper noun, definite or indefinite description, reflexive or personal pronoun, etc.). Pronominalization in particular serves to focus attention on what is being talked about; inappropriate use or failure to use pronouns causes communication to be less fluent. For instance, it takes longer for hearers to process a pronominalized noun phrase that is not in focus than one that is, while it takes longer to process a non-pronominalized noun phrase that is in focus than one that is not [G ui85]. The [G3W86] centering model is based on the following assumptions. A discourse segment consists of a sequence of utterances U1, , Um. With each utterance Un is associated a list of forward-looking centers, C f(Un), consisting of those discourse entities that are directly realized or realized' by linguistic expressions in the utterance. Ranking of an entity on this list corresponds roughly to the likelihood that it will be the primary focus of subsequent discourse; the first entity on this list is the preferred center, Cp(Un). Un actually centers, or is &quot;about&quot;, only one entity at a time, the backward-looking center, Cb(Un). The backward center is a confirmation of an entity that has already been introduced into the discourse; more specifically, it must be realized in the immediately preceding utterance, Un_.1. There are several distinct types of transitions from one utterance to the next. The typology of transitions is based on two factors: whether or not the center of attention, Cb, is the same from Un_1 to Un, and whether or not this entity coincides with the preferred center of Un. Definitions of these transition types appear in figure 1. These transitions describe how utterances are linked together in a coherent local segment of discourse. If a speaker has a number of propositions to express, one very simple way to do this coherently is to express all the propositions about a given entity (continuing) before introducing a related entity As is evident in constraint 3, ranking of the items on the forward center list, Cf, is crucial. We rank the items in Cf by obliqueness of grammatical relation of the subcategorized functions of the main verb: that is, first the subject, object, and object2, followed by other subcategorized functions, and finally, adjuncts. This captures the idea in [GJW86] that subjecthood contributes strongly to the priority of an item on the Cf list. (retaining) and then shifting the center to this new entity. See figure 2. Retaining may be a way to signal an intention to shift. While we do not claim that speakers really behave in such an orderly fashion, an algorithm that expects this kind of behavior is more successful than those which depend solely on recency or parallelism of grammatical function. The interaction of centering with global focusing mechanisms and with other factors such as intentional structure, semantic selectional restrictions, verb tense and aspect, modality, intonation and pitch accent are topics for further research. Note that these transitions are more specific than focus movement as described in [Sid83]. The extension we propose makes them more specific still. Note also that the Cb of [GJW86] corresponds roughly to Sidner's discourse focus and the Cf to her potential foci. The formal system of constraints and rules for centering, as we have interpreted them from [GJW86], are as follows. For each Un in U1, , CONTINUING... Un+i: Carl works at HP on the Natural Language We are aware that this ranking usually coincides with surface constituent order in English. It would be of interest to examine data from languages with relatively freer constituent order (e.g. German) to determine the influence of constituent order upon centering when the grammatical functions are held constant. In addition, languages that provide an identifiable topic function (e.g. Japanese) suggest that topic takes precedence over subject. The part of the HPSG system that uses the centering algorithm for pronoun binding is called the pragmatics processor. It interacts with another module called the semantics processor, which computes representations of intrasentential anaphoric relations, (among other things). The semantics processor has access to information such as the surface syntactic structure of the utterance. It provides the pragmatics processor with representations which include of a set of reference markers. Each reference marker is contraindexed2 with expressions with which it cannot co-specify3. Reference markers also carry information about agreement and grammatical function. Each pronominal reference marker has a unique index from A1, , An and is displayed in the figures in the form [POLLARD:Al], where POLLARD is the semantic representation of the co-specifier. For non-pronominal reference markers the surface string is used as the index. Indices for indefinites are generated from X1, .. •
2 Extension. The constraints proposed by [GJW86] fail in certain examples like the following (read with pronouns destressed): Brennan drives an Alfa Romeo. She drives too fast. Friedman races her on weekends. She often beats her. This example is characterized by its multiple ambiguous pronouns and by the fact that the final utterance achieves a shift (see figure 4). A shift is inevitable because of constraint 3, which states that the Cb(Un) must equal the Cp(Un_i) (since the Cp(U,i) is directly realized by the subject of Un, &quot;Friedman&quot;). However the constraints and rules from [GJW86] would fail to make a choice here between the co-specification possibilities for the pronouns in Un. Given that the transition is a shift, there seem to be more and less coherent ways to shift. Note that the three items being examined in order to characterize the transition between each pair of anchor?' are the
CONTINUING SHIFTING-I RETAINING SHIFTING. Cb of Un_1, the Cb of Un, and the Cp of U. By [GJW86] a shift occurs whenever successive Cb's are not the same. This definition of shifting does not consider whether the Cb of Un and the Cp of Un are equal. It seems that the status of the Cp of Un should be as important in this case as it is in determining the retaining/continuing distinction. Therefore, we propose the following extension which handles some additional cases containing multiple ambiguous pronouns: we have extended rule 2 so that there are two kinds of shifts. A transition for Un is ranked more highly if Cb(Un) = CP(U); this state we call shifting-1 and it represents a more coherent way to shift. The preferred ranking is continuing >- retaining shifting-1 >- shifting (see figure 3). This extension enables us to successfully bind the &quot;she&quot; in the final utterance of the example in figure 4 to &quot;Friedman.&quot; The appendix illustrates the application of the algorithm to figure 4. Kameyama [Kam86] has proposed another extension to the [G3W86] theory — a property-sharing constraint which attempts to enforce a parallellism between entities in successive utterances. She considers two properties: SUB/ and IDENT . With her extension, subject pronouns prefer subject antecedents and non-subject pronouns prefer non-subject antecedents. However, structural parallelism is a consequence of our ordering the Cf list by grammatical function and the preference for continuing over retaining. Furthermore, the constraints suggested in [GJW86] succeed in many cases without invoking an independent structural parallelism constraint, due to the distinction between continuing and retaining, which Kameyama fails to consider. Her example which we reproduce in figure 5 can also be accounted for using the continuing/retaining distinctions. The third utterance in this example has two interpretations which are both consistent with the centering rules and constraints. Because of rule 2, the interpretation in figure 5 is preferred over the one in figure 6.
3 Algorithm for centering and pronoun binding. There are three basic phases to this algorithm. First the proposed anchors are constructed, then they are filtered, and finally, they are classified and ranked. The proposed anchors represent all the cospecification relationships available for this utterance. Each step is discussed and illustrated in figure 7. It would be possible to classify and rank the proposed anchors before filtering them without any other changes to the algorithm. In fact, using this strategy This filter doesn't eliminate any of the proposed anchors in this example. Even though [A4] and [A5] are contraindexed we have not proposed the same co-specifier due to agreement. This filter eliminates proposed anchors ii, iii, iv. This filter doesn't eliminate any of the proposed anchors. The proposed Cb was realized as a pronoun. Anchor i is classified as a retention based on the transition state definition. one could see if the highest ranked proposal passed all the filters, or if the next highest did, etc. The three filters in the filtering phase may be done in parallel. The example we use to illustrate the algorithm is in figure 2.
4 Discussion. The goal of the current algorithm design was conceptual clarity rather than efficiency. The hope is that the structure provided will allow easy addition of further constraints and preferences. It would be simple to change the control structure of the algorithm so that it first proposed all the continuing or retaining anchors and then the shifting ones, thus avoiding a precomputation of all possible anchors. [GJW86] states that a realization may contribute more than one entity to the Cf(U). This is true in cases when a partially specified semantic description is consistent with more than one interpretation. There is no need to enumerate explicitly all the possible interpretations when constructing possible Cf(U)'s6, as long as the associated semantic theory allows partially specified interpretations. This also holds for entities not directly realized in an utterance. On our view, after referring to &quot;a house&quot; in If, a reference to &quot;the door&quot; in Un4.1 might be gotten via inference from the representation for &quot;a house&quot; in Cf( U,,). Thus when the proposed anchors are constructed there is no possibility of having an infinite number of potential Cf's for an utterance of finite length. Another question is whether the preference ordering of transitions in constraint 3 should always be the same. For some examples, particularly where Un contains a single pronoun and Un...1 is a retention, some informants seem to have a preference for shifting, whereas the centering algorithm chooses a continuation (see figure 8). Many of our informants have no strong preference as to the co-specification of the unstressed &quot;She&quot; in Un+4. Speakers can avoid ambiguity by stressing a pronoun with respect to its phonological environment. A computational system for understanding may need to explicitly acknowledge this ambiguity. A computational system for generation would try to plan a retention as a signal of an impending shift, so that after a retention, a shift would be preferred rather than a continuation. Of course the local approach described here does not provide all the necessary information for interpreting pronouns; constraints are also imposed by world knowledge, pragmatics, semantics and phonology. There are other interesting questions concerning the centering algorithm. How should the centering algorithm interact with an inferencing mechanism? Should it make choices when there is more than one proposed anchor with the same ranking? In a database query system, how should answers be incorporated into the discourse model? How does centering interact with a treatment of definite/indefinite NP's and quantifiers? We are exploring ideas for these and other extensions to the centering approach for modeling reference in local discourse.
5 Acknowledgements. We would like to thank the following people for their help and insight: Hewlett Packard Lab's Natural Language group, CSLI's DIA group, Candy Sidner, Dan Flickinger, Mark Gawron, John Nerbonne, Tom Wasow, Barry Arons, Martha Pollack, Aravind Joshi, two anonymous referees, and especially Barbara Grosz.
6 Appendix. This illustrates the extension in the same detail as the example we used in the algorithm. The numbering here corresponds to the numbered steps in the algorithm figure 7. The example is the last utterance from figure 4. EXAMPLE: She often beats her.
2.. (a) Filter by contraindices. Anchors i, iv, v, viii, ix, xii, xiii, xvi are eliminated since [A9] and [A10] are contraindexed.