of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators). It is 'shallow' in that no atof which thetempt is made to fully analyse unbounded dependencies. However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977), by Chomsky-adjunction to maximal projections of adjuncts (XP XP Adjunct) as opposed to 'government' of arguments (i.e. arguments are sisters within projections; X1 XO Argl... ArgN). more, all analyses are rooted (in S) so the grammar assigns global, shallow and often 'spurious' analyses to many sentences. There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class lemmas of arguments, such as suband so forth, to classify patterns as evidence for one of the 160 subcategorization classes. Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles. Currently, the coverage of this grammar—the proportion of sentences for which at least one analysis is found—is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus. Wide coverage is important since information is acquired only from successful parses. The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second. 2.2 The Extractor, Classifier and Evaluator The extractor takes as input the ranked analyses from the probabilistic parser. It locates the subanalyses around the predicate, finding the constituents identified as complements inside each subanalysis, and the subject clause preceding it. Instances of passive constructions are recognized and treated specially. The extractor returns the predicate, the VSUBCAT value, and just the heads of the complements (except in the case of PPs, where it returns the PSUBCAT value, the preposition head, and the heads of the PP's complements). The subcategorization classes recognized by the classifier were obtained by manually merging the classes exemplified in the COMLEX Syntax and ANLT dictionaries and adding around 30 classes found by manual inspection of unclassifiable patterns for corpus examples during development of the system. These consisted of some extra patterns for phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some for non-passivizable patterns with a surface direct object, and some for rarer combinations of governed preposition and complementizer combinations. The classifier filters out as unclassifiable around 15% of patterns found by the extractor when run on all the patternsets extracted from the Susanne corpus. This demonstrates the value of the classifier as a filter of spurious analyses, as well as providing both translation between extracted patterns and two existing subcategorization dictionaries and a definition of the target subcategorization dictionary. The evaluator builds entries by taking the patterns for a given predicate built from successful parses and records the number of observations of each subcategorization class. Patterns provide several types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a predicate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern. Currently, we simply select the pattern supported by the highest ranked parse. However, we are experimenting with alternative approaches. The resulting set of putative classes for a predicate are filtered, following Brent (1993), 358 by hypothesis testing on binomial frequency data. Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis of the behaviour of the extractor. We estimate this probability more directly by first extracting the number of verbs which are members of each class in the ANLT dictionary (with intuitive estimates for the membership of the novel classes) and converting this to a probability of class membership by dividing by the total number of verbs in the dictionary; and secondly, by multiplying the complement of these probabilities by the probability of a pattern for class i, defined as the number of patterns for i extracted from the Susanne corpus divided by the total number of patterns. So, p(v -i), the probability of verb v not of class i occurring with a pattern for class i is: -i) = (1 Ipatterns_f or_i n! n, p) = mi(p im The probability of the event happening m or more times is: n,p) = i=m -i)) the probability that m or more occurrences of patterns for i will occur with a verb which is not a member of i, given n occurrences of that verb. Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have observed for the verb to be in class 2.3 Discussion Our approach to acquiring subcategorization classes is predicated on the following assumptions: • most sentences will not allow the application of all possible rules of English complementation; • some sentences will be unambiguous even given indeterminacy of the (1993:249-253) provides a detailed explanation and justification for the use of this measure. fact, 5% of sentences in Susanne are assigned only a single analysis by the grammar. • many incorrect analyses will yield patterns which are unclassifiable, and are thus filtered out; • arguments of a specific verb will occur with greater frequency than adjuncts (in potential argument positions); • the patternset generator will incorrectly output for certain classes more often than others; and even a highest ranked for i is only a probabilistic cue for membership of i, so membership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i. This simple automated, hybrid linguistic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicog- (Meyers at., who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature. Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often 'understood' or implied if missing. Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993)) which require recognition that the 'same' argument, defined usually by semantic class / thematic role, is occurring across argument positions. We hope to exploit this information where possible at a later stage in the development of our approach. However, recognizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes. At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes. 3 Experimental Evaluation 3.1 Lexicon Evaluation — Method In order to test the accuracy of our system (as developed so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor & Knowles, 1988) and LOB corpora (Garat., total of 1.2 million words—and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each. These verbs, listed in Figure 2, were chosen at random, subject to the constraint that they exhibited multiple complementation patterns. The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting lanit_verbs1 'patterns' The binomial distribution gives the probability of an with probability exactly m times out of n attempts: 359 successful analyses. The citations from which entries were derived totaled approximately 70K words. The results were evaluated against a merged entry for these verbs from the ANLT and COMLEX Syntax dictionaries, and also against a manual analysis of the corpus data for seven of the verbs. The process of evaluating the performance of the system relative to the dictionaries could, in principle, be reduced to automated report of precision of correct subcategorization classes to all classes found) of correct classes found in the dictionary entry). However, since there are disagreements between the dictionaries and there are classes found in the corpus data that are not contained in either dictionary, we report results relative both to a manually merged entry from ANLT and COMLEX, and also, for seven of the verbs, to a manual analysis of the actual corpus data. The latter analysis is necessary because precision and recall measures against the merged entry will still tend to yield inaccurate results as the system cannot acquire classes not exemplified in the data, and may acquire classes incorrectly absent from the dictionaries. We illustrate these problems with reference to there is overlap, but not agreement between the COMLEX and ANLT entries. Thus, predict that occur with a sentential complement and dummy subject, but only ANLT predicts the possibility of a `wh' complement and only COMLEX predicts the (optional) presence of a PP[to] argument with the sentential complement. One ANLT entry covers two COMLEX entries given the different treatment of the relevant complements but the classifier keeps them distinct. The corpus for examples of further classes we judge valid, in which take a and infinitive complement, as in seems to to be insane, a passive participle, as in depressed. comparison illustrates the problem of errors of omission common to computational lexicons constructed manually and also from dictionaries. All classes for exemplified in the corpus data, but for example, eight classes (out of a possible 27 in the merged entry) are not present, so comparison only to the merged entry would give an unreasonably low estimate of recall. Lexicon Evaluation Figure 2 gives the raw results for the merged entries and corpus analysis on each verb. It shows the of positives correct classes proby our system, positives incorrect proposed by our system, and negatives classes not proposed by our system, as judged against the merged entry, and, for seven of the verbs, against the corpus analysis. It also shows, in the final column, the number of sentences from which classes were extracted. Dictionary (14 verbs) Corpus (7 verbs) Precision Recall 65.7% 76.6% 35.5% 43.4% Figure 3: Type precision and recall Ranking Accuracy ask 75.0% begin 100.0% believe 66.7% cause 100.0% give 70.0% seem 75.0% swing 83.3% Mean 81.4% Figure 4: Ranking accuracy of classes Figure 3 gives the type precision and recall of our system's recognition of subcategorization classes as evaluated against the merged dictionary entries (14 verbs) and against the manually analysed corpus data (7 verbs). The frequency distribution of classes is highly skewed: for example for there are 107 instances of the most common class in the corpus data, but only 6 instances in total of the least common four classes. More generally, for the manually analysed verbs, almost 60% of the false negatives have only one or two exemplars each in the corpus citations. None of them are returned by because the binomial filter always rejects classes hypothesised on the basis of such little evidence. In Figure 4 we estimate the accuracy with which our system ranks true positive classes against the correct ranking for the seven verbs whose corpus input was manually analysed. We compute this measure by calculating the percentage of pairs of classes at positions (n, m) s.t. n < m in the system ranking that are ordered the same in the correct ranking. This gives us an estimate of the accuracy of the relative frequencies of classes output by the system. For each of the seven verbs for which we undertook a corpus analysis, we calculate the token recall of our system as the percentage (over all exemplars) of true positives in the corpus. This gives us an estimate of the parsing performance that would result from providing a parser with entries built using the system, shown in Figure 5. Further evaluation of the results for these seven verbs reveals that the filtering phase is the weak in the systeni. There are only 13 negatives which the system failed to propose, each exemplified in the data by a mean of 4.5 examples. On the other there are 67 negatives by an mean of 7.1 examples which should, ide- 360 Merged TP FP Entry Corpus TP FP Data No. of FN FN Sentences ask 9 0 18 9 0 10 390 begin 4 1 7 4 1 7 311 believe 4 4 11 4 4 8 230 cause 2 3 6 2 3 5 95 expect 6 5 3 - - - 223 find 5 7 15 - - - 645 give 5 2 11 5 2 5 639 help 6 3 8 - - - 223 like 3 2 7 - - - 228 move 4 3 9 - - - 217 produce 2 1 3 - - 152 provide 3 2 6 - - - 217 seem 8 1 4 8 1 4 534 swing 4 0 10 4 0 8 45 Totals 65 34 118 36 11 47 4149 Figure 2: Raw results for test of 14 verbs Token Recall ask 78.5% begin 73.8% believe 34.5% cause 92.1% give 92.2% seem 84.7% swing 39.2% Mean 80.9% Figure 5: Token recall have been accepted by the filter, and 11 should have been rejected. The performance of the filter for classes with less than 10 exemplars is around chance, and a simple heuristic of accepting all classes with more than 10 exemplars would have produced broadly similar results for these verbs. The filter may well be performing poorly because the probability of generating a subcategorization class for a given verb is often lower than the error probability for that class. 3.3 Parsing Evaluation In addition to evaluating the acquired subcategorization information against existing lexical resources, we have also evaluated the information in the context of an actual parsing system. In particular we wanted to establish whether the subcategorization frequency information for individual verbs could be used to improve the accuracy of a parser that uses statistical techniques to rank analyses. The experiment used the same probabilistic parser and tag sequence grammar as are present in the acquisition system (see references above)—although the experiment does not in any way rely on the Mean Recall Precision crossings 'Baseline' Lexicalised 1.00 70.7% 72.3% 0.93 71.4% 72.9% Figure 6: GEIG evaluation metrics for parser against Susanne bracketings parsers or grammars being the same. We randomly selected a test set of 250 in-coverage sentences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the 'baseline' accuracy of the unlexicalized parser on the sentences using the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision (e.g. Gral., see figure Next, we colwords in the test corpus tagged as possibly being verbs (giving a total of 356 distinct lemmas) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded. We acquired subcategorization and associated frequency information from the citations, in the process successfully parsing 380K words. We then parsed the test set, with each verb subcategorization possibility weighted by its raw frequency score, and using the naive add-one smoothing technique to allow for omitted possibilities. The GEIG measures for the lexicalized parser show a 7% improvement in the crossing bracket score (figure 6). the existing test corpus this is not statisti- 'Carroll & Briscoe (1996) use the same test set, although the baseline results reported here differ slightly due to differences in the mapping from parse trees to Susanne-compatible bracketings. 361 significant at the 95% level p = if the pattern of differences were maintained over a larger test set of 470 sentences it would be significant. We expect that a more sophisticated smoothing technique, a larger acquisition corpus, and extensions to the system to deal with nominal and adjectival predicates would improve accuracy still further. Nevertheless, this experiment demonstrates that lexicalizing a grammar/parser with subcategorization frequencies can appreciably improve the accuracy of parse ranking. 4 Related Work Brent's (1993) approach to acquiring subcategorization is based on a philosophy of only exploiting unambiguous and determinate information in unanalysed corpora. He defines a number of lexical patterns (mostly involving closed class items, such as pronouns) which reliably cue one of five subcategorization classes. Brent does not report comprehensive results, but for one class, sentential complement verbs, he achieves 96% precision and 76% recall at classifying individual tokens of 63 distinct verbs as exemplars or non-exemplars of this class. He does not attempt to rank different classes for a given verb. al. utilise a PoS tagged corpus and finite-state NP parser to recognize and calculate the relative frequency of six subcategorization classes. They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that incorrect noun phrase boundary detection accounts for the majority of errors. They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the second most frequent class, if there was one. Our system rankings include all classes for each verb, from a total of 160 classes, and average 81.4% correct. Manning (1993) conducts a larger experiment, also using a PoS tagged corpus and a finite-state NP parser, attempting to recognize sixteen distinct complementation patterns. He reports that for a test sample of 200 tokens of 40 verbs in running text, the acquired subcategorization dictionary listed the appropriate entry for 163 cases, giving a token recall of 82% (as compared with 80.9% in our experiment). He also reports a comparison of acquired entries for verbs to the entries given in the Advanced Dictionary of Current English 1989) on which his system achieves a precision of 90% and a recall of 43%. His system averages 3.48 subentries (maximum 10)—less then half the number produced in our experiment. It is not clear what level of evidence the performance of Manning's system is based on, but the system was applied to 4.1 million words of text (c.f. our 1.2 million words) and the verbs are all common, so it is likely that considerably more exemplars of each verb were available. 5 Conclusions and Further Work The experiment and comparison reported above suggests that our more comprehensive subcategorization class extractor is able both to assign classes to individual verbal predicates and also to rank them according to relative frequency with comparable accuracy to extant systems. We have also demonstrated that a subcategorization dictionary built with the system can improve the accuracy of a probabilistic parser by an appreciable amount. The system we have developed is straightforwardly extensible to nominal and adjectival predicates; the existing grammar distinguishes nominal and adjectival arguments from adjuncts structurally, so all that is required is extension of the classifier. Developing an analogous system for another language would be harder but not infeasible; similar taggers and parsers have been developed for a number of languages, but no extant subcategorization dictionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman & Sterling (1992), Poznanski & Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. Boguraev & Briscoe, 1987).