Introduction. Automatic sentence compression can be broadly described as the task of creating a grammaticalsummary of a single sentence with minimal information loss. It has recently attracted much attention, in part because of its relevance to applications. Examples include the generation of sub titles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w 1 , w 2 . . . w n , a compression is formed by dropping any subset of these words (Knight c ? 2008. Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight andMarcu, 2002; Turner and Charniak, 2005; McDon ald, 2006). Furthermore, constraining the problemto word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from in stantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007;Turner and Charniak, 2005), to large-margin learn ing (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008). However, the simplification also renders the tasksomewhat artificial. There are many rewrite operations that could compress a sentence, besides deletion, including reordering, substitution, and inser tion. In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). Therefore, in this paper we consider sentence compression from a more general perspective and generate abstracts rather than extracts. In this framework, the goal is to find a summary of theoriginal sentence which is grammatical and conveys the most important information without necessarily using the same words in the same or der. Our task is related to, but different from, paraphrase extraction (Barzilay, 2003). We must not only have access to paraphrases (i.e., rewrite rules), but also be able to combine them in order to generate new text, while attempting to produce a shorter resulting string. Quirk et al (2004) present an end-to-end paraphrasing system inspired byphrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. However, their model is limited to lexical substitution ? no reordering takes place ? and is 137 lacking the compression objective.Once we move away from extractive compres sion we are faced with two problems. First, wemust find an appropriate training set for our abstractive task. Compression corpora are not natu rally available and existing paraphrase corpora do not normally contain compressions. Our second problem concerns the modeling task itself. Ideally, our learning framework should handle structural mismatches and complex rewriting operations.In what follows, we first present a new cor pus for abstractive compression which we created by having annotators compress sentences while rewriting them. Besides obtaining useful data formodeling purposes, we also demonstrate that ab stractive compression is a meaningful task. We then present a tree-to-tree transducer capable of transforming an input parse tree into a compressed parse tree. Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)),a formalism that can account for structural mismatches, and is trained discriminatively. Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task. We present a noveltree-to-tree grammar extraction method which acquires paraphrases from bilingual corpora and ensure coherent output by including a ngram language model as a feature. We also develop a number of loss functions suited to the abstractive compression task. We hope that some of the work described here might be of relevance to other gen eration tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999).
Abstractive Compression Corpus. . A stumbling block to studying abstractive sentence compression is the lack of widely available corpora for training and testing. Previous work has beenconducted almost exclusively on Ziff-Davis, a cor pus derived automatically from document abstractpairs (Knight and Marcu, 2002), or on human authored corpora (Clarke, 2008). Unfortunately,none of these data sources are suited to our problem since they have been produced with a single rewriting operation, namely word deletion. Al though there is a greater supply of paraphrasing corpora, such as the Multiple-Translation Chinese (MTC) corpus 1 and theMicrosoft Research (MSR) Paraphrase Corpus (Quirk et al, 2004), they are also not ideal, since they have not been created 1 Available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1. with compression in mind. They contain amplerewriting operations, however they do not explic itly target information loss. For the reasons just described, we created our own corpus. We collected 30 newspaper articles (575 sentences) from the British National Corpus (BNC) and the American News Text corpus, forwhich we obtained manual compressions. In or der to confirm that the task was feasible, five of these documents were initially compressed by two annotators (not the authors). The annotators weregiven instructions that explained the task and defined sentence compression with the aid of examples. They were asked to paraphrase while preserv ing the most important information and ensuring the compressed sentences remained grammatical.They were encouraged to use any rewriting opera tions that seemed appropriate, e.g., to delete words, add new words, substitute them or reorder them.Assessing inter-annotator agreement is notori ously difficult for paraphrasing tasks (Barzilay, 2003) since there can be many valid outputs for a given input. Also our task is doubly subjective in deciding which information to remove from the sentence and how to rewrite it. In default of an agreement measure that is well suited to the task and takes both decisions into account, we assessedthem separately. We first examined whether the annotators compressed at a similar level. The com pression rate was 56% for one annotator and 54% for the other. 2 We also assessed whether theyagreed in their rewrites by measuring BLEU (Pap ineni et al, 2002). The inter-annotator BLEU score was 23.79%, compared with the source agreement BLEU of only 13.22%. Both the compression rateand BLEU score indicate that the task is welldefined and the compressions valid. The remaining 25 documents were compressed by a single an notator to ensure consistency. All our experiments used the data from this annotator. 3Table 1 illustrates some examples from our corpus. As can be seen, some sentences contain a single rewrite operation. For instance, a PP is para phrased with a genitive (see (1)), a subordinate clause with a present participle (see (2)), a passive sentence with an active one (see (3)). However, in most cases many rewrite decisions take place allat once. Consider sentence (4). Here, the conjunc tion high winds and snowfalls is abbreviated to 2 The term ?compression rate? refers to the percentage of words retained in the compression. 3 Available from http://homepages.inf.ed.ac.uk/ tcohn/paraphrase. 138 1a. The future of the nation is in your hands. 1b. The nation?s future is in your hands. 2a. As he entered a polling booth in Katutura, he said. 2b. Entering a polling booth in Katutura, he said. 3a. Mr Usta was examined by Dr Raymond Crockett, a Harley Street physician specialising in kidney disease.3b. Dr Raymond Crockett, a Harley Street physician, ex amined Mr Usta. 4a. High winds and snowfalls have, however, grounded at a lower level the powerful US Navy Sea Stallion helicopters used to transport the slabs. 4b. Bad weather, however, has grounded the helicopters transporting the slabs. 5a. To experts in international law and relations, the USaction demonstrates a breach by a major power of in ternational conventions.5b. Experts say the US are in breach of international con ventions.Table 1: Compression examples from our corpus; (a) sen tences are the source, (b) sentences the target. bad weather and the infinitive clause to transport to the present participle transporting. Note that the prenominal modifiers US Navy Sea Stallion and the verb used have been removed. In sentence (5), the verb say is added and the NP a breach by amajor power of international conventions is para phrased by the sentence the US are in breach of international conventions.
Basic Model. . Our work builds on the model developed by Cohnand Lapata (2007). They formulate sentence compression as a tree-to-tree rewriting task. A syn chronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. Each grammar rule is assigned a weight, and these weights are learnt in discriminative training. For prediction, a specialised generation algorithmfinds the best scoring compression using the grammar rules. Cohn and Lapata apply this model to ex tractive compression with state-of-the-art results. This model is appealing for our task for severalreasons. Firstly, the synchronous grammar provides expressive power to model consistent syntactic effects such as reordering, changes in nonterminal categories and lexical substitution. Sec ondly, it is discriminatively trained, which allowsfor the incorporation of all manner of powerful features. Thirdly, the learning framework can be tai lored to the task by choosing an appropriate loss function. In the following we describe their model in more detail with emphasis on the synchronous grammar, the model structure, and the predictionand training algorithms. Section 4 presents our ex tensions and modifications. Grammar The grammar defines a space oftree pairs over uncompressed and compressed sen Grammar rules: ?S, S? ?NP 1 VBD 2 NP 3 , NP 1 VBD 2 NP 3 ? ?S, S? ?NP 1 VBD 2 NP 3 , NP 3 was VBN 2 by NP 1 ? ?NP, NP? ?he, him? ?NP, NP? ?he, he? ?NP, NP? ?he, Peter? ?VBD, VBN? ?sang, sung? ?NP, NP? ?a song, a song? Input tree: [S [NP He NP [VP sang VBD [NP a DT song NN ]]] Output trees: [S [NP He] [VP sang [NP a song]]] [S [NP Him] [VP sang [NP a song]]] [S [NP Peter] [VP sang [NP a song]]] [S [NP A song] [VP was [VP sung [PP by he]]]] [S [NP A song] [VP was [VP sung [PP by him]]]] [S [NP A song] [VP was [VP sung [PP by Peter]]]] Figure 1: Example grammar and the output trees it licences for an input tree. The numbered boxes in the rules denote linked variables. Pre-terminal categories are not shown for the output trees for the sake of brevity. tences, which we refer to henceforth as the source and target. We use the grammar to find the set of sister target sentences for a given source sentence.Figure 1 shows a toy grammar and the set of possi ble target (output) trees for the given source (input)tree. Each output tree is created by applying a se ries of grammar rules, where each rule matches a fragment of the source and creates a fragment of the target tree. A rule in the grammar consists of a pair of elementary trees and a mapping between the variables (frontier non-terminals) in both trees. A derivation is a sequence of rules yielding a target tree with no remaining variables. Cohn and Lapata (2007) extract a STSG froma parsed, word-aligned corpus of source and target sentences. Specifically, they extract the mini mal set of synchronous rules which can describe each tree pair. These rules are minimal in the sensethat they cannot be made smaller (e.g., by replac ing a subtree with a variable) while still honouring the word-alignment. Decoding The grammar allows us to search for all sister trees for a given tree. The decoder maximises over this space: y ? =argmax y:S(y)=x ?(y) (1) where ?(y) = ? r?y ??(r, S(y)), ?? (2) Here x is the source (uncompressed) tree, y is a derivation which produces the source tree, S(y) = x, and a target tree, T (y), 4and r is a gram mar rule. The ? function scores the derivation and 4 Equation 1 optimises over derivations rather than target trees to allow tractable inference. 139 is defined in (2) as a linear function over the rules used. Each rule?s score is an inner product between its feature vector, ?(r,y S), and the model parame ters, ?. The feature functions are set by hand, while the model parameters are learned in training. The maximisation problem in (1) can be solved efficiently using a dynamic program. Derivations will have common sub-structures whenever they transduce the same source sub-tree into a target sub-tree. This is captured in a chart, leading to an efficient bottom-up algorithm. The asymptotic time complexity of this search is O(SR) where S is the number of source nodes andR is the number of rules matching a given node. Training The model is trained using SVM struct , a large margin method for structured output problems (Joachims, 2005; Tsochantaridis et al, 2005). This training method allows the use of a configurable loss function, ?(y ? ,y), whichmeasures the extent to which the model?s predic tion, y, differs from the reference, y ? . Central. to training is the search for a derivation which is both high scoring and has high loss compared to the gold standard. 5 This requires finding the maximiser of H(y) in one of: H s = (1? ??(y ? )??(y), ??)?(y ? ,y) H m = ?(y ? ,y)? ??(y ? )??(y), ?? (3) where the subscripts s and m denote slack and margin rescaling, which are different formulations of the training problem (see Tsochantaridis et al (2005) and Taskar et al (2003) for details). The search for the maximiser of H(y) in (3) requires the tracking of the loss value. This can be achieved by extending the decoding algorithmsuch that the chart cells also store the loss param eters (e.g., for precision, the number of true and false positives (Joachims, 2005)). Consequently, this extension leads to a considerably higher time and space complexity compared to decoding. For example, with precision loss the time complexity is O(S 3 R) as each step must consider O(S 2) pos sible loss parameter values.
Extensions. . In this section we present our extensions of Cohnand Lapata?s (2007) model. The latter was de signed with the simpler extractive compression in mind and cannot be readily applied to our task. 5 Spurious ambiguity in the grammar means that there areoften many derivations linking the source and target. We fol low Cohn and Lapata (2007) by choosing the derivation with the most rules, which should provide good generalisation. Grammar It is relatively straightforward to extract a grammar from our corpus. This grammar will contain many rules encoding deletions and structural transformations but there will be many unobserved paraphrases, no matter how good the extraction method (recall that our corpus consistssolely of 565 sentences). For this reason, we ex tract a grammar from our abstractive corpus in the manner of Cohn and Lapata (2007) (see Section 5for details) and augment it with a larger grammar obtained from a parallel bilingual corpus. Crucially, our second grammar will not contain com pression rules, just paraphrasing ones. We leave itto the model to learn which rules serve the com pression objective. Our paraphrase grammar extraction method uses bilingual pivoting to learn paraphrases over syntax tree fragments, i.e., STSG rules. Pivoting treats the paraphrasing problem as a two-stage translation process. Some English text is translated to a foreign language, and then translated back into English (Bannard and Callison-Burch, 2005): p(e ? |e) = ? f p(e ? |f)p(f |e) (4) where p(f |e) is the probability of translating an English string e into a foreign string f and p(e ?|f) the probability of translating the same for eign string into some other English string e ? . We. thus obtain English-English translation probabili ties p(e ? |e) by marginalizing out the foreign text.Instead of using strings (Bannard and CallisonBurch, 2005), we use elementary trees on the En glish side, resulting in a monolingual STSG. Weobtain the elementary trees and foreign strings us ing the GKHM algorithm (Galley et al, 2004). This takes as input a bilingual word-aligned corpus with trees on one side, and finds the minimal set of tree fragments and their corresponding strings which is consistent with the word alignment. This process is illustrated in Figure 2 where the aligned pair on the left gives rise to the rules shown onthe right. Note that the English rules and for eign strings shown include variable indices where they have been generalised. We estimate p(f |e) and p(e ? |f) from the set of tree-to-string rules and then then pivot each tree fragment to produce STSG rules. Figure 3 illustrates the process for the [VP does not VP] fragment. Modeling and Decoding Our grammar is much larger and noisier than a grammar extractedsolely for deletion-based compression. So, in order to encourage coherence and inform lexical se 140 SNP VP VBZ does RB goHe not ne pasIl va PRP VP NP He Il PRP go vaVP VP VBZ does RB not ne pas VP S NP VP 1 2 1 1 1 2 Figure 2: Tree-to-string grammar extraction using the GHKM algorithm, showing the aligned sentence pair and the resulting rules as tree fragments and their matching strings. The boxed numbers denote variables. VP VBZ does RB not ne pas VP n ' ne ne peut ... VP MD will RB not VB VP VBP do RB not VB 1 1 1 1 1 1 1 Figure 3: Pivoting the [VP does not VP] fragment. lection we incorporate a ngram language model(LM) as a feature. This requires adapting the scor ing function, ?, in (2) to allow features over target ngrams: ?(y) = ? r?y ??(r, S(y)), ??+ ? m?T (y) ??(m,S(y)), ?? (5)where m are the ngrams and ? is a new fea ture function over these ngrams (we use only one ngram feature: the trigram log-probability). Sadly, the scoring function in (5) renders the chart-based search used for training and decoding intractable.In order to provide sufficient context to the chart based algorithm, we must also store in each chart cell the n ? 1 target tokens at the left and right edges of its yield. This is equivalent to using as our grammar the intersection between the original grammar and the ngram LM (Chiang, 2007), and increases the decoding complexity to an infeasible O(SRL 2(n?1)V )whereL is the size of the lexicon. We adopt a popular approach in syntax-inspiredmachine translation to address this problem (Chi ang, 2007). The idea is to use a beam-search overthe intersection grammar coupled with the cube pruning heuristic. The beam limits the number ofitems in a given chart cell to a fixed constant, re gardless of the number of possible LM contexts and non-terminal categories. Cube-pruning furtherlimits the number of items considered for inclu sion in the beam, reducing the time complexity to a more manageable O(SRBV ) where B is the beam size. We refer the interested reader to Chiang (2007) for details. Training The extensions to the model in (5)also necessitate changes in the training proce dure. Recall that training the basic model of Cohn and Lapata (2007) requires finding the maximiserof H(y) in (3). Their model uses a chart-based al gorithm for this purpose. As in decoding we also use a beam search for training, thereby avoiding the exponential time complexity of exact search.The beam search requires an estimate of the qual ity for incomplete derivations. We use the margin rescaling objective, H m in (3), and approximatethe loss using the current (incomplete) loss param eter values in each chart cell. We use a wide beam of 200 unique items or 500 items in total to reduce the impact of the approximation. Our loss functions are tailored to the task anddraw inspiration from metrics developed for ex tractive compression but also for summarisation and machine translation. They are based on the Hamming distance over unordered bags of items. This measures the number of predicted items that did not appear in the reference, along with a penalty for short output: ? hamming (y ? ,y) = f+max (l ? (t+ f), 0) (6) where t and f are the number of true and falsepositives, respectively, when comparing the pre dicted target, y, with the reference, y ? , and l isthe length of the reference. The second term pe nalises short output, as predicting very little or nothing would otherwise be unpenalised. We have three Hamming loss functions over: 1) tokens, 2) ngrams (n ? 3), or 3) CFG productions. Theselosses all operate on unordered bags and therefore might reward erroneous predictions. For ex ample, a permutation of the reference tokens has zero token-loss. The CFG and ngram losses have overlapping items which encode a partial order, and therefore are less affected.In addition, we developed a fourth loss func tion to measure the edit distance between themodel?s prediction and the reference, both as bags of-tokens. This measures the number of insertionsand deletions. In contrast to the previous loss func tions, this requires the true positive counts to be clipped to the number of occurrences of each type in the reference. The edit distance is given by: ? edit (y ? ,y) = p+ q ? 2 ? i min(p i , q i ) (7) where p and q denote the number of target tokensin the predicted and reference derivation, respec tively, and p i and q i are the counts for type i. 141 ?ADJP,NP? ?subject [PP to NP 1 ], part [PP of NP 1 ]? (T) ?ADVP,RB? ?as well, also? (T) ?ADJP,JJ? ?too little, insufficient? (P) ?S,S? ? ?S 1 and S 2 , S 2 and S 1 ? (P) ?NP,NP? ?DT 1 NN 2 , DT 1 NN 2 ? (S) ?NP,NP? ?DT 1 NN 2 , NN 2 ? (S) Table 2: Sample grammar rules extracted from the training set (T), pivoted set (P) or generated from the source (S).
Experimental Design. . In this section we present our experimental set up for assessing the performance of our model. We give details on the corpora and grammars we used, model parameters and features, 6 the baselineused for comparison with our approach, and ex plain how our system output was evaluated. Grammar Extraction Our grammar usedrules extracted directly from our compression cor pus (the training partition, 480 sentences) and a bilingual corpus (see Table 2 for examples). Theformer corpus was word-aligned using the Berke ley aligner (Liang et al, 2006) initialised with a lexicon of word identity mappings, and parsed with Bikel?s (2002) parser. From this we extracted grammar rules following the technique described in Cohn and Lapata (2007). For the pivot grammarwe use the French-English Europarl v2 which con tains approximately 688K sentences. Again, the corpus was aligned using the Berkeley aligner and the English side was parsed with Bikel?s parser. Weextracted tree-to-string rules using our implementation of the GHKM method. To ameliorate the effects of poor alignments on the grammar, we re moved singleton rules before pivoting. In addition to the two grammars described, wescanned the source trees in the compression cor pus and included STSG rules to copy each CFG production or delete up to two of its children. This is illustrated in Table 2 where the last two rules are derived from the CFG production NP?DT NN inthe source tree. All trees are rooted with a distinguished TOP non-terminal which allows the ex plicit modelling of sentence spanning sub-trees. These grammars each had 44,199 (pivot), 7,813 (train) and 22,555 (copy) rules. We took their union, resulting in 58,281 unique rules and 13,619 unique source elementary trees. Model Parameters Our model was trainedon 480 sentences, 36 sentences were used for de velopment and 59 for testing. We used a variety of syntax-based, lexical and compression-specific 6 The software and corpus can be downloaded from http://homepages.inf.ed.ac.uk/tcohn/paraphrase. For every rule: origin of rule for each origin, o: log p o (s, t), log p o (s|t), log p o (t|s) s R , t R , s R ? t R s, t, s ? t, s = t both s and t are pre-terminals and s = t or s 6= t number of terminals/variables/dropped variables ordering of variables as numbers/non-terminals non-terminal sequence of vars identical after reordering pre-terminal or terminal sequences are identical number/identity of common/inserted/dropped terminals source is shorter/longer than target target is a compression of the source using deletes For every ngram : log p(w i |w i?1 i?(n?1) ) Table 3: The feature set. Rules were drawn from the training set, bilingual pivoting and directly from the source trees. s andt are the source and target elementary trees in a rule, the sub script R references the root non-terminal, w are the terminals in the target tree. features (196,419 in total). These are summarised in Table 3. We also use a trigram language model trained on the BNC (100 million words) using the SRI Language Modeling toolkit (Stolcke, 2002), with modified Kneser-Ney smoothing.An important parameter in our modeling frame work is the choice of loss function. We evaluatedthe loss functions presented in Section 4 on the de velopment set. We ran our system for each of the four loss functions and asked two human judgesto rate the output on a scale of 1 to 5. The Ham ming loss over tokens performed best with a meanrating of 3.18, closely followed by the edit dis tance (3.17). We chose the former over the latter as it is less coarsely approximated during search. Baseline There are no existing models thatcan be readily trained on our abstractive com pression data. Instead, we use Cohn and Lapata?s (2007) extractive model as a baseline. The latter was trained on an extractive compression corpus drawn from the BNC (Clarke, 2008) and tunedto provide a similar compression rate to our sys tem. Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora.Evaluation Methodology Sentence compres sion output is commonly evaluated by eliciting human judgments. Following Knight and Marcu(2002), we asked participants to rate the grammati cality of the target compressions and howwell they preserved the most important information from the source. In both cases they used a five pointrating scale where a high number indicates better performance. We randomly selected 30 sen tences from the test portion of our corpus. These 142 Models Grammaticality Importance CompR Extract 3.10 ? 2.43 ? 82.5 Abstract 3.38 ? 2.85 ? ? 79.2 Gold 4.51 4.02 58.4Table 4: Mean ratings on compression output elicited by hu mans; ? : significantly different from the gold standard; ?: sig nificantly different from the baseline. sentences were compressed automatically by our model and the baseline. We also included goldstandard compressions. Our materials thus con sisted of 90 (30 ? 3) source-target sentences. We collected ratings from 22 unpaid volunteers, all self reported native English speakers. Both studies were conducted over the Internet using a custom built web interface.
Results. . Our results are summarised in Table 4, where we show the mean ratings for our system (Abstract), the baseline (Extract), and the gold standard. We first performed an Analysis of Variance (ANOVA)to examine the effect of different system compres sions. The ANOVA revealed a reliable effect on both grammaticality and importance (significant over both subjects and items (p < 0.01)).We next examined in more detail between system differences. Post-hoc Tukey tests revealed that our abstractive model received significantlyhigher ratings than the baseline in terms of impor tance (? < 0.01). We conjecture that this is due to the synchronous grammar we employ which is larger and more expressive than the baseline. In the extractive case, a word sequence is eitherdeleted or retained. We may, however, want to re tain the meaning of the sequence while rendering the sentence shorter, and this is precisely what our model can achieve, e.g., by allowing substitutions.As far as grammaticality is concerned, our abstractive model is numerically better than the extrac tive baseline but the difference is not statistically significant. Note that our model has to work a lotharder than the baseline to preserve grammatical ity since we allow arbitrary rewrites which maylead to agreement or tense mismatches, and selec tional preference violations. The scope for errors is greatly reduced when performing solely deletions.Finally, both the abstractive and extractive out puts are perceived as significantly worse than the gold standard both in terms of grammaticalityand importance (? < 0.01). This is not surpris ing: human-authored compressions are more fluentand tend to omit genuinely superfluous informa tion. This is also mirrored in the compression ratesshown in Table 4. When compressing, humans emO: Kurtz came from Missouri, and at the age of 14, hitch hiked to Los Angeles seeking top diving coaches. E: Kurtz came from Missouri, and at 14, hitch-hiked to Los Angeles seeking top diving coaches. A: Kurtz hitch-hiked to Los Angeles seeking top diving coaches. G: Kurtz came from Missouri, and at 14, hitch-hiked to Los Angeles seeking diving coaches. O: The scheme was intended for people of poor or moderate means. E: The scheme was intended for people of poor means. A: The scheme was planned for poor people. G: The scheme was intended for the poor. O: He died last Thursday at his home from complications following a fall, said his wife author Margo Kurtz. E: He died last at his home from complications following a fall, said wife, author Margo Kurtz. A: His wife author Margo Kurtz died from complications after a decline. G: He died from complications following a fall.O: But a month ago, she returned to Britain, taking the chil dren with her. E: She returned to Britain, taking the children. A: But she took the children with him. G: But she returned to Britain with the children. Table 5: Compression examples including human and systemoutput (O: original sentence, E: Extractive model, A: Abstrac tive model, G: gold standard) ploy not only linguistic but also world knowledge which is not accessible to our model. Although thesystem can be forced to match the human compression rate, the grammaticality and information con tent both suffer. More sophisticated features could allow the system to narrow this gap. We next examined the output of our system inmore detail by recording the number of substitu tions, deletions and insertions it performed on the test data. Deletions accounted for 67% of rewrite operations, substitutions for 27%, and insertions for 6%. Interestingly, we observe a similar ratio in the human compressions. Here, deletions arealso the most common rewrite operation (69%) fol lowed by substitutions (24%), and insertions (7%). The ability to perform substitutions and insertions increases the compression potential of our system, but can also result in drastic meaning changes. In most cases (63%) the compressions produced byour system did not distort the meaning of the orig inal. Humans are clearly better at this, 96.5% of their compressions were meaning preserving. We illustrate example output of our system in Table 5. For comparison we also present the gold standard compressions and baseline output. In thefirst sentence the system rendered Kurtz the sub ject of hitch-hiked. At the same time it deleted the verb and its adjunct from the first conjunct (camefrom Missouri ) as well as the temporal modi fier at the age of 14 from the second conjunct. The second sentence shows some paraphrasing: the verb intended is substituted with planned and 143 poor is now modifying people rather than means.In the third example, our system applies multi ple rewrites. It deletes last Thursday at his home,moves wife author Margo Kurtz to the subject position, and substitutes fall with decline. Unfortu nately, the compressed sentence expresses a rather different meaning from the original. It is not Margo Kurtz who died but her husband. Finally, our last sentence illustrates a counter-intuitive substitution, the pronoun her is rewritten as him. This is becausethey share the French translation lui and thus piv oting learns to replace the less common word (in legal corpora) her with him. This problem could be addressed by pivoting over multiple bitexts with different foreign languages. Possible extensions and improvements to the current model are many and varied. Firstly, ashinted at above, the model would benefit from extensive feature engineering, including source con ditioned features and ngram features besides theLM. A richer grammar would also boost perfor mance. This could be found by pivoting over more bitexts in many foreign languages or making use of existing or paraphrase corpora. Finally, we planto apply the model to other paraphrasing tasks in cluding fully abstractive document summarisation (Daum?e III and Marcu, 2002). Acknowledgements The authors acknowledge the support of EPSRC (grants GR/T04540/01 and GR/T04557/01). Special thanks to Phil Blunsom, James Clarke and Miles Osborne for their insightful suggestions.