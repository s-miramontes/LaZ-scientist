1 Introduction. This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human evaluation. There were a two additions to this year’s workshop that were not part of previous workshops: • Haitian Creole featured task – In addition to translation between European language pairs, we featured a new translation task: translating Haitian Creole SMS messages that were sent to an emergency response hotline in the immediate aftermath of the 2010 Haitian earthquake. The goal of this task is to encourage researchers to focus on challenges that may arise in future humanitarian crises. We invited Will Lewis, Rob Munro and Stephan Vogel to publish a paper about their experience developing translation technology in response to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. • Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system. Although previous workshops have shown evaluation metrics other than BLEU are more strongly correlated with human judgments when ranking outputs from multiple systems, BLEU remains widely used by system developers to optimize their system parameters. We challenged metric developers to tune the parameters of a fixed system, to see if their metrics would lead to perceptibly better translation quality for the system’s resulting output. The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation. As with previous workshops, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality.
2 Overview of the Shared Translation and System Combination Tasks. The recurring task of the workshop examines translation between English and four other languages: German, Spanish, French, and Czech. We created a test set for each language pair by translating newspaper articles. We additionally provided training data and two baseline systems. The test data for this year’s task was created by hiring people to translate news articles that were drawn from a variety of sources from early December 2010. A total of 110 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German, and Spanish news sites:2 Czech: aktualne.cz (4), Novinky.cz (7), iHNed.cz (4), iDNES.cz (4) French: Canoe (5), Le Devoir (5), Le Monde (5), Les Echos (5), Liberation (5) Spanish: ABC.es (6), Cinco Dias (6), El Periodico (6), Milenio (6), Noroeste (7) English: Economist (4), Los Angeles Times (6), New York Times (4), Washington Post (4) German: FAZ (3), Frankfurter Rundschau (2), Financial Times Deutschland (3), Der Spiegel (5), S¨uddeutsche Zeitung (3) The translations were created by the professional translation agency CEET.3 All of the translations were done directly, and not via an intermediate language. Although the translations were done professionally, in some cases errors still cropped up. For instance, in parts of the English-French translations, some of the English source remains in the French reference as if the translator forgot to delete it. As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some statistics about the training materials are given in Figure 1. To lower the barrier of entry for newcomers to the field, we provided two open source toolkits for phrase-based and parsing-based statistical machine translation (Koehn et al., 2007; Li et al., 2010). We received submissions from 56 groups across 37 institutions, as listed in Tables 1, 2 and 3. We also included two commercial off-the-shelf MT systems, two online statistical MT systems, and five online rule-based MT systems. (Not all systems supported all language pairs.) We note that these nine companies did not submit entries themselves, and are therefore anonymized in this paper. Rather, their entries were created by translating the test data via their web interfaces.4 The data used to construct these systems is not subject to the same constraints as the shared task participants. It is possible that part of the reference translations that were taken from online news sites could have been included in the online systems’ models, for instance. We therefore categorize all commercial systems as unconstrained when evaluating the results. In total, we had 148 primary system entries (including the 46 entries crawled from online sources), and 60 contrastive entries. These were made available to distinct words (case-insensitive) is based on the provided tokenizer.
ID Participant. participants in the system combination shared task. Continuing our practice from last year’s workshop, we separated the test set into a tuning set and a final held-out test set for system combinations. The tuning portion was distributed to system combination participants along with reference translations, to aid them set any system parameters. In the European language pairs, the tuning set consisted of 1,003 segments taken from 37 documents, whereas the test set consisted of 2,000 segments taken from 73 documents. In the Haitian Creole task, the split was 674 segments for tuning and 600 for testing. Table 2 lists the 10 participants in the system combination task.
3 Featured Translation Task. The featured translation task of WMT11 was to translate Haitian Creole SMS messages into English. These text messages were sent by people in Haiti in the aftermath of the January 2010 earthquake. In the wake of the earthquake, much of the country’s conventional emergency response services failed. Since cell phone towers remained standing after the earthquake, text messages were a viable mode of communication. Munro (2010) describes how a text-message-based emergency reporting system was set up by a consortium of volunteer organizations named “Mission 4636” after a free SMS short code telephone number that they established. The SMS messages were routed to a system for reporting trapped people and other emergencies. Search and rescue teams within Haiti, including the US Military, recognized the quantity and reliability of actionable information in these messages and used them to provide aid. The majority of the SMS messages were written in Haitian Creole, which was not spoken by most of first responders deployed from overseas. A distributed, online translation effort was established, drawing volunteers from Haitian Creole- and French-speaking communities around the world. The volunteers not only translated messages, but also categorized them and pinpointed them on a map.5 Collaborating online, they employed their local knowledge of locations, regional slang, abbreviations and spelling variants to process more than 40,000 messages in the first six weeks alone. First responders indicated that this volunteer effort helped to save hundreds of lives and helped direct the first food and aid to tens of thousands. Secretary of State Clinton described one success of the Mission 4636 program:“The technology community has set up interactive maps to help us identify needs and target resources. And on Monday, a seven-year-old girl and two women were pulled from the rubble of a collapsed supermarket by an American search-andrescue team after they sent a text message calling for help.” Ushahidi@Tufts described another:“The World Food Program delivered food to an informal camp of 2500 people, having yet to receive food or water, in Diquini to a location that 4636 had identified for them.” In parallel with Rob Munro’s crowdsourcing translation efforts, the Microsoft Translator team developed a Haitian Creole statistical machine translation engine from scratch in a compressed timeframe (Lewis, 2010). Despite the impressive number of translations completed by volunteers, machine translation was viewed as a potentially useful tool for higher volume applications or to provide translations of English medical documents into Haitian Creole. The Microsoft Translator team quickly assembled parallel data from a number of sources, including Mission 4636 and from the archives of Carnegie Mellon’s DIPLOMAT project (Frederking et al., 1997). Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand. They deployed a functional translation system to relief workers in the field in less than 5 days – impressive even when measured against previous rapid MT development efforts like DARPA’s surprise language exercise (Oard, 2003; Oard and Och, 2003). We were inspired by the efforts of Rob Munro and Will Lewis on translating Haitian Creole in the aftermath of the disaster, so we worked with them to create a featured task at WMT11. We thank them for generously sharing the data they assembled in their own efforts. We invited Rob Munro, Will Lewis, and Stephan Vogel to speak at the workshop on the topic of developing translation technology for future crises, and they recorded their thoughts in an invited publication (Lewis et al., 2011). For the WMT11 featured translation task, we anonymized the SMS Haitian Creole messages along with the translations that the Mission 4636 volunteers created. Examples of these messages are given in Table 4. The goal of anonymizing the SMS data was so that it may be shared with researchers who are developing translation and mapping technologies to support future emergency relief efforts and social development. We ask that any researcher working with these messages to be aware that they are actual communications sent by people in need in a time of crisis. Researchers who use this data are asked to be cognizant of the following: should be to understand how we can better respond to future crises. Participants who received the Haitian Creole data for WMT11 were given anonymization guidelines mwen se [FIRSTNAME] mwen gen twaset ki mouri mwen mande nou ed pou nou edem map tan repons I am [FIRSTNAME], I have three sisters who have died. I ask help for us, I await your response. Ki kote yap bay manje Where are they giving out food? Eske lekol kolej marie anne kraze?mesi Was the College Marie Anne school destroyed? Thank you. Nou pa ka anpeche moustik yo m`ode nou paske yo anpil. We can’t prevent the mosquitoes from biting because there are so many. tanpri k`em ap kase mwen pa ka pran nouvel manmanm. Please heart is breaking because I have no news of my mother. 4636:Opital Medesen san Fwonti`e delmas 19 la f`emen. Opital sen lwi gonzag nan delma 33 pran an chaj gratwitman tout moun ki malad ou blese Mwen r´es´evoua mesaj nou yo 5 sou 5 men mwen ta vle di yon bagay kil`e e koman nap kapab f`em jwin `ed sa yo pou moune b la kay mwen ki sinistw´e adr`es la s´e 4636: The Doctors without Borders Hospital in Delmas 19 is closed. The Saint Louis Gonzaga hospital in Delmas 33 is taking in sick and wounded people for free I received your message 5/5 but I would like to ask one thing when and how will you be able to get the aid to me for the people around my house who are victims of the earthquake? The address is Sil vous plait map chehe [LASTNAME][FIRSTNAME].di yo relem nan [PHONENUMBER].mwen se [LASTNAME] [FIRSTNAME] Bonswa mwen rele [FIRSTNAME] [LASTNAME] kay mwen krase mwen pagin anyin poum mange ak fanmi-m tampri di yon mo pou mwen fem jwen yon tante tou ak mange..mrete n I’m looking for [LASTNAME][FIRSTNAME]. Tell him to call me at [PHONENUMBER] I am [LASTNAME] [FIRSTNAME] Hello my name is [FIRSTNAME] [LASTNAME]my house fell down, I’ve had nothing to eat and I’m hungry. Please help me find food. I live Mw rele [FIRSTNAME], mw f`e mason epi mw abite lapl`en. Yo dim minustah ap bay djob mason ki kote pou mw ta pase si mw ta vle jwenn nan djob sa yo. Souple mande lapolis pou fe on ti pase nan magloire ambroise prolonge zone muler ak cadet jeremie ginyin jen gason ki ap pase nan zone sa yo e ki agresi
KIBO MOUN KA JWENN MANJE POU YO MANJE ANDEYO KAPITAL PASKE DEPI 12 JANVYE YO VOYE MANJE POU PEP LA MEN NOU PA JANM JWENN ANYEN.. I’m a young student in computer science, who has suffered a lot during and after the earthquake of January 12th. All my family has died and I feel alone. I wanted to go live. My name is [FIRSTNAME], I’m a construction worker and I live in La Plaine. I heard that the MINUSTAH was giving jobs to construction workers. What do I have to go to find one of these jobs? please ask the police to go to magloire ambroise going towards the ”muler” area and cadet jeremie because there are very aggressive young men in these areas Where can people get food to eat outside of the capital because since January 12th, they’ve sent food for the people but we never received anything. We are dying of hunger Mwen se [FIRSTNAME][LASTNAME] mwen nan aken mwen se yon j`en ki ansent mwen te genyen yon paran ki tap ede li mouri p`otoprens, mwen pral akouye nan k`omansman feviye alongside the SMS data. The WMT organizers requested that if they discovered messages with incorrect or incomplete anonymization, that they notify us and correct the anonymization using the version control repository. To define the shared translation task, we divided the SMS messages into an in-domain training set, along with designated dev, devtest, and test sets. We coordinated with Microsoft and CMU to make available additional out-of-domain parallel corpora. Details of the data are given in Table 5. In addition to this data, participants in the featured task were allowed to use any of the data provided in the standard translation task, as well as linguistic tools such as taggers, parsers, or morphological analyzers. We provided two sets of testing and development data. Participants used their systems to translate two test sets consisting of 1,274 unseen Haitian Creole SMS messages. One of the test sets contains the “raw” SMS messages as they were sent, and the other contains messages that were cleaned up by human post-editors. The English side is the same in both cases, and the only difference is the Haitian Creole input sentences. The post-editors were Haitian Creole language informants hired by Microsoft Research. They provided a number of corrections to the SMS messages, including expanding SMS shorthands, correcting spelling/grammar/capitalization, restoring diacritics that were left out of the original message, and cleaning up accented characters that were lost when the message was transmitted in the wrong encoding. Sil vou pl´e ´ede mwen avek moun ki viktim yo nan tranbleman de t´e a,ki kit´e potoprins ki vini nan provins- mwen ede ak ti kob mwen te ginyin kouni´e a 4636: Manje vin pi che nan PaP apre tranbleman te-a. mamit diri ap van’n 250gd kounye, sete 200gd avan. Mayi-a 125gd, avan sete 100gd Silvouple ede mwen av`ek moun ki viktim yo nan tranblemannt`e a, ki kite P`otoprens ki vini nan pwovens, mwen ede ak ti k`ob mwen te genyen kounye a 4636: Manje vin pi ch`e nan PaP apre tranblemannt`e a. Mamit diri ap vann 250gd kounye a, sete 200gd avan. Mayi-a 125gd, avan sete 100gd. For the test and development sets the informants also edited the English translations. For instance, there were cases where the original crowdsourced translation summarized the content of the message instead of translating it, instances where parts of the source were omitted, and where explanatory notes were added. The editors improved the translations so that they were more suitable for machine translation, making them more literal, correcting disfluencies on the English side, and retranslating them when they were summaries. We are in the area of Petit Goave, we would like .... we need tents and medication for flu/colds... We are in the area of Petit Goave, we would like to receive assistance, however, it should not be the way I see the Minustah guys are handling the people. We need lots of tents and medication for flu/colds, and fever The edited English is provided as the reference for both the “clean” and the “raw” sets, since we intend that distinction to refer to the form that the source language comes in, rather than the target language. Tables 47 and 48 in the Appendix show a significant difference in the translation quality between the clean and the raw test sets. In most cases, systems’ output for the raw condition was 4 BLEU points lower than for the clean condition. We believe that the difference in performance on the raw vs. cleaned test sets highlight the importance of handling noisy input data. All of the in-domain training data is in the raw format. The original SMS messages are unaltered, and the translations are just as the volunteered provided them. In some cases, the original SMS messages are written in French or English instead of Haitian Creole, or contain a mixture of languages. It may be possible to further improve the quality of machine translation systems trained from this data by improving the quality of the data itself. The goals of the Haitian Creole to English translation task were: There are many challenges in translating noisy data in a low resource language, and there are a variety of strategies that might be considered to attempt to tackle them. For instance: It is our hope that by introducing this data as a shared challenge at WMT11 that we will establish a useful community resource so that researchers may explore these challenges and publish about them in the future.
4 Human Evaluation. As with past workshops, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores. It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality. Therefore, we define the manual evaluation to be primary, and use the human judgments to validate automatic metrics. Manual evaluation is time consuming, and it requires a large effort to conduct on the scale of our workshop. We distributed the workload across a number of people, including shared-task participants, interested volunteers, and a small number of paid annotators (recruited by the participating sites). More than 130 people participated in the manual evaluation, with 91 people putting in more than an hour’s worth of effort, and 29 putting in more than four hours. There was a collective total of 361 hours of labor. We asked annotators to evaluate system outputs by ranking translated sentences relative to each other. This was our official determinant of translation quality. The total number of judgments collected for the different ranking tasks is given in Table 6. We performed the manual evaluation of the individual systems separately from the manual evaluation of the system combination entries, rather than comparing them directly against each other. Last year’s results made it clear that there is a large (expected) gap in performance between the two groups. This year, we opted to reduce the number of pairwise comparisons with the hope that we would be more likely to find statistically significant differences between the systems in the same groups. To that same end, we also eliminated the editing/acceptability task that was featured in last year’s evaluation, instead we had annotators focus solely on the system ranking task. Ranking translations relative to each other is a reasonably intuitive task. We therefore kept the instructions simple: You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). Each screen for this task involved judging translations of three consecutive source segments. For each source segment, the annotator was shown the outputs of five submissions, and asked to rank them. With the exception of a few tasks in the system combination track, there were many more than 5 systems participating in any given task—up to 23 for the English-German individual systems track. Rather than attempting to get a complete ordering over the systems, we instead relied on random selection and a reasonably large sample size to make the comparisons fair. We use the collected rank labels to assign each system a score that reflects how highly that system was usually ranked by the annotators. The score for some system A reflects how frequently it was judged to be better than or equal to other systems. Specifically, each block in which A appears includes four implicit pairwise comparisons (against the other presented systems). A is rewarded once for each of the four comparisons in which A wins or ties. A’s score is the number of such winning (or tying) pairwise comparisons, divided by the total number of pairwise comparisons involving A. The system scores are reported in Section 5. Appendix A provides detailed tables that contain pairwise head-to-head comparisons between pairs of systems. We were interested in determining the inter- and intra-annotator agreement for the ranking task, since a reasonable degree of agreement must exist to support our process as a valid evaluation setup. To ensure we had enough data to measure agreement, we purposely designed the sampling of source segments and translations shown to annotators in a way that ensured some items would be repeated, both within the screens completed by an individual annotator, and across screens completed by different annotators. We did so by ensuring that 10% of the generated screens are exact repetitions of previously generated screen within the same batch of screens. Furthermore, even within the other 90%, we ensured that a source segment appearing in one screen appears again in two more screens (though with different system outputs). Those two details, intentional repetition of source sentences and intentional repetition of system outputs, ensured we had enough data to compute meaningful inter- and intra-annotator agreement rates. We measured pairwise agreement among annotators using Cohen’s kappa coefficient (n) (Cohen, 1960), which is defined as where P(A) is the proportion of times that the annotators agree, and P(E) is the proportion of time that they would agree by chance. Note that n is basically a normalized version of P(A), one which takes into account how meaningful it is for annotators to agree with each other, by incorporating P(E). Note also that n has a value of at most 1 (and could possibly be negative), with higher rates of agreement resulting in higher n. The above definition of n is actually used by several definitions of agreement measures, which differ in how P(A) and P(E) are computed. We calculate P(A) by examining all pairs of systems which had been judged by two or more judges, and calculating the proportion of time that they agreed that A > B, A = B, or A < B. In other words, P(A) is the empirical, observed rate at which annotators agree, in the context of pairwise comparisons. P(A) is computed similarly for intraannotator agreement (i.e. self-consistency), but over pairwise comparisons that were annotated more than once by a single annotator. As for P(E), it should capture the probability that two annotators would agree randomly. Therefore: Note that each of the three probabilities in P(E)’s definition are squared to reflect the fact that we are considering the chance that two annotators would agree by chance. Each of these probabilities is computed empirically, by observing how often annotators actually rank two systems as being tied. We note here that this empirical computation is a departure from previous years’ analyses, where we had assumed that the three categories are equally likely (yielding P(E) = 19 + 19 + 19 = 1�). We believe that this is a more principled approach, which faithfully reflects the motivation of accounting for P(E) in the first place.6 6Even if we wanted to assume a “random clicker” model, setting P(E) = 3 is still not entirely correct. Given that Table 7 gives n values for inter-annotator and intra-annotator agreement across the various evaluation tasks. These give an indication of how often different judges agree, and how often single judges are consistent for repeated judgments, respectively. There are some general and expected trends that can be seen in this table. First of all, intra-annotator agreement is higher than inter-annotator agreement. Second, reference translations are noticeably better than other system outputs, which means that annotators have an artificially high level of agreement on pairwise comparisons that include a reference translation. For this reason, we also report the agreement levels when such comparisons are excluded. The exact interpretation of the kappa coefficient is difficult, but according to Landis and Koch (1977), 0 − 0.2 is slight, 0.2 − 0.4 is fair, 0.4 − 0.6 is moderate, 0.6 − 0.8 is substantial, and 0.8 − 1.0 is almost perfect. Based on these interpretations, the agreement for sentence-level ranking is moderate to substantial for most tasks. annotators rank five outputs at once, P(A = B) = 3, not �, since there are only five (out of 25) label pairs that satisfy 1 A = B. Working this back into P(E)’s definition, we have P(A > B) = P(A < B) = 5, and therefore P(E) = 0.36 rather than 0.333. and 50 below for a detailed breakdown by language pair. However, one result that is of concern is that agreement rates are noticeably lower for European language pairs, in particular for the individual systems track. When excluding reference comparisons, the inter- and intra-annotator agreement levels are 0.320 and 0.512, respectively. Not only are those numbers lower than for the other tasks, but they are also lower than last year’s numbers, which were 0.409 and 0.580. We investigated this result a bit deeper. Tables 49 and 50 in the Appendix break down the results further, by reporting agreement levels for each language pair. One observation is that the agreement level for some language pairs deviates in a nontrivial amount from the overall agreement rate. Let us focus on inter-annotator agreement rates in the individual track (excluding reference comparisons), in the top right portion of Table 49. The overall K is 0.320, but it ranges from 0.264 for GermanEnglish, to 0.477 for Spanish-English. What distinguishes those two language pairs from each other? If we examine the results in Table 8, we see that Spanish-English had two very weak systems, which were likely easy for annotators to agree on comparisons involving them. (This is the converse of annotators agreeing more often on comparisons involving the reference.) English-French is similar in that regard, and it too has a relatively high agreement rate. On the other hand, the participants in GermanEnglish formed a large pool of more closelymatched systems, where the gap separating the bottom system is not as pronounced. So it seems that the low agreement rates are indicative of a more competitive evaluation and more closely-matched systems.
5 Results of the Translation Tasks. We used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the workshop. In our analysis, we aimed to address the following questions: Tables 8–10 show the system ranking for each of the translation tasks. For each language pair, we define a system as ‘winning’ if no other system was found statistically significantly better (using the Sign Test, at p G 0.10). In some cases, multiple systems are listed as winners, either due to a large number of participants or a low number of judgments per system pair, both of which are factors that make it difficult to achieve statistical significance. We start by examining the results for the individual system track for the European languages (Table 8). In SpanishHEnglish and GermanHEnglish, unconstrained systems are observed to perform better than constrained systems. In other language pairs, particularly FrenchHEnglish, constrained systems are found to be able to be on the same level or outperform unconstrained systems. It also seems that making use of the Gigaword corpora is likely to yield better systems, even when translating out of English, as in English-French and English-German. For English-German the rule-based MT systems performed well. Of the participating teams, there is no individual system clearly outperforming all other systems across the different language pairs. However, one of the crawled systems, ONLINE-B, performs consistently well, being one of the winners in all eight language pairs. As for the system combination track (Table 9), the CMU-HEAFIELD-COMBO entry performed quite well, being a winner in seven out of eight language pairs. This performance is carried over to the Haitian Creole task, where it again comes out on top (Table 10). In the individual track of the Haitian Creole task, BM-I2R is the sole winner in both the ‘clean’ and ‘raw’ tracks.
6 Evaluation Task. In addition to allowing us to analyze the translation quality of different systems, the data gathered during the manual evaluation is useful for validating automatic evaluation metrics. Our evaluation shared task is similar to the MetricsMATR workshop (Metrics for MAchine TRanslation) that NIST runs (Przybocki et al., 2008; Callison-Burch et al., 2010). Table 11 lists the participants in this task, along with their metrics. A total of 21 metrics and their variants were submitted to the evaluation task by 9 research groups. We asked metrics developers to score the outputs of the machine translation systems and system combinations at the system-level and at the segmentlevel. The system-level metrics scores are given in the Appendix in Tables 39–48. The main goal of the evaluation shared task is not to score the systems, but instead to validate the use of automatic metrics by measuring how strongly they correlate with human judgments. We used the human judgments collected during the manual evaluation for the translation task and the system combination task to calculate how well metrics correlate at system-level and at the segment-level. This year the strongest metric was a new metric developed by Columbia and ETS called MTeRaterPlus. MTeRater-Plus is a machine-learning-based metric that use features from ETS’s e-rater, an automated essay scoring engine designed to assess writing proficiency (Attali and Burstein, 2006). The features include sentence-level and document-level information. Some examples of the e-rater features include: MTeRater uses only the e-rater features, and measures fluency without any need for reference translations. MTeRater-Plus is a meta-metric that incorporates adequacy by combining MTeRater with other MT evaluation metrics and heuristics that take the reference translations into account. Please refer to the proceedings for papers providing detailed descriptions of all of the metrics. for translation out of English, ordered by average absolute value. We did not calculate correlations with the human judgments for the system combinations for the out of English direction, because none of them had more than 4 items. We measured the correlation of the automatic metrics with the human judgments of translation quality at the system-level using Spearman’s rank correlation coefficient ρ. We converted the raw scores assigned to each system into ranks. We assigned a human ranking to the systems based on the percent of time that their translations were judged to be better than or equal to the translations of any other system in the manual evaluation. The reference was not included as an extra translation. When there are no ties, ρ can be calculated using the simplified equation: where di is the difference between the rank for systemi and n is the number of systems. The possible values of ρ range between 1(where all systems are ranked in the same order) and −1 (where the systems are ranked in the reverse order). Thus an automatic evaluation metric with a higher absolute value for ρ is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower absolute ρ. The system-level correlations are shown in Table 13 for translations into English, and Table 12 out of English, sorted by average correlation across the language pairs. The highest correlation for each language pair and the highest overall average are bolded. This year, nearly all of the metrics had stronger correlation with human judgments than BLEU. The metrics that had the strongest correlation this year included two metrics, MTeRater and TINE, as well as metrics that have demonstrated strong correlation in previous years like TESLA and Meteor. We measured the metrics’ segment-level scores with the human rankings using Kendall’s tau rank correlation coefficient. The reference was not included as an extra translation. We calculated Kendall’s tau as: num concordant pairs - num discordant pairs total pairs where a concordant pair is a pair of two translations of the same segment in which the ranks calculated from the same human ranking task and from the corresponding metric scores agree; in a discordant pair, they disagree. In order to account for accuracy- vs. Segment-level correlation for translations out of English error-based metrics correctly, counts of concordant vs. discordant pairs were calculated specific to these two metric types. The possible values of T range between 1 (where all pairs are concordant) and −1 (where all pairs are discordant). Thus an automatic evaluation metric with a higher value for T is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower T. We did not include cases where the human ranking was tied for two systems. As the metrics produce absolute scores, compared to five relative ranks in the human assessment, it would be potentially unfair to the metric to count a slightly different metric score as discordant with a tie in the relative human rankings. A tie in automatic metric rank for two translations was counted as discordant with two corresponding non-tied human judgments. The correlations are shown in Table 14 for translations into English, and Table 15 out of English, sorted by average correlation across the four language pairs. The highest correlation for each language pair and the highest overall average are bolded. There is a clear winner for the metrics that score translations into English: the MTeRater-Plus metric (Parton et al., 2011) has the highest segment level correlation across the board. For metrics that score translation into other languages, there is not such a clear-cut winner. The AMBER metric variants do well, as do MPF and WMPF.
7 Tunable Metrics Task. This year we introduced a new shared task that focuses on using evaluation metrics to tune the parameters of a statistical machine translation system. The intent of this task was to get researchers who develop automatic evaluation metrics for MT to work on the problem of using their metric to optimize the parameters of MT systems. Previous workshops have demonstrated that a number of metrics perform better than BLEU in terms of having stronger correlation with human judgments about the rankings of multiple machine translation systems. However, most MT system developers still optimize the parameters of their systems to BLEU. Here we aim to investigate the question of whether better metrics will result in better quality output when a system is optimized to them. Because this was the first year that we ran the tunable metrics task, participation was limited to a few groups on an invitation-only basis. Table 16 lists the participants in this task. Metrics developers were invited to integrate their evaluation metric into a MERT optimization routine, which was then used to tune the parameters of a fixed statistical machine translation system. We evaluated whether the system tuned on their metrics produced higher-quality output than the baseline system that was tuned to BLEU, as is typically done. In order to evaluate whether the quality was better, we conducted a manual evaluation, in the same fashion that we evaluate the different MT systems submitted to the shared translation task. We provide the participants with a fixed MT system for Urdu-English, along with a small parallel set to be used for tuning. Specifically, we provide developers with the following components: We provided the metrics developers with Omar Zaidan’s Z-MERT software (Zaidan, 2009), which implements Och (2003)’s minimum error rate training procedure. Z-MERT is designed to be modular with respect to the objective function, and allows BLEU to be easily replaced with other automatic evaluation metrics. Metric developers incorporated their metrics into Z-MERT by subclassing the EvaluationMetric.java abstract class. They ran Z-MERT on the dev set with the provided decoder/models, and created a weight vector for the system parameters. Each team produced a distinct final weight vector, which was used to produce English translations of sentences in the test set. The different translations produced by tuning the system to different metrics were then evaluated using the manual evaluation pipeline.7 The results of the evaluation are in Table 18. The scores show that the entries were quite close to each other, with the notable exception of the SHEFFIELDROSE-tuned system, which produced overly-long and erroneous output (possibly due to an implementation issue). This is also evident from the fact that 38% of pairwise comparisons indicated a tie between the two systems, with the tie rate increasing to a full 47% when excluding comparisons involving the reference. This is a very high tie rate – the corresponding figure in, say, European language pairs (individual systems) is only 21%. What makes the different entries appear even more closely-matched is that the ranking changes significantly when ordering systems by their >others score rather than the >others score (i.e. when rewarding only wins, and not rewarding ties). NUS-TESLA-F goes from being a bottom entry to being a top entry, with CU-SEMPOS-BLEU also benefiting, changing from the middle to the top rank. Either way, we see that a BLEU -tuned system is performing just as well as systems tuned to the other metrics. This might be an indication that some work remains to be done before a move away from BLEU-tuning is fully justified. On the other hand, the close results might be an artifact of the language pair choice. Urdu-English translation is still a relatively difficult problem, and MT outputs are still of a relatively low quality. It might be the case that human annotators are simply not very good at distinguishing one bad translation from another bad translation, especially at such a fine-grained level. It is worth noting that the designers of the TESLA family replicated the setup of this tunable metric task for three European language pairs, and found that human judges did perceive a difference in quality between a TESLA-tuned system and a BLEU -tuned system (Liu et al., 2011). This year’s effort was a pilot of the task, so we intentionally limited the task to some degree, to make it easier to iron out the details. Possible changes for next year include:
8 Summary. As in previous editions of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from European languages into English, and vice versa. The number of participants grew slightly compared to previous editions of the WMT workshop, with 36 groups from 27 institutions participating in the translation task of WMT11, 10 groups from 10 institutions participating in the system combination task, and 10 groups from 8 institutions participating in the featured translation task (Haitian Creole SMS into English). This year was also the first time that we included a language pair (Haitian-English) with non-European source language and with very limited resources for the source language side. Also the genre of the Haitian-English task differed from previous WMT tasks as the Haitian-English translations are SMS messages. WMT11 also introduced a new shared task focusing on evaluation metrics to tune the parameters of a statistical machine translation system in which 6 groups have participated. As in previous years, all data sets generated by this workshop, including the human judgments, system translations and automatic scores, are publicly available for other researchers to analyze.8
Acknowledgments. This work was supported in parts by the EuroMatrixPlus project funded by the European Commission (7th Framework Programme), the GALE program of the US Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022, the US National Science Foundation under grant IIS-0713448, and the CoSyne project FP7-ICT-4248531 funded by the European Commission. The views and findings are the authors’ alone. A big thank you to Ondˇrej Bojar, Simon Carter, Christian Federmann, Will Lewis, Rob Munro and Herv´e Saint-Amand, and to the shared task participants.
References. Tables 19–38 show pairwise comparisons between systems for each language pair. The numbers in each of the tables’ cells indicate the percentage of times that the system in that column was judged to be better than the system in that row. Bolding indicates the winner of the two systems. The difference between 100 and the sum of the complementary cells is the percent of time that the two systems were judged to be equal. Because there were so many systems and data conditions the significance of each pairwise comparison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differences (rather than differences that are attributable to chance). In the following tables * indicates statistical significance at p < 0.10, t indicates statistical significance at p < 0.05, and t indicates statistical significance at p < 0.01, according to the Sign Test. Tables 39–48 give the automatic scores for each of the systems.