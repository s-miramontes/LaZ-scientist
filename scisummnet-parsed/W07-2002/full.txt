Introduction. Word Sense Disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, butneed large amounts of hand-tagging data. Exist ing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with WordNetsenses (Fellbaum, 1998) allow for a small improve ment over the simple most frequent sense heuristic,as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In the ory, larger amounts of training data (SemCor hasapprox. 500M words) would improve the perfor mance of supervised WSD, but no current projectexists to provide such an expensive resource. An other problem of the supervised approach is that theinventory and distribution of senses changes dra matically from one domain to the other, requiring additional hand-tagging of corpora (Mart??nez and Agirre, 2000; Koeling et al, 2005). Supervised WSD is based on the ?fixed-list of senses? paradigm, where the senses for a target wordare a closed list coming from a dictionary or lex icon. Lexicographers and semanticists have long warned about the problems of such an approach,where senses are listed separately as discrete entities, and have argued in favor of more complex rep resentations, where, for instance, senses are dense regions in a continuum (Cruse, 2000).Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of think ing, and tries to induce word senses directly fromthe corpus. Typical WSID systems involve clustering techniques, which group together similar examples. Given a set of induced clusters (which repre sent word uses or senses1), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense. One of the problems of unsupervised systems isthat of managing to do a fair evaluation. Most of cur rent unsupervised systems are evaluated in-house, with a brief comparison to a re-implementation of aformer system, leading to a proliferation of unsuper vised systems with little ground to compare amongthem. The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other su pervised and knowledge-based systems. The paper is organized as follows. Section 2 presents the evaluation framework used in this task. Section 3 presents the systems that participated in 1WSID approaches prefer the term ?word uses? to ?word senses?. In this paper we use them interchangeably to refer to both the induced clusters, and to the word senses from some reference lexicon. 7 the task, and the official results. Finally, Section 5 draws the conclusions.
Evaluating WSID systems. . All WSID algorithms need some addition in orderto be evaluated. One alternative is to manually de cide the correctness of the clusters assigned to each occurrence of the words. This approach has twomain disadvantages. First, it is expensive to manually verify each occurrence of the word, and dif ferent runs of the algorithm need to be evaluatedin turn. Second, it is not an easy task to manu ally decide if an occurrence of a word effectively corresponds with the use of the word the assignedcluster refers to, especially considering that the person is given a short list of words linked to the clus ter. We also think that instead of judging whether the cluster returned by the algorithm is correct, theperson should have independently tagged the occur rence with his own senses, which should have been then compared to the cluster returned by the system. This is paramount to compare a corpus which has been hand-tagged with some reference senses (alsoknown as the gold-standard) with the clustering result. The gold standard tags are taken to be the def inition of the classes, and standard measures from the clustering literature can be used to evaluate the clusters against the classes. A second alternative would be to devise a method to map the clusters returned by the systems to thesenses in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then mea sure the quality of the mapping. More recently, themapping has been used to test the system on publicly available benchmarks (Purandare and Peder sen, 2004; Niu et al, 2005).A third alternative is to evaluate the systems ac cording to some performance in an application, e.g. information retrieval (Schu?tze, 1998). This is a veryattractive idea, but requires expensive system devel opment and it is sometimes difficult to separate the reasons for the good (or bad) performance.In this task we decided to adopt the first two alternatives, since they allow for comparison over pub licly available systems of any kind. With this goal onmind we gave all the participants an unlabeled cor pus, and asked them to induce the senses and create a clustering solution on it. We evaluate the results according to the following types of evaluation: 1. Evaluate the induced senses as clusters of ex-. amples. The induced clusters are compared to the sets of examples tagged with the given gold standard word senses (classes), and evaluated using the FScore measure for clusters. We will call this evaluation unsupervised. 2. Map the induced senses to gold standard. senses, and use the mapping to tag the test cor pus with gold standard tags. The mapping is automatically produced by the organizers, and the resulting results evaluated according to theusual precision and recall measures for super vised word sense disambiguation systems. We call this evaluation supervised. We will see each of them in turn. 2.1 Unsupervised evaluation. In this setting the results of the systems are treated as clusters of examples and gold standard senses are classes. In order to compare the clusters with the classes, hand annotated corpora is needed. The testset is first tagged with the induced senses. A per fect clustering solution will be the one where each cluster has exactly the same examples as one of the classes, and vice versa.Following standard cluster evaluation practice (Zhao and Karypis, 2005), we consider the FS core measure for measuring the performance of the systems. The FScore is used in a similar fashion to Information Retrieval exercises, with precisionand recall defined as the percentage of correctly ?retrieved? examples for a cluster (divided by total clus ter size), and recall as the percentage of correctly ?retrieved? examples for a cluster (divided by total class size). Given a particular class sr of size nr and a cluster hi of size ni, suppose nir examples in the class sr belong to hi. The F value of this class and cluster is defined to be: f(sr, hi) = 2P (sr, hi)R(sr, hi) P (sr, hi) + R(sr, hi) where P (sr, hi) = nir nr is the precision value and R(sr, hi) = nir ni is the recall value defined for classsr and cluster hi. The FScore of class sr is the max imum F value attained at any cluster, that is, 8 F (sr) = max hi f(sr, hi) and the FScore of the entire clustering solution is: FScore = c? r=1 nr n F (sr) where q is the number of classes and n is the size of the clustering solution. If the clustering is theidentical to the original classes in the datasets, FS core will be equal to one which means that the higher the FScore, the better the clustering is. For the sake of completeness we also include thestandard entropy and purity measures in the unsupervised evaluation. The entropy measure consid ers how the various classes of objects are distributedwithin each cluster. In general, the smaller the entropy value, the better the clustering algorithm per forms. The purity measure considers the extent to which each cluster contained objects from primarilyone class. The larger the values of purity, the bet ter the clustering algorithm performs. For a formal definition refer to (Zhao and Karypis, 2005). 2.2 Supervised evaluation. We have followed the supervised evaluation frame work for evaluating WSID systems as described in (Agirre et al, 2006). First, we split the corpus intoa train/test part. Using the hand-annotated sense in formation in the train part, we compute a mappingmatrix M that relates clusters and senses in the fol lowing way. Suppose there are m clusters and n senses for the target word. Then, M = {mij} 1 ? i ? m, 1 ? j ? n, and each mij = P (sj |hi), that is, mij is the probability of a word having sense jgiven that it has been assigned cluster i. This probability can be computed counting the times an occur rence with sense sj has been assigned cluster hi in the train corpus. The mapping matrix is used to transform any cluster score vector h? = (h1, . . . , hm) returned by the WSID algorithm into a sense score vector s? = (s1, . . . , sn). It suffices to multiply the score vector by M , i.e., s? = h?M . We use the M mapping matrix in order to convert the cluster score vector of each test corpus instance into a sense score vector, and assign the sense with All Nouns Verbs train 22281 14746 9773 test 4851 2903 2427 all 27132 17649 12200 Table 1: Number of occurrences for the 100 target words in the corpus following the train/test split.maximum score to that instance. Finally, the result ing test corpus is evaluated according to the usual precision and recall measures for supervised word sense disambiguation systems.
Results. . In this section we will introduce the gold standard and corpus used, the description of the systems andthe results obtained. Finally we provide some mate rial for discussion. Gold StandardThe data used for the actual evaluation was bor rowed from the SemEval-2007 ?English lexical sample subtask? of task 17. The texts come from the Wall Street Journal corpus, and were hand-annotated with OntoNotes senses (Hovy et al, 2006). Note that OntoNotes senses are coarser than WordNet senses, and thus the number of senses to be induced is smaller in this case. Participants were provided with information about 100 target words (65 verbs and 35 nouns), each target word having a set of contexts where the word appears. After removing the sense tags from the train corpus, the train and test parts were joined into the official corpus and given to the participants. Participants had to tag with the induced senses all the examples in this corpus. Table 1 summarizes the size of the corpus. Participant systems In total there were 6 participant systems. One of them (UoFL) was not a sense induction system, but rather a knowledge-based WSD system. We include their data in the results section below for coherence with the official results submitted to participants, but we will not mention it here. I2R: This team used a cluster validation method to estimate the number of senses of a target word in untagged data, and then grouped the instances of thistarget word into the estimated number of clusters us ing the sequential Information Bottleneck algorithm. 9 UBC-AS: A two stage graph-based clustering where a co-occurrence graph is used to compute similarities against contexts. The context similarity matrix is pruned and the resulting associated graphis clustered by means of a random-walk type al gorithm. The parameters of the system are tuned against the Senseval-3 lexical sample dataset, and some manual tuning is performed in order to reduce the overall number of induced senses. Note that thissystem was submitted by the organizers. The orga nizers took great care in order to participate under the same conditions as the rest of participants.UMND2: A system which clusters the second or der co-occurrence vectors associated with each word in a context. Clustering is done using k-means and the number of clusters was automatically discoveredusing the Adapted Gap Statistic. No parameter tun ing is performed. upv si: A self-term expansion method based onco-ocurrence, where the terms of the corpus are ex panded by its best co-ocurrence terms in the samecorpus. The clustering is done using one implemen tation of the KStar method where the stop criterionhas been modified. The trial data was used for de termining the corpus structure. No further tuning is performed.UOY: A graph based system which creates a co occurrence hypergraph model. The hypergraph isfiltered and weighted according to some associa tion rules. The clustering is performed by selecting the nodes of higher degree until a stop criterion isreached. WSD is performed by assigning to each in duced cluster a score equal to the sum of weights of hyperedges found in the local context of the target word. The system was tested and tuned on 10 nouns of Senseval-3 lexical-sample. Official Results Participants were required to induce the senses of the target words and cluster all target word contextsaccordingly2. Table 2 summarizes the average num ber of induced senses as well as the real senses in the gold standard. 2They were allowed to label each context with a weighted score vector, assigning a weight to each induced sense. In the unsupervised evaluation only the sense with maximum weightwas considered, but for the supervised one the whole score vector was used. However, none of the participating systems la beled any instance with more than one sense. system All nouns verbs I2R 3.08 3.11 3.06 UBC-AS? 1.32 1.63 1.15 UMND2 1.36 1.71 1.17 upv si 5.57 7.2 4.69 UOY 9.28 11.28 8.2 Gold standard test 2.87 2.86 2.86 train 3.6 3.91 3.43 all 3.68 3.94 3.54Table 2: Average number of clusters as returned by the par ticipants, and number of classes in the gold standard. Note that UBC-AS? is the system submitted by the organizers of the task. System R. All Nouns Verbs FSc. Pur. Entr. FSc. FSc. 1c1word 1 78.9 79.8 45.4 80.7 76.8 UBC-AS? 2 78.7 80.5 43.8 80.8 76.3 upv si 3 66.3 83.8 33.2 69.9 62.2 UMND2 4 66.1 81.7 40.5 67.1 65.0 I2R 5 63.9 84.0 32.8 68.0 59.3 UofL?? 6 61.5 82.2 37.8 62.3 60.5 UOY 7 56.1 86.1 27.1 65.8 45.1 Random 8 37.9 86.1 27.7 38.1 37.7 1c1inst 9 9.5 100 0 6.6 12.7 Table 3: Unsupervised evaluation on the test corpus (FScore), including 3 baselines. Purity and entropy are also provided. UBC-AS? was submitted by the organizers. UofL?? is not a sense induction system. System Rank Supervised evaluation All Nouns Verbs I2R 1 81.6 86.8 75.7 UMND2 2 80.6 84.5 76.2 upv si 3 79.1 82.5 75.3 MFS 4 78.7 80.9 76.2 UBC-AS? 5 78.5 80.7 76.0 UOY 6 77.7 81.6 73.3 UofL?? 7 77.1 80.5 73.3Table 4: Supervised evaluation as recall. UBC-AS? was submitted by the organizers. UofL?? is not a sense induction sys tem. Table 3 shows the unsupervised evaluation of the systems on the test corpus. We also include three baselines: the ?one cluster per word? baseline (1c1word), which groups all instances of a word intoa single cluster, the ?one cluster per instance? baseline (1c1inst), where each instance is a distinct clus ter, and a random baseline, where the induced wordsenses and their associated weights have been ran domly produced. The random baseline figures in this paper are averages over 10 runs. As shown in Table 3, no system outperforms the 1c1word baseline, which indicates that this baseline 10is quite strong, perhaps due the relatively small num ber of classes in the gold standard. However, all systems outperform by far the random and 1c1instbaselines, meaning that the systems are able to in duce correct senses. Note that the purity and entropy measures are not very indicative in this setting. For completeness, we also computed the FScore usingthe complete corpus (both train and test). The re sults are similar and the ranking is the same. We omit them for brevity. The results of the supervised evaluation can be seen in Table 4. The evaluation is also performed over the test corpus. Apart from participants, we also show the most frequent sense (MFS), which tags every test instance with the sense that occurredmost often in the training part. Note that the su pervised evaluation combines the information in theclustering solution implicitly with the MFS information via the mapping in the training part. Pre vious Senseval evaluation exercises have shown thatthe MFS baseline is very hard to beat by unsuper vised systems. In fact, only three of the participant systems are above the MFS baseline, which showsthat the clustering information carries over the map ping successfully for these systems. Note that the1c1word baseline is equivalent to MFS in this setting. We will review the random baseline in the dis cussion section below. Further Results Table 5 shows the results of the best systems from the lexical sample subtask of task 17. The best sense induction system is only 6.9 percentage points belowthe best supervised, and 3.5 percentage points be low the best (and only) semi-supervised system. If the sense induction system had participated, it would be deemed as semi-supervised, as it uses, albeit in ashallow way, the training data for mapping the clusters into senses. In this sense, our supervised evalu ation does not seek to optimize the available training data.After the official evaluation, we realized that con trary to previous lexical sample evaluation exercises task 17 organizers did not follow a random train/test split. We decided to produce a random train/testsplit following the same 82/18 proportion as the of ficial split, and re-evaluated the systems. The results are presented in Table 6, where we can see that all System Supervised evaluation best supervised 88.7 best semi-supervised 85.1 best induction (semi-sup.) 81.6 MFS 78.7 best unsupervised 53.8 Table 5: Comparing the best induction system in this task with those of task 17. System Supervised evaluation I2R 82.2 UOY 81.3 UMND2 80.1 upv si 79.9 UBC-AS 79.0 MFS 78.4 Table 6: Supervised evaluation as recall using a random train/test split. participants are above the MFS baseline, showingthat all of them learned useful clustering informa tion. Note that UOY was specially affected by the original split. The distribution of senses in this split did not vary (cf. Table 2).Finally, we also studied the supervised evalua tion of several random clustering algorithms, which can attain performances close to MFS, thanks to the mapping information. This is due to the fact that therandom clusters would be mapped to the most fre quent senses. Table 7 shows the results of random solutions using varying numbers of clusters (e.g. random2 is a random choice between two clusters). Random2 is only 0.1 below MFS, but as the number of clusters increases some clusters don?t get mapped, and the recall of the random baselines decrease.
Discussion. . The evaluation of clustering solutions is not straightforward. All measures have some bias towards cer tain clustering strategy, and this is one of the reasonsof adding the supervised evaluation as a complemen tary information to the more standard unsupervised evaluation.In our case, we noticed that the FScore penal ized the systems with a high number of clusters, and favored those that induce less senses. Given the fact that FScore tries to balance precision (higher for large numbers of clusters) and recall (higher for small numbers of clusters), this was not expected. We were also surprised to see that no system could 11 System Supervised evaluation random2 78.6 random10 77.6 ramdom100 64.2 random1000 31.8 Table 7: Supervised evaluation of several random baselines.beat the ?one cluster one word? baseline. An expla nation might lay in that the gold-standard was based on the coarse-grained OntoNotes senses. We also noticed that some words had hundreds of instancesand only a single sense. We suspect that the partic ipating systems would have beaten all baselines if a fine-grained sense inventory like WordNet had been used, as was customary in previous WSD evaluation exercises. Supervised evaluation seems to be more neutral regarding the number of clusters, as the ranking of systems according to this measure include diverse cluster averages. Each of the induced clusters is mapped into a weighted vector of senses, and thus inducing a number of clusters similar to the number of senses is not a requirement for good results. With this measure some of the systems3 are able to beat all baselines.
Conclusions. . We have presented the design and results of theSemEval-2007 task 02 on evaluating word sense induction and discrimination systems. 6 systems participated, but one of them was not a sense induction system. We reused the data from the SemEval 2007 English lexical sample subtask of task 17, and. set up both clustering-style unsupervised evaluation(using OntoNotes senses as gold-standard) and a su pervised evaluation (using the training part of thedataset for mapping). We also provide a compari son to the results of the systems participating in the lexical sample subtask of task 17.Evaluating clustering solutions is not straightfor ward. The unsupervised evaluation seems to besensitive to the number of senses in the gold stan dard, and the coarse grained sense inventory usedin the gold standard had a great impact in the results. The supervised evaluation introduces a mapping step which interacts with the clustering solu tion. In fact, the ranking of the participating systems 3All systems in the case of a random train/test split varies according to the evaluation method used. We think the two evaluation results should be taken to be complementary regarding the information learned by the clustering systems, and that the evaluation of word sense induction and discrimination systemsneeds further developments, perhaps linked to a cer tain application or purpose. Acknowledgments We want too thank the organizers of SemEval-2007 task 17 for kindly letting us use their corpus. We are also grateful to Ted Pedersen for his comments on the evaluation results. This work has been partially funded by the Spanish education ministry (project KNOW) and by the regional government of Gipuzkoa (project DAHAD).