10 restarts 1 restart 793 Optimization Procedure labeled dependency acc. [%] Slovenian Bulgarian Dutch Max. like. 27.78 47.23 36.78 Min. error 22.52 54.72 36.78 Ann. min. risk 31.16 54.66 36.71 2: dependency accuracy on parsing 200sentence test corpora, after training 10 experts on 1000 senand fitting their weights 200 more. For Slovenian, minimum risk annealing is significantly better than the other training methods, while minimum error is significantly worse. For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other. For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped. Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do. The orthogonal of Bayes risk decoding achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004). Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones. The distinction is in using a loss function to calculate the required margins. 8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems. Different methods can be used to attempt this global, non-convex optimization. We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface. It never does significantly worse. With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP. References L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer. 1988. A new algorithm for the estimation of hidden model parameters. In pages 493–496. E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best and maxent discriminative reranking. In pages 173–180. S. F. Chen and R. Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report, CS Dept., Carnegie Mellon University. K. Crammer, R. McDonald, and F. Pereira. 2004. New large algorithms for structured prediction. In Structured Outputs M. Dreyer, D. A. Smith, and N. A. Smith. 2006. Vine parsing and minimum risk reranking for speed and precision. In G. Elidan and N. Friedman. 2005. Learning hidden variable The information bottleneck approach. 6:81–127. V. Goel and W. J. Byrne. 2000. Minimum Bayes-Risk auspeech recognition. Speech and Lan- 14(2):115–135. J. T. Goodman. 1996. Parsing algorithms and metrics. In pages 177–183. Hinton. 1999. Products of experts. In of volume 1, pages 1–6. K.-U. Hoffgen, H.-U. Simon, and K. S. Van Horn. 1995. trainability of single neurons. of Computer and 50(1):114–125. D. S. Johnson and F. P. Preparata. 1978. The densest hemiproblem. Comp. 6(93–107). S. Katagiri, B.-H. Juang, and C.-H. Lee. 1998. Pattern recognition using a family of design algorithms based upon the probabilistic descent method. 86(11):2345–2373, November. P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrasetranslation. In pages 48–54. S. Kumar and W. Byrne. 2004. Minimum bayes-risk decodfor statistical machine translation. In J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting labeling sequence data. In F. J. Och. 2003. Minimum error rate training in statistical translation. In pages 160–167. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. A method for automatic evaluation of machine In pages 311–318. K. A. Papineni. 1999. Discriminative training via linear In A. Rao and K. Rose. 2001. Deterministically annealed deof Hidden Markov Model speech recognizers. on Speech and Audio 9(2):111–126. K. Rose. 1998. Deterministic annealing for clustering, compression, classification, regression, and related optimizaproblems. 86(11):2210–2239. N. A. Smith and J. Eisner. 2004. Annealing techniques for statistical language learning. In pages 486–493.