1 Introduction. Many people are working on acquisition of multiword expressions, although terminology varies. In this paper, we are interested in lexicalised expressions (Sag et al., 2002) where special interpretation is required because of some degree of non-compositionality or semantic opacity. We are specifically concerned with what are commonly referred to as phrasal verbs, or verb and particle constructions (Baldwin and Villavicencio, 2002). As well as having idiosyncratic semantics, phrasals also display specific syntactic behaviour such as permitting particle movement when used in the transitive; for example: Jo ate up her food Jo ate her food up We are interested in phrasal verbs because we want to acquire predicate selectional preferences for word sense disambiguation (McCarthy et al., 2001). When acquiring such lexical information for a verb it is important to know when there is a special interpretation required for the verb and particle combination so that these combinations are handled separately from the simplex case. Whilst it is possible to put every single occurrence of a verb and particle combination into a lexicon this is not desirable. One wants to achieve generalisation and avoid redundancy, only storing details which cannot be created from what is already there. Not every verb modified by a particle may be a genuine multiword unit, but may instead be a fully compositional verb modified by an adverbial e.g. fly up. Also very productive verb particle combinations such as those involving verbs of motion, which often occur with a particle e.g. up, such as wander, stroll, go etc... might be better handled in the grammar (Villavicencio and Copestake, 2002). Additionally, in lexical acquisition, and for word sense disambiguation, it is important that related senses of words are identified. For example, if the verb eat is closer in meaning to a phrasal construction eat up, compared to other simplex verbs with their phrasal constructions such as blow/blow up, then the lexicon should reflect that. Having a measure of compositionality should help in this. In this paper we are not concerned with evaluation of precision and recall of the extraction of phrasal verbs from a parser, although we have done some preliminary experiments in this direction on the Wall Street Journal (wsJ), see section 3. Instead, our focus is on methods of using an automatically acquired thesaurus for detecting compositionality of candidate phrasals output from our parser. We contrast this with some statistics commonly used for multiword extraction. The thesaurus is acquired from the grammatical relations occurring with verbs, both the target phrasals and their simplex counterparts. The intuition is that the neighbours of the simplex verb should be similar to those of the phrasal where the phrasal has a compositional meaning, and that the phrasal neighbours should include phrasal candidates with the same particle. For evaluation, we obtain a sample of multiword candidates from our parser and then obtain human judgements of compositionality using an ordinal scale for compositionality. We demonstrate that there is highly significant agreement on the rank order of these judgements and use the average ranks for each item as a gold-standard to compare various measures aimed at detecting non-compositionality. In the following section we look at related work. In section 3 we show how phrasals are identified by our parser. We talk about the generation of the goldstandard set of compositionality judgements in section 4. In section 5 we describe the construction of the automatic thesaurus and the measurements we explored for detecting compositionality. In section 6 we show the correlations of our measures with the gold-standard, and compare these to some statistics commonly used for identifying compositional multiwords. In section 7 we analyse our findings, and conclude (section 8) with directions for future work.
2 Related Research. There has been a lot of recent work on extraction of multiwords from corpora we focus specifically on work involving multiword verbs, and detecting compositionality of multiwords. There have been a number of methods proposed in the literature for extracting multiword verb constructions from corpora. Baldwin and Villavicencio (2002) demonstrated that combining syntactic evidence using automatic PoS taggers and statistical chunkers, and feeding evidence from a number of tokens into a memory based-learner gave high precision and recall, using marked up WSJ text to gauge precision, and phrasals listed in the Alvey Natural Language Tools (ANLT) (Grover et al., 1993) attested in the same corpus for recall. No distinction on opaqueness of the verb and particle constructions was made. Blaheta and Johnson (2001) used log-linear models to extract English multiword verbs involving verb and particle constructions from parsed data; these include phrasal and prepositional verbs. Krenn and Evert (2001) investigated German support verb constructions (identifiable on grammatical grounds) and figurative expressions (having idiomatic interpretations). In their experiments, true positives were typically defined as such according to the annotator scanning the list. Krenn and Evert found that different statistics are suited to different types of collocation - there is no easy route for collocation extraction. Moreover, they found that a simple co-occurrence frequency fares comparably, if not better, than most statistical tests of significance. Most people researching into multiwords assume some degree of non-compositionality. Blaheta and Johnson took human judgements on phrasality, opaqueness (a dichotomous scale) and a subjective judgement of relatedness (on a scale between 1 and 5). They showed that the opaqueness judgements correlated with the relatedness (good collocation) judgement. Also, those constructions judged to be phrasals tended to have higher ranks (higher opaqueness and relatedness) than prepositional verb particle constructions. Both Lin (1999) and Schone and Jurafsky (2001) have used distributional similarity to detect compositionality in multiwords. Schone and Jurafsky used measures on the vectors representing the multiword candidates compared to measures for the words that the multiword contains but this failed to improve performance, using WordNet and other machine readable resources as gold-standards for evaluation. There was some success though in using latent semantic analysis (LsA) models to identify multiwords by the fact that the component words are typically non-substitutable, but they felt that much of what is captured by this is already handled by the statistics that they employ. Lin (1999) had already done something similar to the substitutability experiments using the method he had proposed earlier (Lin, 1998a) for automatic thesaurus construction. He identified general multiwords involving several open-class words output from his parser and filtered by the log-likelihood statistic. Using the parser yielded much better results than just a simple window for co-occurrence relationships. Lin proposed that if there is a multiword obtained by substitution of either the head or modifier in the multiword with a near neighbour, then the mutual information of this and the original multiword must be significantly different for the original multiword to be considered non-compositional. He evaluated this manually on a sample. As well as finding noncompositional multiwords, there were also a higher proportion of parser errors that met these criteria. Bannard et al. (2003) are investigating compositionality by looking at the contribution of the verb, and the particle to the semantics of the verb and particle combination; this follows on from Bannard's earlier work (2002) where he showed that compositionality judgements correlate with human judgements of similarity between the head verb and the verb and particle combination. Bannard et al. (2003) point out that Lin's method of using substitution of component words in a multiword with semantic neighbours is a good indication of productivity, but not necessarily of compositionality, since an institutionalised non-productive combination, such as frying pan would not have near neighbour substitutes, but would nevertheless be compositional. They explore four methods for detecting compositionality using resources acquired from distributional data. They use these on 40 candidates on 4 separate tasks which aim to determine whether i) the item is compositional, ii) one component word contributes its meaning iii) the verb contributes its meaning iv) the particle contributes its meaning. The classifications on each of these tasks according to these methods are contrasted with a gold standard classification from 26 judges on the same data. The methods exceed the mean agreement of the annotators in some cases, particularly as regards the contribution from the particle. Baldwin et al. (2003) are also exploring empirical models of compositionality using LSA with nounnoun compounds and verb-particle constructions. In their study, they compare the similarities of the component words with WordNet based similarity scores and demonstrate a moderate correlation, lower for noun-noun compounds. We are also exploring the relation between a verb and verb and particle combination (we use the term phrasal verb) using distributional techniques, but our evaluation is somewhat different. Evaluation of collocation extraction is a notoriously thorny problem (Krenn and Evert, 2001; Pearce, 2002). People do use MRDs such as WordNet (Schone and Jurafsky, 2001) even though they acknowledge that there will be omissions in these resources, and the phenomena in the resource may be rare or simply not attested in the particular corpus used for acquisition. Many researchers use manually annotated samples, where the judges make a binary decision on whether each candidate multiword is &quot;genuine&quot; or not (Lin, 1999; Blaheta and Johnson, 2001; Krenn and Evert, 2001; Baldwin and Villavicencio, 2002). As Krenn and Evert point out, there is low agreement between annotators who are asked to mark &quot;typical&quot; multiwords, or collocations. The intuitions behind what is typical vary, and likewise association scores vary in their ability to partition the set depending on the notion of &quot;typicality&quot; employed by the annotators. Researchers also sometimes show how well the results accord with the contents of MRDs, even though these cannot be taken as definitive. In this study we are less interested in the dichotomy of whether a putative phrasal candidate is indeed a genuine multiword or not (although it is more likely that those with low compositionality are likely to be) but we use empirical methods to gauge the position of a candidate on a continuum between the fully opaque idiom and transparent compositional phrases. Variability of idioms on a scale of compositionality has been discussed by Nunberg et al. (1994) and in the psycholinguistics literature, see (Gibbs and Nayak, 1989). Tseng (2000) also advocates use of a spectrum when considering the semantics of prepositions. We will consider compositionality as a continuous scale and ask human judges to rank multiword candidates along this. We investigate the use of these ranked judgements for evaluating compositionality measures. We also look at the relation between these judgements and appearance of the candidates in gold-standard resources such as WordNet (Miller et al., 1993) or the ANLT lexicon (Grover et al., 1993), on the premise that non-compositional phrases are more likely to be listed as multiwords in man-made resources.
3 Parser Output. For these experiments we use data from the ninety million words of the written portion of the British National Corpus parsed with the RASP parser (Briscoe and Carroll, 2002). The output of the parser is a set of grammatical relations (Carroll et al., 1998) specifying the syntactic dependency between each head and its dependent(s), read off from the phrase structure tree that is returned from the disambiguation phase. The parser uses information from ANLT such as phrasals in its dictionary. This makes it more likely to spot phrasal constructions from this list. We have already looked at recognition of verb and particle constructions in the WSJ identified purely on syntactic grounds using the parses provided with the WSJ Penn Treebank 2 (Marcus et al., 1995) as a gold standard. The results for identifying verb and particle tokens are reported in table 1, both with and without the ANLT phrasal list (ANLT phr). We also give results for comparison obtained on the same data for another wide coverage parser, (MINIPAR (Lin, 1998b)). 2 In the RASP parser grammatical relation output we identify phrasal verbs as being a verb modified by a particle (tagged RP) under the ncmod (non-clausal modifier) relation. It is quite possible that some particle tags have been given erroneously and that some genuine particles are not recognised as such by the parser, or are not attached to the verb by the parser. We only look at tokens in isolation and therefore do not collate evidence to look for syntactic evidence of particle movement as Baldwin and Villavicencio do. This would be a good way to improve phrasal extraction accuracy, particularly where a particle follows a pronoun.
4 Human Compositionality Judgements. In our experiments we asked human judges to rank phrasal verb candidates as to how compositional they are. From the full set of 4272 phrasal verb candidate types output from the RASP parser we obtained 100 candidates randomly subject to the constraint that 33 3 each came from one of 3 frequency ranges (each range covering an even number of phrasal types) from 20 to the maximum frequency. A further 16 manually selected phrasals were added to this test set. Three native English speakers ranked the 116 candidates on a numerical score 0 to 10 (10 for fullycompositional, 0 for totally opaque), or gave a &quot;don't know&quot; response. We discounted any item where any of the judges had put such a &quot;don't know&quot;. This only removed a total of 5 items, leaving a ranking from all 3 judges on 111 candidates. To investigate if the rankings from the 3 judges agreed we employed the Kendall Coefficient of Concordance (W) (Siegel and Castellan, 1988). This statistic is useful for determining inter-rater agreement where there are 3 or more judges and the judgements are ordinal, and one is interested in the ranks rather than the actual numerical values. W ranges between 0 (little agreement) and 1 (full agreement) and bears a linear relationship to the average Spearman Rank-order Correlation Coefficient taken over all possible pairs of the rankings. W is calculated as shown in equation 1 below, where n is the number of items (111 in this case), Ri is the average rank for the ith item and k is the number of raters. The second term in the denominator includes a correction for ties where: where ti is the number of tied ranks in the ith grouping of ranks. The value k(n —1)W is approximately distributed as X2 with n — 1 degrees of freedom. We obtained a W score of 0.594 which gives a X2 score of 196.30 for 110 degrees of freedom which is highly significant (probability of this value <= 0.000001).
5 Detecting Compositionality. Thesaurus: Using the method proposed by Lin (1998a) we produced a thesaurus with 500 nearest neighbours for the set of phrasal verbs as described above. Tuples of the form <verb, argument head, grammatical relation> from the parsed BNC data were used for this purpose where the verb was the multiword phrasal and the grammatical relations used were subjects and direct objects. We did likewise for the simplex verbs contained within the phrasals (e.g. blow from blow up). We investigated various measures which compare the nearest neighbours of the phrasal verb to the neighbours of the corresponding simplex verb. We also tried various measures on the neighbours of the phrasal verb. We supply short labels for these for ease of reference. overlap The size of the overlap of the top X phrasal neighbours with the same number of the corresponding simplex verb's neighbours, not including the simplex verb itself. We tried this for sures which use the automatic thesaurus we indicate whether the measure relies only on the phrasal neighbours (PN), or the simplex neighbours (SN) or some combination of both (PN SN). In this first column, we also indicate how many of the top ranked neighbours were used. Where we are evaluating scores on a numerical scale, such as the size of the overlap, we use the ranks of the numerical values and compare these to the average ranks of our gold-standard using the Spearman Rank-Order Correlation Coefficient (r8). Since we have a large enough sample, these can be used to obtain a normally distributed Z score and we can thus obtain the probability of obtaining a score such as this by chance under the null hypothesis (that there is no relationship). For the scores which involve a binary decision, such as whether a score is in WordNet or not, we use the Mann Whitney U test, which compares the gold-standard ranks for the partitioned set and gives a Z score. We use one-tailed tests because we predict the direction of the relationship. For all the scores using the automatic thesaurus, we assume that the larger the value, the more compositional the item. For the statistics (commonly used for multiword extraction) the relationship is in the other direction: high values are indicative of a non-compositional reading. We change the log-likelihood statistic to add a sign where the joint frequency of particle and verb is smaller than anticipated from that expected. From these results we can see that some of the measures from the automatic thesaurus correlate significantly with the human compositionality judgements and that these correlations are slightly stronger than those of any of the statistics used. The statistics used all correlate (in the other direction) with the human compositionality judgements, although this is slightly less so for the log-likelihood ratio. The frequency of the verb and particle seems to bear no significant relation to compositionality judgements. This is interesting because Krenn and Evert found that co-occurrence frequency was a good indication of the German multiwords, although the task there was identification of the multiwords, as opposed to measuring compositionality.
7 Analysis. MI is the statistic with the strongest value of T., and the thesaurus measure with the strongest relationship was sameparticle-simplex. These two measures correlated well together (T., = -0.51, z = -5.37) and both are significantly correlated (using the Mann Whitney U test) with whether the candidate is found in either WordNet or ANLT, see table 3, although the relationship using the automatic thesaurus is slightly higher. Lin uses a log-likelihood ratio to filter multiword candidates before using his automatic thesaurus to detect compositionality in multiwords containing 2 or more open class words. For phrasal candidates at least, it might be worth using evidence from the thesaurus on the unfiltered list. We were surprised, and a little disappointed that the straight overlap of neighbours did not give a significant relationship, other than for the overlap of 30 neighbours. We believe this is due to the large scope for open class words as neighbours, and that there is often some element of meaning added by the particle. Thus the overlap where we reduce neighbours of the phrasal to simplex form compensated for this. We have not yet explored varying the number of neighbours for methods other than the overlap and overlapS. We feel that it would be worth exploring the effect of the number of neighbours further, and also to use the similarity scores of the neighbours, rather than simple measures operating on the types occurring as neighbours. This would help control for the fact that for some verbs there are not many close neighbours and neighbours further down the ranked list may in fact be quite distant. Whilst statistics are useful indicators of noncompositionality, there are compositional multiwords which have low values for these statistics, yet are highly non-compositional. A good example is cock up; it is the lowest ranked for compostionality by the human judges, but its MI value is only 5.02, and according to MI it is ranked between the somewhat more compositional candidates tie down and come down. The automatic thesaurus measures such as sameparticle-simplex give a low compositionality score and place it at the end between carry out and latch on. There are also candidates with high values of the statistics, yet they are in the middle range of the compositionality judgements, for example, plod on. This is simply because of a high co-occurrence frequency. Whether such an unexpectedly high cooccurrence frequency warrants an entry in the lexicon depends on the type of lexicon being built.
8 Conclusions. We can see that there is a significant relationship between the human compositionality judgements and some of the measures from the automatic thesaurus, particularly those that endeavour to take into acof measures with man-made resources count the semantics of the particle. This relationship is stronger than statistics which have previously been used for filtering candidate multiwords which suggests that it might be better not to filter with statistics before looking at compositionality using an automatic thesaurus. We have not yet exploited these measures in the construction of a lexicon for phrasal verbs. Identifying non-compositional phrasals by employing thresholds to force a binary decision is one option. This would help in determining which candidate phrasals should be treated separately from the simplex for purposes such as selectional preference acquisition and word sense disambiguation. The thresholds might be acquired empirically from some training data, such as the compositionality judgements we have used. However, we believe that permitting measurements and evaluation on a continuum of compositionality allows for a more natural exploration of relationships, without imposing an arbitrary cut-off point required only when finally categorising items for a lexicon. It also could be useful to use the measures to tell whether the meaning comes from the verb or the particle or both, as Bannard et al. (2003) do, because if the verb contributes its meaning then data for selectional preference acquisition might be amalgamated with those of the simplex counterpart.
9 Acknowledgements. This work was supported by the EPSRC-funded RASP project (grant GR/N36493), and the EU 5th Framework project MEANING — Developing Multilingual Web-scale Language Technologies (IST-200134460). We are grateful to Timothy Baldwin and Colin Bannard for their helpful comments and useful references.