While a different order for these predictions is possible, we only experimented with this one. Parameter Estimation We only have built a decision tree to the rule probability component (3) of the model. For the mowe are using with the usual interpolation smoothing for the other four components of the model. We have assigned bit strings to the syntactic and semantic categories and to the rules manually. Our intention is that bit strings differing in the least significant bit positions correspond to categories of non-terminals or rules that are similar. We also have assigned bitstrings for the words in the vocabulary (the lexical heads) using automatic clustering algorithms using the bigram mutual information clustering algorithm (see (5)). Given the bitsting of a history, we then designed a decision tree for modeling the probability that a rule will be used for rewriting a node in the parse tree. Since the grammar produces parses which may be more detailed than the Treebank, the decision tree was built using a training set constructed in the following manner. Using the grammar with the P-CFG model we determined the most likely parse that is consistent with the Treebank and considered the resulting sentence-tree pair as an event. Note that the grammar parse will also provide the lexical head structure of the parse. Then, we extracted using leftmost derivation order tuples of a history (truncated to the definition of a history in the HBG model) and the corresponding rule used in expanding a node. Using the resulting data set we built a decision tree by classifying histories to locally minimize the entropy of the rule template. With a training set of about 9000 sentencetree pairs, we had about 240,000 tuples and we grew a tree with about 40,000 nodes. This required 18 hours on a 25 MIPS RISC-based machine and the resulting decision tree was nearly 100 megabytes. Immediate vs. Functional Parents model employs two types of parents, the and the The a list Figure 3: Sample representation of &quot;with a list&quot; in HBG model. R: PP1 Syn: PP H1: list with R: NBAR4 Syn: NP Sem: Data H1: list H2: a R: N1 Syn: N Sem: Data H1: list H2: * 35 immediate parent is the constituent that immediately dominates the constituent being predicted. If the immediate parent of a constituent has a different syntactic type from that of the constituent, then the immediate parent is also the functional parent; otherwise, the functional parent is the functional parent of the immediate parent. The distinction between functional parents and immediate parents arises primarily to cope with unit productions. When unit productions of the form XP2 ---> XP1 occur, the immediate parent of XP1 is XP2. But, in general, the constituent XP2 does not contain enough useful information for ambiguity resolution. In particular, when considering only immediate parents, unit rules such as NP2 —■ NP1 prevent the probabilistic model from allowing the NP1 constituent to interact with the VP rule which is the functional parent of NP1. When the two parents are identical as it often happens, the duplicate information will be ignored. However, when they differ, the decision tree will select that parental context which best resolves ambiguities. Figure 3 shows an example of the representation of a history in HBG for the prepositional phrase &quot;with a list.&quot; In this example, the immediate parent of the Ni node is the NBAR4 node and the functional parent of Ni is the PP1 node. Results We compared the performance of HBG to the &quot;broad-coverage&quot; probabilistic context-free gram- P-CFG. The of the grammar is 90% on test sentences of 7 to 17 words. The of P-CFG is 60% on the same test corpus of 760 sentences used in our experiments. On the same test sentences, the HBG model has a of 75%. This is a reduction of 37% in error rate. Accuracy P-CFG 59.8% HBG 74.6% Error Reduction 36.8% Figure 4: Parsing accuracy: P-CFG vs. HBG In developing HBG, we experimented with similar models of varying complexity. One discovery made during this experimentation is that models which incorporated more context than HBG performed slightly worse than HBG. This suggests that the current training corpus may not contain enough sentences to estimate richer models. Based on the results of these experiments, it appears likely that significantly increasing the size of the training corpus should result in a corresponding improvement in the accuracy of HBG and richer HBG-like models. To check the value of the above detailed history, we tried the simpler model: 1. 2. 3. p(Syn p(Sem ISyn, p(R ISyn, Sem, This model corresponds to a P-CFG with NTs that are the crude syntax and semantic categories with the lexical heads. The in this case was 66%, a small improvement over the P-CFG model indicating the value of using more context from the derivation tree. Conclusions The success of the HBG model encourages future development of general history-based grammars as a more promising approach than the usual P-CFG. More experimentation is needed with a larger Treebank than was used in this study and with different aspects of the derivation history. In addition, this paper illustrates a new approach to grammar development where the parsing problem is divided (and hopefully conquered) into two subproblems: one of grammar coverage for the grammarian to address and the other of statistical modeling to increase the probability of picking the correct parse of a sentence.