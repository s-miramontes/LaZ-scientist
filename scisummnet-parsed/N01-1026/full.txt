. French sides. There are two central limitations to this paradigm, however. The first is the often very poor accuracy of word alignments, due both to the current limitations of word-alignment algorithms, and also to the often weak or incomplete inherent match between the two sides of a bilingual corpus. The paper will address and handle this problem through robust, noise-tolerant learning algorithms capable of being trained effectively on incomplete and highly inaccurate alignments. The second limitation is the potential mismatch in the annotation needs of two languages; not all distinctions that may be desirable for one language (such as grammatical gender in French) are compatible or even present in a parallel language such as English. The paper will discuss solutions to these language-level mismatches, and will illustrate that at the level of noun-phrase structure and core part-of-speech tags, essential annotations can be projected with remarkable effectiveness and coverage in many cases. Finally, the paper will empirically evaluate two major questions for each of the tasks:
2 Background. The approach and general algorithms investigated in this paper were initiated in conjunction with the EGYPT project of the 1999 Johns Hopkins summer machine translation workshop (Al-Onaizan et al., 1999). Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments rather than the induction of stand-alone monolingual analyzers via cross-language projection. For example, Kupiec (1993) began with existing Xerox monolingual bracketers to improve translation alignments, rather than the converse. The primary exception has been in the area of parallel bilingual parsing. Wu (1995, 1997) proposed a framework for inversion transduction grammars, where parallel corpora in languages such as English and Chinese are parsed concurrently, with crosslanguage order differences captured via mobile-like CFG production reordering. Structural relationships in one language help constrain structural relationships in the second language. Evaluation on noun-phrase bracketing showed 78% precision for Chinese, and 80% precision for English. Thus, while remarkably effective for learning without humanannotated training data, the algorithm does assume the existence of a parallel second-language mirror for all sentences to be parsed. Also, Wu observed significant performance degradation when either the word alignment or translation faithfulness in these pairs are weak. This further motivates the noise-robust training and stand-alone application of our current work. In a related framework, Jones and Havrilla (1998) investigated the use of twisted-pair grammars for syntactic transfer. Given an existing Hindi/Urdu sentence parse, English output was generated by rotating subtrees using the constraints and preferences of the transduction grammar. The ability to generate candidate target-language orderings in this manner offers great potential to productively constrain search in a statistical MT system. Yet the assumption of existing syntactic analyses for each source language further motivates the need to induce such analyses.
3 Data Resources. The data used in our experiments are the EnglishFrench Canadian Hansards and English-Chinese Hong Kong Hansards, parallel records of parliamentary proceedings and publications. Both corpora were word-aligned by the now publicly available EGYPT system (Al-Onaizan et al., 1999) and based on IBM's Model 3 statistical MT formalism (Brown et al., 1990). The data sets used for our projection studies both contained approximately 2 million words in each language. Their alignment was based on strictly word-based model variants for English and character-based model variants for Chinese, with no use of morphological analysis or stemming, POS-tagging, bracketing, outside dictionaries or any other external data source or annotation tool.' Thus the experiments were carefully designed 1-The two exceptions are end-of-sentence detection and tokenization. For the French Hansards, before alignment only even when the high-error automatic alignments have been manually corrected, yielding 69% and 78% direct projection accuracy respectively (at English tagset granularity). Traditional supervised learning algorithms tend to perform poorly at this level of noise, and a standard bigram tagger trained on the automatically aligned (uncorrected) data achieves only 82% when evaluated on a held-out test set. More highly lexicalized learning algorithms exhibit even greater potential for overmodeling the specific projection errors of this data. Thus our research has focused on noise-robust techniques for distilling a conservative but effective tagger from this challenging raw projection data. To do so, we (a) downweight or exclude training data segments identified as poorly aligned or likely noise (b) use a conservative bigram learning algorithm, and (c) train the lexical prior and tag-sequence models separately using aggressive generalization techniques. In a standard bigram tagging model, one selects a tag sequence T for a word sequence W by: argmax P(TIW) = P(WIT)P(T) where using standard independence assumptions. Section 4.2.2 will discuss the estimation of P(tilti-i)• The following section describes the estimation of P(tiiwi), which using Bayes rule and direct (relatively noise-free) measurement of P(w) from the French data, can be used to calculate P(wiiti) as: Inspection of the raw projected tag data shows the need for an improved estimation of P(t1w). Temporarily excluding the case of compound alignments (e.g. NNS(,), Table 1 shows the observed frequency distributions of English tags projected onto four French words from 1-to-1 alignments, for the core N/V/J/R/I POS tags. Note that the total probability mass assigned to potentially correct tags (in bold) is relatively low, with fairly broad misassignment to incorrect tags for the given word. At the core tag level in particular, we observe empirically that words in French have a strong tendency to have only 1 possible core POS tag, and very rarely have more than 2. Even in English, with relatively high P(POSIw) ambiguity, only 0.37% of the tokens in the Brown Corpus are not covered by a word type's two most frequent core tags, and in French the percentage drops to 0.03%. Thus we employ an aggressive re-estimation in favor of this bias, where for t(i) = the ith most frequent tag for w: giving the large majority of the new probability mass to the single highest frequency core tag. Applying this model recursively, the finer grained subtag probabilities (e.g. NN, NNS) are assigned by selecting the two highest frequency subtags for each of the two remaining core tags, and reallocating the core tag probability mass between these two as in the equations above, as illustrated in Table 2. Finally, the issue arises of what to do with the 1-to-n phrasal alignment cases shown in Figure 2 (e.g. potatoes/NNS pommesINNS„ de/NNSb terre/NNS, and Laws/NNS Les/NNS„ /ois/NNSb). The potential seems to be great for function words to inherit substantial spurious probability mass via such data. However, the relatively frequent occurrence of correct 1-to-1 alignments (e.g. TheluN4Les and ofliN-xcle), the diffuse nature of the noise, and the aggressive smoothing towards a single POS tag, prevent these cases from adversely affecting final function word assignments. Given the lower frequency of most content words, the potential risks of using these 1-to-n alignments are greater, but so are the benefits given that the 1-to-1 alignments tend to be both sparse and somewhat biased. Several options are under investigation for combining these two P(t1w) estimators, but the simplest, and currently most effective, is to perform basic interpolation between the tag distributions estimated from 1-to-1 alignments only and from the entire set of 1-to-n alignments (including 1-to-1) as follows: While this does indeed introduce substantial spurious tag probabilities initially, the aggressive smoothing towards the majority tag(s) described above tends to eliminate most of this noise. The major reason for estimating the lexical priors and tag sequence model separately is that a tag sequence bigram (or even trigram) model has far fewer parameters than the lexical prior model and thus can be estimated on a very conservatively chosen set of filtered, high confidence alignment data. In contrast, the lexical prior models already suffer from sparse data problems and are negatively affected by an order-of-magnitude data reduction, even if the data is of higher quality. The proposed model for identifying high-quality tag sequence data for training considers two different information sources for sentence filtering/weighting. The first is the final Model-3 alignment score for the sentence, indicating a multi-source measure of overall alignment confidence. The second measure more directly targets confidence in the tag sequences themselves. After the lexical prior models have been trained (as above), sentences are also tested to identify those where the directly projected tag sequence (from the automatic alignments) is closely compatible with the estimated lexical prior probabilities for each word. A pseudo-divergence weighting is computed for a sentence of length k by I Ejk_i log P (projected-tag, lw,), penalizing words whose projected tag doesn't match the majority lexical prior.2 Sorting and filtering/weighting by the cumulative normalized score yields a subset of training data where multiple sources essentially concur on the correct tag sequence. While the potential exists that this higher confidence data subset may be biased in the sequence phenomena it contains, the substantial noise reduction in preliminary investigations appears to be a worthwhile tradeoff. Future work will focus on differential confidence weighting of sentence fragments, and iterative (E-M) re-estimation. 2The exception is for function words (i.e. the majority lexical prior is not a Noun, Verb, Adjective or Adverb) located in a 1-to-n alignment sequence. Given the very high probability of these raw projections being incorrect, and their prevalence, it is expedient to attempt to correct (rather than weight/filter) these tag instances prior to the first tagsequence-model training, by replacing their raw projection tag with the majority lexical prior for the word from 4.2.1. Doing so salvages very large quantities of otherwise accurate tag sequence data with very little introduced noise. Evaluation of the tagger projection and induction algorithms is conducted on two granularities of tagset. The first tagset is at the level of core part-of-speech tags such as Verb (V), Noun (N), Pronoun (P), Adjective (J), Adverb (R), Preposition (I), Determiner (D), etc., for which English and French share remarkable compatibility.' The second is at the level of granularity captured in the English Penn Treebank tagset, where for example singular and plural nouns (NN and NNS) are distinguished. As previously noted, the goal of this work is not to induce potential French tagset features such as grammatical gender, mood or subtle tense distinctions that do not appear in English, but to focus on the algorithm's effectiveness at accurately transferring tagging capabilities at the granularity that is present in English (or whichever projection source language used). For independent evaluation data, a 120K-word hand-tagged French dataset generously provided by Universite de Montreal was used. However, because both this text stream and tagset had no overlap with parallel data used to train the algorithm, a simple mapping table between the tagsets was defined so that output could be compared on a compatible common denominator. An abbreviated version is shown in Table 3:4 The large majority of these compatible divergences in bracketing convention are due to the projection algorithm's tendency to bracket possessive compounds as single NP's (e.g. [DT N de N]), and its tendency to bracket simple conjunctive compounds (e.g. [DT N et ND also as single NPs, following the Ramshaw and Marcus convention which differed from the French and Chinese goldstandard annotator's intuitions. Overall, these translingual projection results are quite encouraging. For Chinese, they are similar to Wu's 78% precision result, and especially promising given that no word segmentation (only raw characters) were used. For French, the increase from 59% F-measure on direct projection to 91% F-measure for the stand-alone induced bracketer shows that the training algorithm is able to generalize successfully from the very noisy raw projection data, distilling a reasonably accurate (and transferable) model of BaseNP structure from this high degree of noise.
6 Conclusion. This paper has shown that automatically wordaligned bilingual corpora can be used to induce both successful part-of-speech taggers and noun-phrase bracketers. It has further illustrated that simple direct projection of POS and NP annotations across languages is very noisy, even when the word alignments have been manually corrected. Noise-robust data filtering and modeling procedures are shown to train effectively on this low-quality data. The resulting stand-alone part-of-speech taggers and BaseNP bracketers significantly outperform the raw direct projections on which they were trained. This indicates that they have successfully distilled and modeled the signal present in the very noisy projection data, and are able to perform as respectable standalone monolingual tools with absolutely no humansupervised training data in the target language. These results also show considerable potential for further improvement by co-training with monolingually induced morphological analyzers. The standalone monolingual POS taggers and bracketers induced from word-aligned data also show potential for improving their initial alignments. NP bracketings for both the source and target language can improve the IBM MT distortion model, by boosting the probabilities of word alignments consistent with cohesive NP structure, and penalizing alignments that break NP cohesion. A stand-alone POS tagger applicable to new data can be used to improve statistical MT translation models, both by supporting finer translation model granularity (e.g. wind/NN modeled distinctly from wind/VB), and by serving as a source of backoff alignment probabilities for previously unseen words. Thus tagging models induced from bilingual alignments can be used to improve these very alignments, and hence improve their own training source.