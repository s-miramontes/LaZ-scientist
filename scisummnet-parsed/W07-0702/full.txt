Introduction. In large-scale machine translation evaluations,phrase-based models generally outperform syntax based models1. Phrase-based models are effectivebecause they capture the lexical dependencies be tween languages. However, these models, which are equivalent to finite-state machines (Kumar and Byrne, 2003), are unable to model long range word order differences. Phrase-based models also lack the ability to incorporate the generalisations implicit insyntactic knowledge and they do not respect linguistic phrase boundaries. This makes it difficult to im prove reordering in phrase-based models. Syntax-based models can overcome some of theproblems associated with phrase-based models because they are able to capture the long range struc tural mappings that occur in translation. Recently 1www.nist.gov/speech/tests/mt/mt06eval official results.html there have been a few syntax-based models that show performance comparable to the phrase-basedmodels (Chiang, 2005; Marcu et al, 2006). How ever, reliably learning powerful rules from parallel data is very difficult and prone to problems with sparsity and noise in the data. These models also suffer from a large search space when decoding with an integrated language model, which can lead to search errors (Chiang, 2005).In this paper we investigate the idea of incorporating syntax into phrase-based models, thereby lever aging the strengths of both the phrase-based models and syntactic structures. This is done using CCG supertags, which provide a rich source of syntactic information. CCG contains most of the structure ofthe grammar in the lexicon, which makes it possible to introduce CCG supertags as a factor in a factored translation model (Koehn et al, 2006). Fac tored models allow words to be vectors of features:one factor could be the surface form and other fac tors could contain linguistic information. Factored models allow for the easy inclusion of supertags in different ways. The first approach is to generate CCG supertags as a factor in the target and then apply an n-gram model over them, increasing the probability of more frequently seen sequencesof supertags. This is a simple way of including syn tactic information in a phrase-based model, and has also been suggested by Hassan et al (2007). For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation. By preferring more likely sequences of supertags, it is conceivable that the output of the decoder is 9 more grammatical. However, its not clear exactlyhow syntactic information can benefit a flat structured model: the constraints contained within su pertags are not enforced and relationships between supertags are not linear. We perform experiments to explore the nature and limits of the contribution of supertags, using different orders of n-gram models, reordering models and focussed manual evaluation. It seems that the benefit of using n-gram supertagsequence models is largely from improving reordering, as much of the gain is eroded by using a lexi calised reordering model. This is supported by the manual evaluation which shows a 44% improvement in reordering Dutch-English verb final sentences. The second and novel way we use supertags is to direct the translation process. Supertags on thesource sentence allows the decoder to make decisions based on the structure of the input. The subcategorisation of a verb, for instance, might help select the correct translation. Using multiple dependencies on factors in the source, we need a strat egy for dealing with sparse data. We propose using a logarithmic opinion pool (Smith et al, 2005) to combine the more specific models (which depend onboth words and supertags) with more general mod els (which only depends on words). This paper is the first to suggest this approach for combining multiple information sources in machine translation.Although the addition of supertags to phrase based translation does show some improvement, their overall impact is limited. Sequence models over supertags clearly result in some improvementsin local reordering but syntactic information con tains long distance dependencies which are simply not utilised in phrase-based models.
Factored Models. . Inspired by work on factored language models, Koehn et al (2006) extend phrase-based models to incorporate multiple levels of linguistic knowledgeas factors. Phrase-based models are limited to se quences of words as their units with no access to additional linguistic knowledge. Factors allow for richer translation models, for example, the gender or tense of a word can be expressed. Factors also allow the model to generalise, for example, the lemma of a word could be used to generalise to unseen inflected forms. The factored translation model combines features in a log-linear fashion (Och, 2003). The most likely target sentence t? is calculated using the decision rule in Equation 1: t? = argmax t { M? m=1 ?mhm(s Fs 1 , t Ft 1 ) } (1) t? M? m=1 ?mhm(s Fs 1 , t Ft 1 ) (2) where M is the number of features, hm(s Fs 1 , t Ft 1 ) are the feature functions over the factors, and ? are the weights which combine the features which areoptimised using minimum error rate training (Venu gopal and Vogel, 2005). Each function depends on a vector sFs1 of source factors and a vector t Ft1 of tar get factors. An example of a factored model used in upcoming experiments is: t? M? m=1 ?mhm(sw, twc) (3) where sw means the model depends on (s)ource (w)ords, and twc means the model generates (t)arget (w)ords and (c)cg supertags. The model is shown graphically in Figure 1. WordWord CCG SOURCE TARGETFigure 1. Factored translation with source words deter mining target words and CCG supertagsFor our experiments we used the following fea tures: the translation probabilities Pr(sFs1 |t Ft 1 ) and Pr(tFt1 |s Fs 1 ), the lexical weights (Koehn et al, 2003) lex(sFs1 |t Ft 1 ) and lex(t Ft 1 |s Fs 1 ), and a phrase penalty e, which allows the model to learn a preference for longer or shorter phrases. Added to these features 10 is the word penalty e?1 which allows the model to learn a preference for longer or shorter sentences, the distortion model d that prefers monotone word order, and the language model probability Pr(t). All these features are logged when combined in the log-linear model in order to retain the impact of very unlikely translations or sequences. One of the strengths of the factored model is it allows for n-gram distributions over factors on the target. We call these distributions sequence models. By analogy with language models, for example, we can construct a bigram sequence model as follows: p(f1, f2, . . . fn) = p(f1) n? i=2 p(fi|f(i?1)) where f is a factor (eg. CCG supertags) and n is the length of the string. Sequence models over POS tags or supertags are smaller than language modelsbecause they have restricted lexicons. Higher or der, more powerful sequence models can therefore be used. Applying multiple factors in the source can lead to sparse data problems. One solution is to break down the translation into smaller steps and translate each factor separately like in the following model where source words are translated separately to the source supertags: t? M? m=1 ?mhm(sw, tw) + N? n=1 ?nhn(sc, tw) However, in many cases multiple dependenciesare desirable. For instance translating CCG supertags independently of words could introduce er rors. Multiple dependencies require some form of backing off to simpler models in order to cover the cases where, for instance, the word has been seen intraining, but not with that particular supertag. Dif ferent backoff paths are possible, and it would beinteresting but prohibitively slow to apply a strat egy similar to generalised parallel backoff (Bilmesand Kirchhoff, 2003) which is used in factored language models. Backoff in factored language models is made more difficult because there is no obvious backoff path. This is compounded for fac tored phrase-based translation models where one has to consider backoff in terms of factors and n-gramlengths in both source and target languages. Fur thermore, the surface form of a word is probably themost valuable factor and so its contribution must al ways be taken into account. We therefore did not use backoff and chose to use a log-linear combination of features and models instead. Our solution is to extract two translation models: t? M? m=1 ?mhm(swc, tw) + N? n=1 ?nhn(sw, tw) (4) One model consists of more specific features m and would return log probabilities, for example log2Pr(tw|swc), if the particular word and supertaghad been seen before in training. Otherwise it re turns ?C, a negative constant emulating log2(0). The other model consist of more general features n and always returns log probabilities, for example log2Pr(tw|sw).
CCG and Supertags. . CCGs have syntactically rich lexicons and a small set of combinatory operators which assemble the parse-trees. Each word in the sentence is assigned a category from the lexicon. A category may either be atomic (S, NP etc.) or complex (S\S, (S\NP)/NP etc.). Complex categories have the general form?/? or ?\? where ? and ? are themselves cate gories. An example of a CCG parse is given: Peter eats apples NP (S\NP)/NP NP > S\NP < S where the derivation proceeds as follows: ?eats? is combined with ?apples? under the operation of forward application. ?eats? can be thought of as a function that takes a NP to the right and returns a S\NP. Similarly the phrase ?eats apples? can be thought of as a function which takes a noun phraseNP to the left and returns a sentence S. This opera tion is called backward application.A sentence together with its CCG categories al ready contains most of the information present in a full parse. Because these categories are lexicalised, 11they can easily be included into factored phrase based translation. CCG supertags are categories that have been provided by a supertagger. Supertagswere introduced by Bangalore (1999) as a way of in creasing parsing efficiency by reducing the number of structures assigned to each word. Clark (2002) developed a suppertagger for CCG which uses a conditional maximum entropy model to estimate theprobability of words being assigned particular cat egories. Here is an example of a sentence that has been supertagged in the training corpus: We all agree on that . NP NP\NP (S[dcl]\NP)/PP PP/NP NP .The verb ?agree? has been assigned a complex su pertag (S[dcl]\NP)/PP which determines the type and direction of its arguments. This information can be used to improve the quality of translation.
Experiments. . The first set of experiments explores the effect of CCG supertags on the target, translating from Dutch into English. The last experiment shows the effect of CCG supertags on the source, translating from German into English. These language pairs present a considerable reordering challenge. For example,Dutch and German have SOVword order in subordi nate clauses. This means that the verb often appears at the end of the clause, far from the position of the English verb. 4.1 Experimental Setup. The experiments were run using Moses2, an opensource factored statistical machine translation system. The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation. The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was pro vided with the C&C Language Processing Tools3. The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006). The Dutch-English parallel training data comesfrom the Europarl corpus (Koehn, 2005) and ex cludes the proceedings from the last quarter of 2000. 2see http://www.statmt.org/moses/ 3see http://svn.ask.it.usyd.edu.au/trac/candc/wiki This consists of 855,677 sentences with a maximum of 50 words per sentence. 500 sentences of tuning data and the 2000 sentences of test data are takenfrom the ACLWorkshop on Building and Using Par allel Texts4. The German-English experiments use data from the NAACL 2006 Workshop on Statistical Machine Translation5. The data consists of 751,088 sentences of training data, 500 sentences of tuning data and3064 sentences of test data. The English and Ger man training sets were POS tagged and supertagged before lowercasing. The language models and thesequence models were trained on the Europarl train ing data. Where not otherwise specified, the POStag and supertag sequence models are 5-gram mod els and the language model is a 3-gram model. 4.2 Sequence Models Over Supertags. Our first Dutch-English experiment seeks to estab lish what effect sequence models have on machinetranslation. We show that supertags improve trans lation quality. Together with Shen et al (2006) it is one of the first results to confirm the potential of the factored model. Model BLEU sw, tw 23.97 sw, twp 24.11 sw, twc 24.42 sw, twpc 24.43 Table 1. The effect of sequence models on Dutch-EnglishBLEU score. Factors are (w)ords, (p)os tags, (c)cg su pertags on the source s or the target tTable 1 shows that sequence models over CCG su pertags in the target (model sw, twc) improves over the baseline (model sw, tw) which has no supertags. Supertag sequence models also outperform models which apply POS tag sequence models (sw, twp) and, interestingly do just as well as models whichapply both POS tag and supertag sequence mod els (sw, twps). Supertags are more informative than POS tags as they contain the syntactic context of a word. These experiments were run with the distortion limit set to 6. This means that at most 6 words in 4see http://www.statmt.org/wpt05/ 5see http://www.statmt.org/wpt06/ 12 the source sentence can be skipped. We tried setting the distortion limit to 15 to see if allowing longer distance reorderings with CCG supertag sequence models could further improve performance, however it resulted in a decrease in performance to a BLEU score of 23.84. 4.3 Manual Analysis. The BLEU score improvement in Table 1 does not explain how the supertag sequence models affect the translation process. As suggested by Callison-Burch et al(2006) we perform a focussed manual analysis of the output to see what changes have occurred. From the test set, we randomly selected 100 sentences which required reordering of verbs: the Dutch sentences ended with a verb which had to be moved forward in the English translation. We record whether or not the verb was correctly translated and whether it was reordered to the correct position in the target sentence. Model Translated Reordered sw, tw 81 36 sw, twc 87 52 Table 2. Analysis of % correct translation and reordering of verbs for Dutch-English translation In Table 2 we can see that the addition of the CCGsupertag sequence model improved both the transla tion of the verbs and their reordering. However, theimprovement is much more pronounced for reordering. The difference in the reordering results is signif icant at p < 0.05 using the ?2 significance test. Thisshows that the syntactic information in the CCG su pertags is used by the model to prefer better word order for the target sentence.In Figure 2 we can see two examples of DutchEnglish translations that have improved with the ap plication of CCG supertag sequence models. In the first example the verb ?heeft? occurs at the end of the source sentence. The baseline model (sw, tw) does not manage to translate ?heeft?. The model with the CCG supertag sequence model (sw, twc) translates it correctly as ?has? and reorders it correctly 4 placesto the left. The second example also shows the se quence model correctly translating the Dutch verb at the end of the sentence ?nodig?. One can see that it is still not entirely grammatical. The improvements in reordering shown here are reorderings over a relatively short distance, two or three positions. This is well within the 5-gram orderof the CCG supertag sequence model and we there fore consider this to be local reordering. 4.4 Order of the Sequence Model. The CCG supertags describe the syntactic context of the word they are attached to. Therefore theyhave an influence that is greater in scope than sur face words or POS tags. Increasing the order ofthe CCG supertag sequence model should also increase the ability to perform longer distance reorder ing. However, at some point the reliability of the predictions of the sequence models is impaired due to sparse counts. Model None 1gram 3gram 5gram 7gram sw, twc 24.18 23.96 24.19 24.42 24.32 sw, twpc 24.34 23.86 24.09 24.43 24.14 Table 3. BLUE scores for Dutch-English models which apply CCG supertag sequence models of varying orders In Table 3 we can see that the optimal order for the CCG supertag sequence models is 5. 4.5 Language Model vs. Supertags. The language model makes a great contribution to the correct order of the words in the target sentence. In this experiment we investigate whether by using astronger language model the contribution of the sequence model will no longer be relevant. The relative contribution of the language mode and different sequence models is investigated for different lan guage model n-gram lengths. Model None 1gram 3gram 5gram 7gram sw, tw - 21.22 23.97 24.05 24.13 sw, twp 21.87 21.83 24.11 24.25 24.06 sw, twc 21.75 21.70 24.42 24.67 24.60 sw, twpc 21.99 22.07 24.43 24.48 24.42 Table 4. BLEU scores for Dutch-English models which use language models of increasing n-gram length. Column None does not apply any language model. Model sw, tw does not apply any sequence models, and model sw, twpc applies both POS tag and supertag sequence models. In Table 4 we can see that if no language model is present(None), the system benefits slightly from 13 source:hij kan toch niet beweren dat hij daar geen exacte informatie over heeft ! reference: how can he say he does not have any precise information ? sw, tw:he cannot say that he is not an exact information about . sw, twc: he cannot say that he has no precise information on this ! source: wij moeten hun verwachtingen niet beschamen . meer dan ooit hebben al die landen thans onze bijstand nodig reference: we must not disappoint them in their expectations , and now more than ever these countries need our help sw, tw:we must not fail to their expectations , more than ever to have all these countries now our assistance necessary sw, twc: we must not fail to their expectations , more than ever , those countries now need our assistance Figure 2. Examples where the CCG supertag sequence model improves Dutch-English translation having access to all the other sequence models. However, the language model contribution is verystrong and in isolation contributes more to transla tion performance than any other sequence model. Even with a high order language model, applyingthe CCG supertag sequence model still seems to im prove performance. This means that even if we usea more powerful language model, the structural in formation contained in the supertags continues to be beneficial. 4.6 Lexicalised Reordering vs. Supertags. In this experiment we investigate using a strongerreordering model to see how it compares to the con tribution that CCG supertag sequence models make. Moses implements the lexicalised reordering model described by Tillman (2004), which learns whetherphrases prefer monotone, inverse or disjoint orienta tions with regard to adjacent phrases. We apply this reordering models to the following experiments. Model None Lex. Reord. sw, tw 23.97 24.72 sw, twc 24.42 24.78Table 5. Dutch-English models with and without a lexi calised reordering model.In Table 5 we can see that lexicalised reordering improves translation performance for both models. However, the improvement that was seen us ing CCG supertags without lexicalised reordering, almost disappears when using a stronger reorderingmodel. This suggests that CCG supertags? contribution is similar to that of a reordering model. The lex icalised reordering model only learns the orientation of a phrase with relation to its adjacent phrase, so its influence is very limited in range. If it can replace CCG supertags, it suggests that supertags? influence is also within a local range. 4.7 CCG Supertags on Source. Sequence models over supertags improve the performance of phrase-based machine translation. However, this is a limited way of leveraging the rich syn tactic information available in the CCG categories. We explore the potential of letting supertags direct translation by including them as a factor on the source. This is similar to syntax-directed translation originally proposed for compiling (Aho and Ullman, 1969), and also used in machine translation (Quirk et al., 2005; Huang et al, 2006). Information about thesource words? syntactic function and subcategori sation can directly influence the hypotheses beingsearched in decoding. These experiments were per formed on the German to English translation task, in contrast to the Dutch to English results given in previous experiments. We use a model which combines more specificdependencies on source words and source CCG su pertags, with a more general model which only has dependancies on the source word, see Equation 4.We explore two different ways of balancing the sta tistical evidence from these multiple sources. The first way to combine the general and specific sources of information is by considering features from bothmodels as part of one large log-linear model. However, by including more and less informative features in one model, we may transfer too much ex planatory power to the more specific features. Toovercome this problem, Smith et al (2006) demon strated that using ensembles of separately trainedmodels and combining them in a logarithmic opin ion pool (LOP) leads to better parameter values. This approach was used as the second way in which 14 we combined our models. An ensemble of log-linearmodels was combined using a multiplicative con stant ? which we train manually using held out data. t? ? M? m=1 ?mhm(swc, tw) + ? ( N? n=1 ?nhn(sw, tw) )Typically, the two models would need to be normalised before being combined, but here the multi plicative constant fulfils this ro?le by balancing theirseparate contributions. This is the first work suggesting the application of LOPs to decoding in ma chine translation. In the future more sophisticated translation models and ensembles of models willneed methods such as LOPs in order to balance sta tistical evidence from multiple sources. Model BLEU sw, tw 23.30 swc, tw 19.73 single 23.29 LOP 23.46 Table 6. German-English: CCG supertags are used as a factor on the source. The simple models are combined in two ways: either as a single log-linear model or as a LOP of log-linear models Table 6 shows that the simple, general model (model sw, tw) performs considerably better thanthe simple specific model, where there are multi ple dependencies on both words and CCG supertags (model swc, tw). This is because there are words in the test sentence that have been seen before but not with the CCG supertag. Statistical evidence from multiple sources must be combined. The first wayto combine them is to join them in one single log linear model, which is trained over many features.This makes finding good weights difficult as the influence of the general model is greater, and its dif ficult for the more specific model to discover goodweights. The second method for combining the in formation is to use the weights from the separately trained simple models and then combine them in a LOP. Held out data is used to set the multiplicative constant needed to balance the contribution of the two models. We can see that this second approach is more successful and this suggests that it is importantto carefully consider the best ways of combining dif ferent sources of information when using ensembles of models. However, the results of this experiment are not very conclusive. There is no uncertainty inthe source sentence and the value of modelling it us ing CCG supertags is still to be demonstrated.
Conclusion. . The factored translation model allows for the inclusion of valuable sources of information in many dif ferent ways. We have shown that the syntacticallyrich CCG supertags do improve the translation pro cess and we investigate the best way of including them in the factored model. Using CCG supertagsover the target shows the most improvement, especially when using targeted manual evaluation. How ever, this effect seems to be largely due to improvedlocal reordering. Reordering improvements can per haps be more reliably made using better reordering models or larger, more powerful language models. A further consideration is that supertags will always be limited to the few languages for which there are treebanks. Syntactic information represents embedded structures which are naturally incorporated intogrammar-based models. The ability of a flat struc tured model to leverage this information seems to be limited. CCG supertags? ability to guide translation would be enhanced if the constraints encoded in the tags were to be enforced using combinatory operators.
Acknowledgements. . We thank Hieu Hoang for assistance with Moses, Ju lia Hockenmaier for access to CCGbank lexicons in German and English, and Stephen Clark and James Curran for providing the supertagger. This work was supported in part under the GALE program of theDefense Advanced Research Projects Agency, Con tract No. HR0011-06-C-0022 and in part under theEuroMatrix project funded by the European Com mission (6th Framework Programme). 15